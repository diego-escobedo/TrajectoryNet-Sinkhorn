/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    logps = [logpz]
    for i, delta_logp in enumerate(deltas[::-1]):
        logpx = logps[-1] - delta_logp
        if args.use_growth:
            logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
        logps.append(logpx[: -args.batch_size])
        losses.append(-torch.mean(logpx[-args.batch_size :]))
    losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(logpx)
    if args.leaveout_timepoint >= 0:
        weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
Iter 0001 | Time 12.5864(12.5864) | Loss 9.073777(9.073777) | NFE Forward 14(14.0) | NFE Backward 138(138.0)
Iter 0002 | Time 11.8407(12.5342) | Loss 8.852876(9.058314) | NFE Forward 14(14.0) | NFE Backward 138(138.0)
Iter 0003 | Time 11.5807(12.4675) | Loss 8.724459(9.034944) | NFE Forward 14(14.0) | NFE Backward 138(138.0)
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    logps = [logpz]
    for i, delta_logp in enumerate(deltas[::-1]):
        logpx = logps[-1] - delta_logp
        if args.use_growth:
            logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
        logps.append(logpx[: -args.batch_size])
        losses.append(-torch.mean(logpx[-args.batch_size :]))
    losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(logpx)
    if args.leaveout_timepoint >= 0:
        weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
Iter 0001 | Time 12.5342(12.5342) | Loss 10.334513(10.334513) | NFE Forward 14(14.0) | NFE Backward 132(132.0)
Iter 0002 | Time 11.7633(12.4802) | Loss 10.122818(10.319694) | NFE Forward 14(14.0) | NFE Backward 132(132.0)
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    print("here")
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      print(pred_z.shape, full_data.shape, full_data[i].shape)
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    print(len(zs))
    for i, pred_z in enumerate(zs[::-1]):
      print(pred_z.shape, full_data.shape, full_data[i].shape)
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    print(len(zs))
    for i, pred_z in enumerate(zs[::-1]):
      print(pred_z.shape, full_data.shape, full_data[i].shape)
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    print(len(zs))
    for i, pred_z in enumerate(zs[::-1]):
      print(pred_z.shape, full_data.shape, full_data[i].shape)
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    print(len(zs))
    for i, pred_z in enumerate(zs[::-1]):
      print(pred_z.shape, full_data.shape, full_data[i].shape)
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      print(pred_z.shape, full_data.shape, full_data[i].shape)
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    print("what he fuck goin on")
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      print(pred_z.shape, full_data.shape, full_data[i].shape)
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)
    print("data shape", args.data.shape)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      print(pred_z.shape, full_data.shape, full_data[i].shape)
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)
    print("data shape", args.data.data.shape)
    print("data shape", args.data.labels.shape)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      print(pred_z.shape, full_data.shape, full_data[i].shape)
      l = train_loss_fn(pred_z, full_data[i])
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)
    print("data shape", args.data.labels[0])

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = args.data.data[args.data.labels == i]
      print(pred_z.shape, fd.shape)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == I]).to(zs)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == i]).to(zs)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == i]).to(pred_z)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == i]).to(pred_z)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == i]).to(pred_z)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses * weights)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=1, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == i]).to(pred_z)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    # weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=1, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == i]).to(pred_z)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    # weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(losses)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == i]).to(pred_z)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    # weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(torch.tensor(losses))

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == i]).to(pred_z)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    # weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(torch.tensor(losses))
    print(losses.device)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
/content/drive/MyDrive/NeuralODE/trajnet_ablation/TrajectoryNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from geomloss import SamplesLoss

from TrajectoryNet.lib.growth_net import GrowthNet
from TrajectoryNet.lib import utils
from TrajectoryNet.lib.visualize_flow import visualize_transform
from TrajectoryNet.lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from TrajectoryNet.lib.viz_scrna import save_trajectory_density


# from train_misc import standard_normal_logprob
from TrajectoryNet.train_misc import (
    set_cnf_options,
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_tabular,
)

from TrajectoryNet import dataset
from TrajectoryNet.parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, growth_model, logger, full_data):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.
    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """
    def mask_split(tensor, indices):
        unique = torch.unique(indices)
        return [tensor[indices == i] for i in unique]
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    # Backward pass accumulating losses, previous state and deltas
    deltas = []
    zs = []
    z = None
    interp_loss = 0.0
    for i, (itp, tp) in enumerate(zip(args.int_tps[::-1], args.timepoints[::-1])):
        # tp counts down from last
        integration_times = torch.tensor([itp - args.time_scale, itp])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        idx = args.data.sample_index(args.batch_size, tp)
        x = args.data.get_data()[idx]
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise
        x = torch.from_numpy(x).type(torch.float32).to(device)

        if i > 0:
            x = torch.cat((z, x))
            zs.append(z)
        zero = torch.zeros(x.shape[0], 1).to(x)

        # transform to previous timepoint
        z, delta_logp = model(x, zero, integration_times=integration_times)
        deltas.append(delta_logp)

        # Straightline regularization
        # Integrate to random point at time t and assert close to (1 - t) * end + t * start
        if args.interp_reg:
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            int_x = int_x.detach()
            actual_int_x = x * (1 - t) + z * t
            interp_loss += F.mse_loss(int_x, actual_int_x)
    if args.interp_reg:
        print("interp_loss", interp_loss)

    logpz = args.data.base_density()(z)

    # build growth rates
    if args.use_growth:
        growthrates = [torch.ones_like(logpz)]
        for z_state, tp in zip(zs[::-1], args.timepoints[:-1]):
            # Full state includes time parameter to growth_model
            time_state = tp * torch.ones(z_state.shape[0], 1).to(z_state)
            full_state = torch.cat([z_state, time_state], 1)
            growthrates.append(growth_model(full_state))

    # Accumulate losses
    losses = []
    for i, pred_z in enumerate(zs[::-1]):
      fd = torch.tensor(args.data.data[args.data.labels == i]).to(pred_z)
      l = train_loss_fn(pred_z, fd)
      losses.append(l)
    # logps = [logpz]
    # for i, delta_logp in enumerate(deltas[::-1]):
    #     logpx = logps[-1] - delta_logp
    #     if args.use_growth:
    #         logpx += torch.log(torch.clamp(growthrates[i], 1e-4, 1e4))
    #     logps.append(logpx[: -args.batch_size])
    #     losses.append(-torch.mean(logpx[-args.batch_size :]))
    # losses = torch.stack(losses)
    # weights = torch.ones_like(losses).to(device)
    # if args.leaveout_timepoint >= 0:
    #     weights[args.leaveout_timepoint] = 0
    losses = torch.mean(torch.tensor(losses)).to(device)

    # Direction regularization
    if args.vecint:
        similarity_loss = 0
        for i, (itp, tp) in enumerate(zip(args.int_tps, args.timepoints)):
            itp = torch.tensor(itp).type(torch.float32).to(device)
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            v = args.data.get_velocity()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
            v = torch.from_numpy(v).type(torch.float32).to(device)
            x += torch.randn_like(x) * 0.1
            # Only penalizes at the time / place of visible samples
            direction = -model.chain[0].odefunc.odefunc.diffeq(itp, x)
            if args.use_magnitude:
                similarity_loss += torch.mean(F.mse_loss(direction, v))
            else:
                similarity_loss -= torch.mean(F.cosine_similarity(direction, v))
        logger.info(similarity_loss)
        losses += similarity_loss * args.vecint

    # Density regularization
    if args.top_k_reg > 0:
        density_loss = 0
        tp_z_map = dict(zip(args.timepoints[:-1], zs[::-1]))
        if args.leaveout_timepoint not in tp_z_map:
            idx = args.data.sample_index(args.batch_size, tp)
            x = args.data.get_data()[idx]
            if args.training_noise > 0.0:
                x += np.random.randn(*x.shape) * args.training_noise
            x = torch.from_numpy(x).type(torch.float32).to(device)
            t = np.random.rand()
            int_t = torch.tensor([itp - t * args.time_scale, itp])
            int_t = int_t.type(torch.float32).to(device)
            int_x = model(x, integration_times=int_t)
            samples_05 = int_x
        else:
            # If we are leaving out a timepoint the regularize there
            samples_05 = tp_z_map[args.leaveout_timepoint]

        # Calculate distance to 5 closest neighbors
        # WARNING: This currently fails in the backward pass with cuda on pytorch < 1.4.0
        #          works on CPU. Fixed in pytorch 1.5.0
        # RuntimeError: CUDA error: invalid configuration argument
        # The workaround is to run on cpu on pytorch <= 1.4.0 or upgrade
        cdist = torch.cdist(samples_05, full_data)
        values, _ = torch.topk(cdist, 5, dim=1, largest=False, sorted=False)
        # Hinge loss
        hinge_value = 0.1
        values -= hinge_value
        values[values < 0] = 0
        density_loss = torch.mean(values)
        print("Density Loss", density_loss.item())
        losses += density_loss * args.top_k_reg
    losses += interp_loss
    return losses


def train(
    device, args, model, growth_model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    if args.use_growth:
        growth_model.eval()
    end = time.time()
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, growth_model, logger, full_data)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, growth_model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            if args.data.get_shape()[0] > 2:
                logger.warning("Skipping vis as data dimension is >2")
            else:
                with torch.no_grad():
                    visualize(device, args, model, itr)
        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }
            if args.use_growth:
                chkpt.update({"growth_state_dict": growth_model.state_dict()})
            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, growth_model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, growth_model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        if args.use_growth:
            chkpt.update({"growth_state_dict": growth_model.state_dict()})
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    if args.layer_type == "blend":
        logger.info("!! Setting time_scale from None to 1.0 for Blend layers.")
        args.time_scale = 1.0

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")
    
    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )
    growth_model = None
    if args.use_growth:
        if args.leaveout_timepoint == -1:
            growth_model_path = "../data/externel/growth_model_v2.ckpt"
        elif args.leaveout_timepoint in [1, 2, 3]:
            assert args.max_dim == 5
            growth_model_path = "../data/growth/model_%d" % args.leaveout_timepoint
        else:
            print("WARNING: Cannot use growth with this timepoint")

        growth_model = torch.load(growth_model_path, map_location=device)
    if args.spectral_norm:
        add_spectral_norm(model)
    set_cnf_options(args, model)

    if args.test:
        state_dict = torch.load(args.save + "/checkpt.pth", map_location=device)
        model.load_state_dict(state_dict["state_dict"])
        # if "growth_state_dict" not in state_dict:
        #    print("error growth model note in save")
        #    growth_model = None
        # else:
        #    checkpt = torch.load(args.save + "/checkpt.pth", map_location=device)
        #    growth_model.load_state_dict(checkpt["growth_state_dict"])
        # TODO can we load the arguments from the save?
        # eval_utils.generate_samples(
        #    device, args, model, growth_model, timepoint=args.leaveout_timepoint
        # )
        # with torch.no_grad():
        #    evaluate(device, args, model, growth_model)
    #    exit()
    else:
        logger.info(model)
        n_param = count_parameters(model)
        logger.info("Number of trainable parameters: {}".format(n_param))

        train(
            device,
            args,
            model,
            growth_model,
            regularization_coeffs,
            regularization_fns,
            logger,
        )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, interp_reg=None, l1int=None, l2int=None, layer_type='concatsquash', leaveout_timepoint=-1, log_freq=10, lr=0.001, max_dim=5, niters=10000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.1, train_T=True, training_noise=0.0, use_cpu=False, use_density=False, use_growth=False, use_magnitude=False, val_freq=100, vecint=None, viz_batch_size=2000, viz_freq=100, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=5, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=5, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=5, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=5, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9621
Iter 0001 | Time 1.0757(1.0757) | Loss 7.861552(7.861552) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0002 | Time 1.0017(1.0706) | Loss 7.576643(7.841608) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0003 | Time 1.0222(1.0672) | Loss 7.318413(7.804984) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0004 | Time 1.0386(1.0652) | Loss 7.058055(7.752699) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0005 | Time 1.0361(1.0631) | Loss 6.985351(7.698985) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0006 | Time 1.1144(1.0667) | Loss 6.917171(7.644258) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0007 | Time 1.0999(1.0690) | Loss 6.594548(7.570778) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0008 | Time 1.0158(1.0653) | Loss 6.527258(7.497732) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0009 | Time 1.0820(1.0665) | Loss 6.196088(7.406617) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0010 | Time 1.0422(1.0648) | Loss 6.141345(7.318048) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0011 | Time 0.9897(1.0595) | Loss 5.934883(7.221226) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0012 | Time 1.0033(1.0556) | Loss 5.747671(7.118077) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0013 | Time 0.9932(1.0512) | Loss 5.517593(7.006043) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0014 | Time 1.0360(1.0502) | Loss 5.563598(6.905072) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0015 | Time 1.0330(1.0490) | Loss 5.368293(6.797498) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0016 | Time 1.0167(1.0467) | Loss 5.185570(6.684663) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0017 | Time 1.0078(1.0440) | Loss 5.112353(6.574601) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0018 | Time 0.9717(1.0389) | Loss 4.927442(6.459300) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0019 | Time 1.0408(1.0390) | Loss 4.890247(6.349466) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0020 | Time 1.0495(1.0398) | Loss 4.845807(6.244210) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0021 | Time 1.0553(1.0409) | Loss 4.800071(6.143120) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0022 | Time 1.0409(1.0409) | Loss 4.719326(6.043455) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0023 | Time 0.9926(1.0375) | Loss 4.604046(5.942696) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0024 | Time 0.9582(1.0319) | Loss 4.640970(5.851575) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0025 | Time 0.9489(1.0261) | Loss 4.567588(5.761696) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0026 | Time 0.9902(1.0236) | Loss 4.531440(5.675578) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0027 | Time 0.9532(1.0187) | Loss 4.506552(5.593747) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0028 | Time 0.9390(1.0131) | Loss 4.395967(5.509902) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0029 | Time 0.9949(1.0118) | Loss 4.438638(5.434913) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0030 | Time 0.9721(1.0091) | Loss 4.533637(5.371824) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0031 | Time 0.9842(1.0073) | Loss 4.527605(5.312729) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0032 | Time 0.9897(1.0061) | Loss 4.472504(5.253913) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0033 | Time 1.0186(1.0070) | Loss 4.481373(5.199835) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0034 | Time 1.0177(1.0077) | Loss 4.495964(5.150564) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0035 | Time 0.9719(1.0052) | Loss 4.423292(5.099655) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0036 | Time 0.9747(1.0031) | Loss 4.470992(5.055649) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0037 | Time 1.0108(1.0036) | Loss 4.603470(5.023996) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0038 | Time 1.0013(1.0034) | Loss 4.617860(4.995567) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0039 | Time 0.9944(1.0028) | Loss 4.462125(4.958226) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0040 | Time 0.9513(0.9992) | Loss 4.594541(4.932768) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0041 | Time 0.9640(0.9967) | Loss 4.547829(4.905822) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0042 | Time 1.0170(0.9981) | Loss 4.574984(4.882663) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0043 | Time 0.9813(0.9970) | Loss 4.589583(4.862148) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0044 | Time 0.9852(0.9961) | Loss 4.602813(4.843994) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0045 | Time 0.9775(0.9948) | Loss 4.676738(4.832286) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0046 | Time 0.9507(0.9917) | Loss 4.583845(4.814896) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0047 | Time 0.9421(0.9883) | Loss 4.597397(4.799671) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0048 | Time 0.9349(0.9845) | Loss 4.582437(4.784464) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0049 | Time 0.9258(0.9804) | Loss 4.501464(4.764654) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0050 | Time 0.9738(0.9800) | Loss 4.506232(4.746565) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0051 | Time 0.9421(0.9773) | Loss 4.474552(4.727524) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0052 | Time 0.9282(0.9739) | Loss 4.625989(4.720416) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0053 | Time 0.9615(0.9730) | Loss 4.602267(4.712146) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0054 | Time 0.9287(0.9699) | Loss 4.415097(4.691353) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0055 | Time 0.9696(0.9699) | Loss 4.527910(4.679912) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0056 | Time 0.9383(0.9677) | Loss 4.424391(4.662025) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0057 | Time 0.9324(0.9652) | Loss 4.506904(4.651167) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0058 | Time 0.9834(0.9665) | Loss 4.410002(4.634285) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0059 | Time 0.9447(0.9650) | Loss 4.481333(4.623578) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0060 | Time 0.9799(0.9660) | Loss 4.308645(4.601533) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0061 | Time 0.9739(0.9666) | Loss 4.441182(4.590308) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0062 | Time 0.9970(0.9687) | Loss 4.382850(4.575786) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0063 | Time 0.9918(0.9703) | Loss 4.397215(4.563286) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0064 | Time 0.9348(0.9678) | Loss 4.515978(4.559975) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0065 | Time 0.9745(0.9683) | Loss 4.495601(4.555469) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0066 | Time 0.9882(0.9697) | Loss 4.362060(4.541930) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0067 | Time 0.9634(0.9692) | Loss 4.352747(4.528687) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0068 | Time 0.9268(0.9663) | Loss 4.292594(4.512161) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0069 | Time 0.9302(0.9637) | Loss 4.320094(4.498716) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0070 | Time 0.9510(0.9629) | Loss 4.402066(4.491951) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0071 | Time 0.9358(0.9610) | Loss 4.404876(4.485855) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0072 | Time 0.9232(0.9583) | Loss 4.256947(4.469832) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0073 | Time 1.0062(0.9617) | Loss 4.438308(4.467625) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0074 | Time 0.9564(0.9613) | Loss 4.430445(4.465022) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0075 | Time 0.9469(0.9603) | Loss 4.425699(4.462270) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0076 | Time 0.9612(0.9604) | Loss 4.247832(4.447259) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0077 | Time 0.9477(0.9595) | Loss 4.397939(4.443807) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0078 | Time 0.9682(0.9601) | Loss 4.333120(4.436059) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0079 | Time 0.9369(0.9585) | Loss 4.354998(4.430384) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0080 | Time 0.9407(0.9572) | Loss 4.311536(4.422065) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0081 | Time 0.9335(0.9556) | Loss 4.295944(4.413237) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0082 | Time 0.9366(0.9542) | Loss 4.221911(4.399844) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0083 | Time 0.9624(0.9548) | Loss 4.238371(4.388541) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0084 | Time 0.9431(0.9540) | Loss 4.236650(4.377908) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0085 | Time 0.9531(0.9539) | Loss 4.187994(4.364614) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0086 | Time 0.9351(0.9526) | Loss 4.148014(4.349452) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0087 | Time 0.9873(0.9550) | Loss 4.231540(4.341198) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0088 | Time 0.9544(0.9550) | Loss 4.110446(4.325046) | NFE Forward 14(14.0) | NFE Backward 15(15.0)
Iter 0089 | Time 0.9903(0.9575) | Loss 4.105711(4.309692) | NFE Forward 20(14.4) | NFE Backward 15(15.0)
Iter 0090 | Time 1.0882(0.9666) | Loss 4.077162(4.293415) | NFE Forward 14(14.4) | NFE Backward 15(15.0)
Iter 0091 | Time 0.9888(0.9682) | Loss 4.049547(4.276345) | NFE Forward 14(14.4) | NFE Backward 15(15.0)
Iter 0092 | Time 1.0662(0.9750) | Loss 4.136062(4.266525) | NFE Forward 14(14.3) | NFE Backward 15(15.0)
Iter 0093 | Time 1.0738(0.9819) | Loss 4.097657(4.254704) | NFE Forward 20(14.7) | NFE Backward 15(15.0)
Iter 0094 | Time 1.0385(0.9859) | Loss 4.080198(4.242489) | NFE Forward 20(15.1) | NFE Backward 15(15.0)
Iter 0095 | Time 1.0092(0.9875) | Loss 4.127633(4.234449) | NFE Forward 20(15.4) | NFE Backward 15(15.0)
Iter 0096 | Time 1.1137(0.9964) | Loss 4.023162(4.219659) | NFE Forward 20(15.8) | NFE Backward 15(15.0)
Iter 0097 | Time 1.0623(1.0010) | Loss 4.094064(4.210867) | NFE Forward 20(16.1) | NFE Backward 15(15.0)
Iter 0098 | Time 1.0225(1.0025) | Loss 4.077745(4.201548) | NFE Forward 20(16.3) | NFE Backward 15(15.0)
Iter 0099 | Time 1.1322(1.0116) | Loss 3.919139(4.181780) | NFE Forward 20(16.6) | NFE Backward 21(15.4)
Iter 0100 | Time 1.0892(1.0170) | Loss 3.874327(4.160258) | NFE Forward 20(16.8) | NFE Backward 15(15.4)
[TEST] Iter 0100 | Test Loss 3.883348 | NFE 20
Skipping vis as data dimension is >2
Iter 0101 | Time 1.1271(1.0247) | Loss 3.943325(4.145073) | NFE Forward 20(17.1) | NFE Backward 21(15.8)
Iter 0102 | Time 1.1350(1.0324) | Loss 3.850657(4.124464) | NFE Forward 20(17.3) | NFE Backward 15(15.7)
Iter 0103 | Time 1.1294(1.0392) | Loss 3.823210(4.103376) | NFE Forward 20(17.5) | NFE Backward 15(15.7)
Iter 0104 | Time 1.1883(1.0497) | Loss 3.678013(4.073600) | NFE Forward 20(17.6) | NFE Backward 21(16.1)
Iter 0105 | Time 1.1070(1.0537) | Loss 3.650006(4.043949) | NFE Forward 20(17.8) | NFE Backward 15(16.0)
Iter 0106 | Time 1.1317(1.0591) | Loss 3.787273(4.025982) | NFE Forward 20(18.0) | NFE Backward 21(16.3)
Iter 0107 | Time 1.1418(1.0649) | Loss 3.729643(4.005238) | NFE Forward 20(18.1) | NFE Backward 21(16.7)
Iter 0108 | Time 1.2135(1.0753) | Loss 3.747227(3.987177) | NFE Forward 20(18.2) | NFE Backward 21(17.0)
Iter 0109 | Time 1.1964(1.0838) | Loss 3.593497(3.959620) | NFE Forward 20(18.4) | NFE Backward 21(17.2)
Iter 0110 | Time 1.1350(1.0874) | Loss 3.686537(3.940504) | NFE Forward 20(18.5) | NFE Backward 21(17.5)
Iter 0111 | Time 1.1658(1.0929) | Loss 3.639324(3.919421) | NFE Forward 20(18.6) | NFE Backward 21(17.7)
Iter 0112 | Time 1.1246(1.0951) | Loss 3.636472(3.899615) | NFE Forward 20(18.7) | NFE Backward 21(18.0)
Iter 0113 | Time 1.1760(1.1008) | Loss 3.548906(3.875065) | NFE Forward 20(18.8) | NFE Backward 21(18.2)
Iter 0114 | Time 1.1696(1.1056) | Loss 3.465416(3.846390) | NFE Forward 20(18.9) | NFE Backward 21(18.4)
Iter 0115 | Time 1.1219(1.1067) | Loss 3.585062(3.828097) | NFE Forward 20(18.9) | NFE Backward 21(18.6)
Iter 0116 | Time 1.1264(1.1081) | Loss 3.469951(3.803027) | NFE Forward 20(19.0) | NFE Backward 21(18.7)
Iter 0117 | Time 1.1677(1.1123) | Loss 3.468083(3.779580) | NFE Forward 20(19.1) | NFE Backward 21(18.9)
Iter 0118 | Time 1.1286(1.1134) | Loss 3.527268(3.761919) | NFE Forward 20(19.1) | NFE Backward 21(19.0)
Iter 0119 | Time 1.1796(1.1180) | Loss 3.469762(3.741468) | NFE Forward 20(19.2) | NFE Backward 21(19.2)
Iter 0120 | Time 1.1720(1.1218) | Loss 3.490450(3.723896) | NFE Forward 20(19.3) | NFE Backward 21(19.3)
Iter 0121 | Time 1.1592(1.1244) | Loss 3.430130(3.703333) | NFE Forward 20(19.3) | NFE Backward 21(19.4)
Iter 0122 | Time 1.1724(1.1278) | Loss 3.452047(3.685743) | NFE Forward 20(19.4) | NFE Backward 21(19.5)
Iter 0123 | Time 1.1731(1.1310) | Loss 3.372307(3.663802) | NFE Forward 20(19.4) | NFE Backward 21(19.6)
Iter 0124 | Time 1.1974(1.1356) | Loss 3.406208(3.645771) | NFE Forward 20(19.4) | NFE Backward 21(19.7)
Iter 0125 | Time 1.2259(1.1419) | Loss 3.416478(3.629720) | NFE Forward 20(19.5) | NFE Backward 21(19.8)
Iter 0126 | Time 1.1667(1.1437) | Loss 3.407865(3.614190) | NFE Forward 20(19.5) | NFE Backward 21(19.9)
Iter 0127 | Time 1.1615(1.1449) | Loss 3.447713(3.602537) | NFE Forward 20(19.6) | NFE Backward 21(20.0)
Iter 0128 | Time 1.1769(1.1472) | Loss 3.500822(3.595417) | NFE Forward 20(19.6) | NFE Backward 21(20.1)
Iter 0129 | Time 1.1679(1.1486) | Loss 3.447868(3.585088) | NFE Forward 20(19.6) | NFE Backward 21(20.1)
Iter 0130 | Time 1.1709(1.1502) | Loss 3.326581(3.566993) | NFE Forward 20(19.6) | NFE Backward 21(20.2)
Iter 0131 | Time 1.1644(1.1512) | Loss 3.356651(3.552269) | NFE Forward 20(19.7) | NFE Backward 21(20.2)
Iter 0132 | Time 1.1686(1.1524) | Loss 3.319656(3.535986) | NFE Forward 20(19.7) | NFE Backward 21(20.3)
Iter 0133 | Time 1.1977(1.1556) | Loss 3.295142(3.519127) | NFE Forward 20(19.7) | NFE Backward 21(20.3)
Iter 0134 | Time 1.1545(1.1555) | Loss 3.265318(3.501360) | NFE Forward 20(19.7) | NFE Backward 21(20.4)
Iter 0135 | Time 1.1525(1.1553) | Loss 3.321974(3.488803) | NFE Forward 20(19.8) | NFE Backward 21(20.4)
Iter 0136 | Time 1.1582(1.1555) | Loss 3.353260(3.479315) | NFE Forward 20(19.8) | NFE Backward 21(20.5)
Iter 0137 | Time 1.1897(1.1579) | Loss 3.359852(3.470953) | NFE Forward 20(19.8) | NFE Backward 21(20.5)
Iter 0138 | Time 1.2125(1.1617) | Loss 3.337456(3.461608) | NFE Forward 20(19.8) | NFE Backward 21(20.5)
Iter 0139 | Time 1.1691(1.1622) | Loss 3.305818(3.450703) | NFE Forward 20(19.8) | NFE Backward 21(20.6)
Iter 0140 | Time 1.1511(1.1614) | Loss 3.260548(3.437392) | NFE Forward 20(19.8) | NFE Backward 21(20.6)
Iter 0141 | Time 1.1700(1.1620) | Loss 3.305604(3.428167) | NFE Forward 20(19.8) | NFE Backward 21(20.6)
Iter 0142 | Time 1.1968(1.1645) | Loss 3.348381(3.422582) | NFE Forward 20(19.8) | NFE Backward 21(20.7)
Iter 0143 | Time 1.1749(1.1652) | Loss 3.321702(3.415520) | NFE Forward 20(19.9) | NFE Backward 21(20.7)
Iter 0144 | Time 1.2085(1.1682) | Loss 3.249212(3.403879) | NFE Forward 20(19.9) | NFE Backward 21(20.7)
Iter 0145 | Time 1.2381(1.1731) | Loss 3.296070(3.396332) | NFE Forward 20(19.9) | NFE Backward 21(20.7)
Iter 0146 | Time 1.1663(1.1726) | Loss 3.320296(3.391010) | NFE Forward 20(19.9) | NFE Backward 21(20.7)
Iter 0147 | Time 1.2129(1.1755) | Loss 3.217927(3.378894) | NFE Forward 20(19.9) | NFE Backward 21(20.8)
Iter 0148 | Time 1.2322(1.1794) | Loss 3.220477(3.367805) | NFE Forward 20(19.9) | NFE Backward 21(20.8)
Iter 0149 | Time 1.2277(1.1828) | Loss 3.227940(3.358014) | NFE Forward 20(19.9) | NFE Backward 21(20.8)
Iter 0150 | Time 1.2070(1.1845) | Loss 3.242372(3.349919) | NFE Forward 20(19.9) | NFE Backward 21(20.8)
Iter 0151 | Time 1.2235(1.1872) | Loss 3.267154(3.344126) | NFE Forward 20(19.9) | NFE Backward 21(20.8)
Iter 0152 | Time 1.2193(1.1895) | Loss 3.180413(3.332666) | NFE Forward 20(19.9) | NFE Backward 21(20.8)
Iter 0153 | Time 1.2172(1.1914) | Loss 3.233905(3.325752) | NFE Forward 20(19.9) | NFE Backward 21(20.8)
Iter 0154 | Time 1.2051(1.1924) | Loss 3.246545(3.320208) | NFE Forward 20(19.9) | NFE Backward 21(20.9)
Iter 0155 | Time 1.2122(1.1938) | Loss 3.176468(3.310146) | NFE Forward 20(19.9) | NFE Backward 21(20.9)
Iter 0156 | Time 1.2061(1.1946) | Loss 3.200817(3.302493) | NFE Forward 20(19.9) | NFE Backward 21(20.9)
Iter 0157 | Time 1.1999(1.1950) | Loss 3.167585(3.293050) | NFE Forward 20(19.9) | NFE Backward 21(20.9)
Iter 0158 | Time 1.2022(1.1955) | Loss 3.199567(3.286506) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0159 | Time 1.1978(1.1957) | Loss 3.105383(3.273827) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0160 | Time 1.2062(1.1964) | Loss 3.201575(3.268769) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0161 | Time 1.2199(1.1980) | Loss 3.232051(3.266199) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0162 | Time 1.1943(1.1978) | Loss 3.186718(3.260635) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0163 | Time 1.1908(1.1973) | Loss 3.140289(3.252211) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0164 | Time 1.2013(1.1976) | Loss 3.222605(3.250139) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0165 | Time 1.1952(1.1974) | Loss 3.214100(3.247616) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0166 | Time 1.1783(1.1961) | Loss 3.163948(3.241759) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0167 | Time 1.2074(1.1969) | Loss 3.117837(3.233085) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0168 | Time 1.2026(1.1973) | Loss 3.159602(3.227941) | NFE Forward 20(20.0) | NFE Backward 21(20.9)
Iter 0169 | Time 1.2230(1.1991) | Loss 3.171499(3.223990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0170 | Time 1.1963(1.1989) | Loss 3.067854(3.213061) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0171 | Time 1.1986(1.1989) | Loss 3.204802(3.212483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0172 | Time 1.2225(1.2005) | Loss 3.183030(3.210421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0173 | Time 1.2216(1.2020) | Loss 3.193878(3.209263) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0174 | Time 1.1830(1.2007) | Loss 3.224214(3.210309) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0175 | Time 1.2259(1.2024) | Loss 3.222939(3.211193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0176 | Time 1.1813(1.2010) | Loss 3.236493(3.212964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0177 | Time 1.2366(1.2035) | Loss 3.228000(3.214017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0178 | Time 1.2452(1.2064) | Loss 3.156523(3.209992) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0179 | Time 1.2133(1.2069) | Loss 3.150830(3.205851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0180 | Time 1.2130(1.2073) | Loss 3.097803(3.198288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0181 | Time 1.2262(1.2086) | Loss 3.082346(3.190172) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0182 | Time 1.2201(1.2094) | Loss 3.074566(3.182079) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0183 | Time 1.2108(1.2095) | Loss 3.090860(3.175694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0184 | Time 1.2093(1.2095) | Loss 3.120595(3.171837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0185 | Time 1.2237(1.2105) | Loss 3.060949(3.164075) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0186 | Time 1.2174(1.2110) | Loss 3.092173(3.159042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0187 | Time 1.2290(1.2122) | Loss 3.094030(3.154491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0188 | Time 1.2257(1.2132) | Loss 3.244805(3.160813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0189 | Time 1.2303(1.2144) | Loss 3.173648(3.161711) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0190 | Time 1.2246(1.2151) | Loss 3.137872(3.160043) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0191 | Time 1.2075(1.2146) | Loss 3.139683(3.158618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0192 | Time 1.2303(1.2157) | Loss 3.052395(3.151182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0193 | Time 1.2163(1.2157) | Loss 3.230715(3.156749) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0194 | Time 1.2099(1.2153) | Loss 3.249486(3.163241) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0195 | Time 1.2227(1.2158) | Loss 3.149636(3.162288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0196 | Time 1.2156(1.2158) | Loss 3.165211(3.162493) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0197 | Time 1.2022(1.2149) | Loss 3.156152(3.162049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0198 | Time 1.2046(1.2141) | Loss 3.230122(3.166814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0199 | Time 1.2477(1.2165) | Loss 3.197633(3.168972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0200 | Time 1.2140(1.2163) | Loss 3.169483(3.169007) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 0200 | Test Loss 3.179540 | NFE 20
Skipping vis as data dimension is >2
Iter 0201 | Time 1.2162(1.2163) | Loss 3.035521(3.159663) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0202 | Time 1.2681(1.2199) | Loss 3.064598(3.153009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0203 | Time 1.2200(1.2199) | Loss 3.173398(3.154436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0204 | Time 1.2179(1.2198) | Loss 2.945292(3.139796) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0205 | Time 1.2593(1.2226) | Loss 3.036305(3.132552) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0206 | Time 1.2418(1.2239) | Loss 3.093343(3.129807) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0207 | Time 1.2362(1.2248) | Loss 3.121590(3.129232) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0208 | Time 1.2405(1.2259) | Loss 3.049669(3.123662) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0209 | Time 1.2417(1.2270) | Loss 3.077470(3.120429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0210 | Time 1.2129(1.2260) | Loss 3.124798(3.120735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0211 | Time 1.1944(1.2238) | Loss 3.078701(3.117792) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0212 | Time 1.1940(1.2217) | Loss 3.047343(3.112861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0213 | Time 1.2190(1.2215) | Loss 3.057785(3.109006) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0214 | Time 1.1788(1.2185) | Loss 3.028915(3.103399) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0215 | Time 1.1819(1.2159) | Loss 3.011568(3.096971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0216 | Time 1.1704(1.2128) | Loss 3.006404(3.090631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0217 | Time 1.1867(1.2109) | Loss 3.018640(3.085592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0218 | Time 1.1818(1.2089) | Loss 3.057543(3.083629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0219 | Time 1.1708(1.2062) | Loss 3.057931(3.081830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0220 | Time 1.1857(1.2048) | Loss 3.014297(3.077102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0221 | Time 1.2069(1.2049) | Loss 3.034494(3.074120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0222 | Time 1.2313(1.2068) | Loss 3.054486(3.072745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0223 | Time 1.2110(1.2071) | Loss 2.927524(3.062580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0224 | Time 1.2192(1.2079) | Loss 2.954135(3.054989) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0225 | Time 1.2062(1.2078) | Loss 2.888331(3.043323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0226 | Time 1.2183(1.2085) | Loss 2.986487(3.039344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0227 | Time 1.2142(1.2089) | Loss 2.979547(3.035158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0228 | Time 1.1913(1.2077) | Loss 2.962045(3.030041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0229 | Time 1.2036(1.2074) | Loss 2.953498(3.024683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0230 | Time 1.2173(1.2081) | Loss 2.908848(3.016574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0231 | Time 1.1953(1.2072) | Loss 2.968380(3.013201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0232 | Time 1.1982(1.2066) | Loss 2.921003(3.006747) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0233 | Time 1.1892(1.2054) | Loss 2.993155(3.005795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0234 | Time 1.2027(1.2052) | Loss 2.890053(2.997693) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0235 | Time 1.2016(1.2049) | Loss 2.909472(2.991518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0236 | Time 1.2126(1.2055) | Loss 2.823256(2.979740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0237 | Time 1.1957(1.2048) | Loss 2.829944(2.969254) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0238 | Time 1.2024(1.2046) | Loss 2.799443(2.957367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0239 | Time 1.1955(1.2040) | Loss 2.791939(2.945787) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0240 | Time 1.2080(1.2043) | Loss 2.920496(2.944017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0241 | Time 1.1804(1.2026) | Loss 2.883378(2.939772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0242 | Time 1.2109(1.2032) | Loss 2.829609(2.932061) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0243 | Time 1.2004(1.2030) | Loss 2.818577(2.924117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0244 | Time 1.1964(1.2025) | Loss 2.869931(2.920324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0245 | Time 1.1969(1.2021) | Loss 2.959212(2.923046) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0246 | Time 1.1913(1.2014) | Loss 2.843537(2.917480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0247 | Time 1.2018(1.2014) | Loss 2.746458(2.905509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0248 | Time 1.2106(1.2020) | Loss 2.846534(2.901380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0249 | Time 1.1704(1.1998) | Loss 2.890676(2.900631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0250 | Time 1.1719(1.1979) | Loss 2.927729(2.902528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0251 | Time 1.1677(1.1958) | Loss 2.848632(2.898755) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0252 | Time 1.1817(1.1948) | Loss 2.869190(2.896686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0253 | Time 1.2107(1.1959) | Loss 2.902547(2.897096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0254 | Time 1.1813(1.1949) | Loss 2.801856(2.890429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0255 | Time 1.2323(1.1975) | Loss 2.851797(2.887725) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0256 | Time 1.2431(1.2007) | Loss 2.871915(2.886618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0257 | Time 1.2512(1.2042) | Loss 2.792491(2.880029) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0258 | Time 1.2146(1.2049) | Loss 2.838764(2.877141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0259 | Time 1.2273(1.2065) | Loss 2.843597(2.874793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0260 | Time 1.1988(1.2060) | Loss 2.833805(2.871924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0261 | Time 1.2053(1.2059) | Loss 2.821718(2.868409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0262 | Time 1.1942(1.2051) | Loss 2.847380(2.866937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0263 | Time 1.2541(1.2085) | Loss 2.868771(2.867065) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0264 | Time 1.1971(1.2077) | Loss 2.782273(2.861130) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0265 | Time 1.1930(1.2067) | Loss 2.770628(2.854795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0266 | Time 1.1877(1.2054) | Loss 2.913735(2.858921) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0267 | Time 1.1923(1.2045) | Loss 2.791898(2.854229) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0268 | Time 1.1939(1.2037) | Loss 2.868424(2.855223) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0269 | Time 1.2381(1.2061) | Loss 2.756487(2.848311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0270 | Time 1.2257(1.2075) | Loss 2.737998(2.840589) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0271 | Time 1.2155(1.2081) | Loss 2.721310(2.832240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0272 | Time 1.6181(1.2368) | Loss 2.753309(2.826715) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0273 | Time 1.2537(1.2379) | Loss 2.658979(2.814973) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0274 | Time 1.2668(1.2400) | Loss 2.815244(2.814992) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0275 | Time 1.3303(1.2463) | Loss 2.734111(2.809330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0276 | Time 1.2388(1.2458) | Loss 2.755991(2.805597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0277 | Time 1.2040(1.2428) | Loss 2.671512(2.796211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0278 | Time 1.2213(1.2413) | Loss 2.823847(2.798145) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0279 | Time 1.2946(1.2451) | Loss 2.787400(2.797393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0280 | Time 1.3018(1.2490) | Loss 2.803751(2.797838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0281 | Time 1.2350(1.2481) | Loss 2.734325(2.793392) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0282 | Time 1.2451(1.2478) | Loss 2.751859(2.790485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0283 | Time 1.2615(1.2488) | Loss 2.703402(2.784389) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0284 | Time 1.2476(1.2487) | Loss 2.756834(2.782460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0285 | Time 1.2842(1.2512) | Loss 2.756370(2.780634) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0286 | Time 1.2207(1.2491) | Loss 2.687130(2.774089) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0287 | Time 1.2368(1.2482) | Loss 2.796798(2.775678) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0288 | Time 1.2159(1.2459) | Loss 2.711857(2.771211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0289 | Time 1.2236(1.2444) | Loss 2.660384(2.763453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0290 | Time 1.2253(1.2430) | Loss 2.851532(2.769619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0291 | Time 1.2424(1.2430) | Loss 2.766205(2.769380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0292 | Time 1.2390(1.2427) | Loss 2.776346(2.769867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0293 | Time 1.2786(1.2452) | Loss 2.757287(2.768987) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0294 | Time 1.2386(1.2448) | Loss 2.758858(2.768278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0295 | Time 1.2403(1.2445) | Loss 2.752265(2.767157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0296 | Time 1.2314(1.2435) | Loss 2.823585(2.771107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0297 | Time 1.2504(1.2440) | Loss 2.816895(2.774312) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0298 | Time 1.2793(1.2465) | Loss 2.709994(2.769810) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0299 | Time 1.2166(1.2444) | Loss 2.674468(2.763136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0300 | Time 1.2162(1.2424) | Loss 2.649552(2.755185) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 0300 | Test Loss 2.571122 | NFE 20
Skipping vis as data dimension is >2
Iter 0301 | Time 1.2087(1.2401) | Loss 2.697999(2.751182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0302 | Time 1.2367(1.2398) | Loss 2.650510(2.744135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0303 | Time 1.2596(1.2412) | Loss 2.656618(2.738009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0304 | Time 1.2108(1.2391) | Loss 2.651191(2.731931) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0305 | Time 1.2097(1.2370) | Loss 2.657357(2.726711) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0306 | Time 1.2452(1.2376) | Loss 2.561108(2.715119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0307 | Time 1.2056(1.2354) | Loss 2.652649(2.710746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0308 | Time 1.1981(1.2328) | Loss 2.602757(2.703187) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0309 | Time 1.2031(1.2307) | Loss 2.682010(2.701704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0310 | Time 1.2393(1.2313) | Loss 2.640568(2.697425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0311 | Time 1.2156(1.2302) | Loss 2.638757(2.693318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0312 | Time 1.2162(1.2292) | Loss 2.674848(2.692025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0313 | Time 1.2152(1.2282) | Loss 2.790777(2.698938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0314 | Time 1.2225(1.2278) | Loss 2.779594(2.704584) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0315 | Time 1.2123(1.2267) | Loss 2.689784(2.703548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0316 | Time 1.1998(1.2249) | Loss 2.651831(2.699928) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0317 | Time 1.1981(1.2230) | Loss 2.677040(2.698326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0318 | Time 1.2034(1.2216) | Loss 2.783974(2.704321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0319 | Time 1.1924(1.2196) | Loss 2.603835(2.697287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0320 | Time 1.2045(1.2185) | Loss 2.695557(2.697166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0321 | Time 1.2082(1.2178) | Loss 2.622957(2.691971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0322 | Time 1.2041(1.2168) | Loss 2.614944(2.686579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0323 | Time 1.2149(1.2167) | Loss 2.598822(2.680436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0324 | Time 1.2042(1.2158) | Loss 2.536698(2.670375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0325 | Time 1.1997(1.2147) | Loss 2.639685(2.668226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0326 | Time 1.1953(1.2133) | Loss 2.576212(2.661785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0327 | Time 1.2121(1.2132) | Loss 2.552483(2.654134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0328 | Time 1.2139(1.2133) | Loss 2.631151(2.652525) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0329 | Time 1.2150(1.2134) | Loss 2.606293(2.649289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0330 | Time 1.1903(1.2118) | Loss 2.576007(2.644159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0331 | Time 1.2026(1.2111) | Loss 2.659867(2.645259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0332 | Time 1.2283(1.2123) | Loss 2.674102(2.647278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0333 | Time 1.2486(1.2149) | Loss 2.709584(2.651639) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0334 | Time 1.2118(1.2147) | Loss 2.601353(2.648119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0335 | Time 1.2283(1.2156) | Loss 2.659569(2.648921) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0336 | Time 1.2276(1.2165) | Loss 2.596340(2.645240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0337 | Time 1.2308(1.2175) | Loss 2.636415(2.644622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0338 | Time 1.2182(1.2175) | Loss 2.568257(2.639277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0339 | Time 1.2266(1.2182) | Loss 2.508317(2.630110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0340 | Time 1.2512(1.2205) | Loss 2.572643(2.626087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0341 | Time 1.2136(1.2200) | Loss 2.619689(2.625639) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0342 | Time 1.2574(1.2226) | Loss 2.561558(2.621153) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0343 | Time 1.2448(1.2242) | Loss 2.581449(2.618374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0344 | Time 1.2221(1.2240) | Loss 2.591297(2.616479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0345 | Time 1.1981(1.2222) | Loss 2.581833(2.614053) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0346 | Time 1.2128(1.2215) | Loss 2.600198(2.613084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0347 | Time 1.1920(1.2195) | Loss 2.606046(2.612591) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0348 | Time 1.1826(1.2169) | Loss 2.583603(2.610562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0349 | Time 1.1981(1.2156) | Loss 2.575245(2.608090) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0350 | Time 1.1959(1.2142) | Loss 2.547200(2.603827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0351 | Time 1.1984(1.2131) | Loss 2.610992(2.604329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0352 | Time 1.2059(1.2126) | Loss 2.517035(2.598218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0353 | Time 1.2015(1.2118) | Loss 2.599020(2.598275) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0354 | Time 1.2028(1.2112) | Loss 2.551364(2.594991) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0355 | Time 1.1894(1.2097) | Loss 2.506707(2.588811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0356 | Time 1.1973(1.2088) | Loss 2.530209(2.584709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0357 | Time 1.1953(1.2079) | Loss 2.624449(2.587491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0358 | Time 1.1925(1.2068) | Loss 2.563874(2.585837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0359 | Time 1.1843(1.2052) | Loss 2.567248(2.584536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0360 | Time 1.2271(1.2067) | Loss 2.511211(2.579403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0361 | Time 1.2094(1.2069) | Loss 2.625060(2.582599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0362 | Time 1.2010(1.2065) | Loss 2.572898(2.581920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0363 | Time 1.2013(1.2061) | Loss 2.599564(2.583155) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0364 | Time 1.2002(1.2057) | Loss 2.470580(2.575275) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0365 | Time 1.2038(1.2056) | Loss 2.539198(2.572750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0366 | Time 1.2152(1.2063) | Loss 2.551553(2.571266) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0367 | Time 1.2141(1.2068) | Loss 2.551240(2.569864) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0368 | Time 1.2270(1.2082) | Loss 2.492351(2.564438) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0369 | Time 1.2025(1.2078) | Loss 2.517095(2.561124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0370 | Time 1.2125(1.2081) | Loss 2.491703(2.556265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0371 | Time 1.1911(1.2070) | Loss 2.571463(2.557329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0372 | Time 1.1981(1.2063) | Loss 2.539964(2.556113) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0373 | Time 1.1912(1.2053) | Loss 2.516167(2.553317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0374 | Time 1.1982(1.2048) | Loss 2.562845(2.553984) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0375 | Time 1.1972(1.2042) | Loss 2.480166(2.548817) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0376 | Time 1.1960(1.2037) | Loss 2.519964(2.546797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0377 | Time 1.1999(1.2034) | Loss 2.469867(2.541412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0378 | Time 1.1995(1.2031) | Loss 2.578032(2.543975) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0379 | Time 1.2047(1.2032) | Loss 2.448488(2.537291) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0380 | Time 1.2083(1.2036) | Loss 2.544479(2.537794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0381 | Time 1.1942(1.2029) | Loss 2.516768(2.536322) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0382 | Time 1.1980(1.2026) | Loss 2.508320(2.534362) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0383 | Time 1.1823(1.2012) | Loss 2.490027(2.531259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0384 | Time 1.1816(1.1998) | Loss 2.583110(2.534888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0385 | Time 1.1897(1.1991) | Loss 2.530941(2.534612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0386 | Time 1.1844(1.1981) | Loss 2.621557(2.540698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0387 | Time 1.1927(1.1977) | Loss 2.634749(2.547282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0388 | Time 1.2291(1.1999) | Loss 2.620017(2.552373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0389 | Time 1.2382(1.2026) | Loss 2.560920(2.552972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0390 | Time 1.2113(1.2032) | Loss 2.572609(2.554346) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0391 | Time 1.2156(1.2041) | Loss 2.488136(2.549711) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0392 | Time 1.2072(1.2043) | Loss 2.391695(2.538650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0393 | Time 1.2032(1.2042) | Loss 2.484642(2.534870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0394 | Time 1.2057(1.2043) | Loss 2.387546(2.524557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0395 | Time 1.1961(1.2037) | Loss 2.400379(2.515865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0396 | Time 1.1921(1.2029) | Loss 2.514964(2.515802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0397 | Time 1.1950(1.2024) | Loss 2.468045(2.512459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0398 | Time 1.1877(1.2013) | Loss 2.484627(2.510510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0399 | Time 1.1851(1.2002) | Loss 2.540607(2.512617) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0400 | Time 1.2053(1.2006) | Loss 2.585519(2.517720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 0400 | Test Loss 2.454855 | NFE 20
Skipping vis as data dimension is >2
Iter 0401 | Time 1.1870(1.1996) | Loss 2.517338(2.517694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0402 | Time 1.1905(1.1990) | Loss 2.492944(2.515961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0403 | Time 1.1903(1.1984) | Loss 2.521420(2.516343) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0404 | Time 1.1864(1.1975) | Loss 2.424861(2.509939) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0405 | Time 1.2097(1.1984) | Loss 2.458049(2.506307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0406 | Time 1.2198(1.1999) | Loss 2.474132(2.504055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0407 | Time 1.1891(1.1991) | Loss 2.457132(2.500770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0408 | Time 1.1951(1.1988) | Loss 2.365748(2.491319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0409 | Time 1.1842(1.1978) | Loss 2.460001(2.489127) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0410 | Time 1.1956(1.1977) | Loss 2.489959(2.489185) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0411 | Time 1.1779(1.1963) | Loss 2.409088(2.483578) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0412 | Time 1.1900(1.1958) | Loss 2.530629(2.486872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0413 | Time 1.1983(1.1960) | Loss 2.475727(2.486091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0414 | Time 1.2574(1.2003) | Loss 2.455942(2.483981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0415 | Time 1.2024(1.2005) | Loss 2.492817(2.484600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0416 | Time 1.2010(1.2005) | Loss 2.569662(2.490554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0417 | Time 1.1871(1.1996) | Loss 2.510920(2.491980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0418 | Time 1.2095(1.2003) | Loss 2.494851(2.492181) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0419 | Time 1.1881(1.1994) | Loss 2.540373(2.495554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0420 | Time 1.1861(1.1985) | Loss 2.432412(2.491134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0421 | Time 1.2016(1.1987) | Loss 2.483406(2.490593) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0422 | Time 1.1962(1.1985) | Loss 2.498082(2.491117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0423 | Time 1.1978(1.1985) | Loss 2.449609(2.488212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0424 | Time 1.1961(1.1983) | Loss 2.489081(2.488273) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0425 | Time 1.1891(1.1977) | Loss 2.381966(2.480831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0426 | Time 1.1856(1.1968) | Loss 2.426088(2.476999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0427 | Time 1.2631(1.2015) | Loss 2.545956(2.481826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0428 | Time 1.1840(1.2002) | Loss 2.460118(2.480306) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0429 | Time 1.1888(1.1994) | Loss 2.519877(2.483076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0430 | Time 1.1905(1.1988) | Loss 2.460991(2.481530) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0431 | Time 1.1946(1.1985) | Loss 2.525329(2.484596) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0432 | Time 1.1830(1.1974) | Loss 2.460869(2.482935) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0433 | Time 1.1730(1.1957) | Loss 2.374726(2.475361) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0434 | Time 1.1908(1.1954) | Loss 2.511039(2.477858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0435 | Time 1.2114(1.1965) | Loss 2.462622(2.476792) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0436 | Time 1.2026(1.1969) | Loss 2.474795(2.476652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0437 | Time 1.2015(1.1972) | Loss 2.442544(2.474264) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0438 | Time 1.2245(1.1991) | Loss 2.505822(2.476473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0439 | Time 1.2133(1.2001) | Loss 2.382410(2.469889) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0440 | Time 1.2512(1.2037) | Loss 2.384656(2.463923) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0441 | Time 1.2170(1.2046) | Loss 2.404122(2.459737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0442 | Time 1.2231(1.2059) | Loss 2.368035(2.453318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0443 | Time 1.1949(1.2052) | Loss 2.426461(2.451438) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0444 | Time 1.1954(1.2045) | Loss 2.338559(2.443536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0445 | Time 1.1995(1.2041) | Loss 2.331532(2.435696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0446 | Time 1.1921(1.2033) | Loss 2.336555(2.428756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0447 | Time 1.2042(1.2033) | Loss 2.302348(2.419907) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0448 | Time 1.1989(1.2030) | Loss 2.387095(2.417610) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0449 | Time 1.2123(1.2037) | Loss 2.289407(2.408636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0450 | Time 1.2166(1.2046) | Loss 2.357295(2.405042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0451 | Time 1.2192(1.2056) | Loss 2.222214(2.392244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0452 | Time 1.2193(1.2066) | Loss 2.304058(2.386071) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0453 | Time 1.2086(1.2067) | Loss 2.289231(2.379292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0454 | Time 1.2063(1.2067) | Loss 2.304334(2.374045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0455 | Time 1.2119(1.2070) | Loss 2.294910(2.368506) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0456 | Time 1.2023(1.2067) | Loss 2.285976(2.362729) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0457 | Time 1.2203(1.2077) | Loss 2.249277(2.354787) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0458 | Time 1.2082(1.2077) | Loss 2.248451(2.347344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0459 | Time 1.1994(1.2071) | Loss 2.266404(2.341678) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0460 | Time 1.2066(1.2071) | Loss 2.306275(2.339200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0461 | Time 1.1911(1.2060) | Loss 2.279921(2.335050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0462 | Time 1.2066(1.2060) | Loss 2.268634(2.330401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0463 | Time 1.2046(1.2059) | Loss 2.252419(2.324942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0464 | Time 1.2148(1.2065) | Loss 2.319657(2.324572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0465 | Time 1.2253(1.2078) | Loss 2.368925(2.327677) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0466 | Time 1.1990(1.2072) | Loss 2.332503(2.328015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0467 | Time 1.2140(1.2077) | Loss 2.287900(2.325207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0468 | Time 1.2086(1.2078) | Loss 2.305995(2.323862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0469 | Time 1.2375(1.2098) | Loss 2.279607(2.320764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0470 | Time 1.2418(1.2121) | Loss 2.332319(2.321573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0471 | Time 1.2172(1.2124) | Loss 2.264128(2.317552) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0472 | Time 1.2193(1.2129) | Loss 2.361958(2.320660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0473 | Time 1.2238(1.2137) | Loss 2.398939(2.326140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0474 | Time 1.2100(1.2134) | Loss 2.366711(2.328980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0475 | Time 1.2215(1.2140) | Loss 2.430979(2.336120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0476 | Time 1.2111(1.2138) | Loss 2.393230(2.340117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0477 | Time 1.2162(1.2139) | Loss 2.420163(2.345721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0478 | Time 1.2086(1.2136) | Loss 2.415552(2.350609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0479 | Time 1.2152(1.2137) | Loss 2.406338(2.354510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0480 | Time 1.2197(1.2141) | Loss 2.315638(2.351789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0481 | Time 1.2040(1.2134) | Loss 2.350288(2.351684) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0482 | Time 1.2131(1.2134) | Loss 2.315739(2.349168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0483 | Time 1.2086(1.2130) | Loss 2.329357(2.347781) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0484 | Time 1.2083(1.2127) | Loss 2.381931(2.350171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0485 | Time 1.1923(1.2113) | Loss 2.335933(2.349175) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0486 | Time 1.1947(1.2101) | Loss 2.361606(2.350045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0487 | Time 1.2157(1.2105) | Loss 2.283216(2.345367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0488 | Time 1.2065(1.2102) | Loss 2.244232(2.338287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0489 | Time 1.2086(1.2101) | Loss 2.274020(2.333789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0490 | Time 1.2133(1.2103) | Loss 2.279865(2.330014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0491 | Time 1.2048(1.2099) | Loss 2.313616(2.328866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0492 | Time 1.2662(1.2139) | Loss 2.285622(2.325839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0493 | Time 1.2113(1.2137) | Loss 2.306894(2.324513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0494 | Time 1.1975(1.2126) | Loss 2.329790(2.324882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0495 | Time 1.1952(1.2114) | Loss 2.352310(2.326802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0496 | Time 1.2188(1.2119) | Loss 2.325792(2.326732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0497 | Time 1.2007(1.2111) | Loss 2.376084(2.330186) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0498 | Time 1.1989(1.2102) | Loss 2.376953(2.333460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0499 | Time 1.2054(1.2099) | Loss 2.363098(2.335535) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0500 | Time 1.2093(1.2099) | Loss 2.372178(2.338100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 0500 | Test Loss 2.317382 | NFE 20
Skipping vis as data dimension is >2
Iter 0501 | Time 1.2137(1.2101) | Loss 2.294279(2.335032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0502 | Time 1.2017(1.2095) | Loss 2.364027(2.337062) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0503 | Time 1.2034(1.2091) | Loss 2.356127(2.338396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0504 | Time 1.2014(1.2086) | Loss 2.231646(2.330924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0505 | Time 1.1985(1.2079) | Loss 2.266114(2.326387) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0506 | Time 1.2155(1.2084) | Loss 2.286705(2.323609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0507 | Time 1.2099(1.2085) | Loss 2.233190(2.317280) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0508 | Time 1.2305(1.2100) | Loss 2.270561(2.314010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0509 | Time 1.1945(1.2090) | Loss 2.337554(2.315658) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0510 | Time 1.1863(1.2074) | Loss 2.262465(2.311934) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0511 | Time 1.1844(1.2058) | Loss 2.352525(2.314776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0512 | Time 1.2031(1.2056) | Loss 2.298732(2.313653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0513 | Time 1.1913(1.2046) | Loss 2.361203(2.316981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0514 | Time 1.1967(1.2040) | Loss 2.326762(2.317666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0515 | Time 1.2099(1.2044) | Loss 2.379047(2.321962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0516 | Time 1.2110(1.2049) | Loss 2.342633(2.323409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0517 | Time 1.2087(1.2052) | Loss 2.310965(2.322538) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0518 | Time 1.2027(1.2050) | Loss 2.297733(2.320802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0519 | Time 1.2050(1.2050) | Loss 2.399946(2.326342) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0520 | Time 1.2192(1.2060) | Loss 2.305849(2.324907) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0521 | Time 1.2413(1.2085) | Loss 2.333097(2.325481) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0522 | Time 1.2164(1.2090) | Loss 2.320638(2.325142) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0523 | Time 1.2328(1.2107) | Loss 2.291528(2.322789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0524 | Time 1.2204(1.2113) | Loss 2.333501(2.323539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0525 | Time 1.2217(1.2121) | Loss 2.341536(2.324798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0526 | Time 1.2120(1.2121) | Loss 2.316302(2.324204) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0527 | Time 1.2119(1.2121) | Loss 2.275882(2.320821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0528 | Time 1.2082(1.2118) | Loss 2.293061(2.318878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0529 | Time 1.2263(1.2128) | Loss 2.383154(2.323377) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0530 | Time 1.2053(1.2123) | Loss 2.360225(2.325957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0531 | Time 1.2002(1.2114) | Loss 2.293729(2.323701) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0532 | Time 1.2127(1.2115) | Loss 2.318962(2.323369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0533 | Time 1.2249(1.2125) | Loss 2.408141(2.329303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0534 | Time 1.2134(1.2125) | Loss 2.313238(2.328178) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0535 | Time 1.2052(1.2120) | Loss 2.237157(2.321807) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0536 | Time 1.2121(1.2120) | Loss 2.261413(2.317579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0537 | Time 1.2362(1.2137) | Loss 2.262706(2.313738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0538 | Time 1.2294(1.2148) | Loss 2.300860(2.312837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0539 | Time 1.2219(1.2153) | Loss 2.214526(2.305955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0540 | Time 1.2235(1.2159) | Loss 2.286572(2.304598) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0541 | Time 1.2172(1.2160) | Loss 2.320544(2.305714) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0542 | Time 1.2160(1.2160) | Loss 2.334117(2.307703) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0543 | Time 1.2207(1.2163) | Loss 2.339165(2.309905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0544 | Time 1.2229(1.2168) | Loss 2.286200(2.308246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0545 | Time 1.2363(1.2181) | Loss 2.332159(2.309920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0546 | Time 1.2212(1.2183) | Loss 2.259518(2.306391) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0547 | Time 1.2199(1.2185) | Loss 2.332239(2.308201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0548 | Time 1.1944(1.2168) | Loss 2.329796(2.309712) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0549 | Time 1.2103(1.2163) | Loss 2.403854(2.316302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0550 | Time 1.2669(1.2199) | Loss 2.322441(2.316732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0551 | Time 1.2104(1.2192) | Loss 2.316230(2.316697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0552 | Time 1.2121(1.2187) | Loss 2.271107(2.313506) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0553 | Time 1.2120(1.2182) | Loss 2.202789(2.305755) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0554 | Time 1.2053(1.2173) | Loss 2.283311(2.304184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0555 | Time 1.1918(1.2155) | Loss 2.248028(2.300253) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0556 | Time 1.1993(1.2144) | Loss 2.236286(2.295776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0557 | Time 1.2062(1.2138) | Loss 2.149751(2.285554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0558 | Time 1.2010(1.2129) | Loss 2.189008(2.278796) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0559 | Time 1.2178(1.2133) | Loss 2.256495(2.277235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0560 | Time 1.2079(1.2129) | Loss 2.180162(2.270440) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0561 | Time 1.2103(1.2127) | Loss 2.187965(2.264666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0562 | Time 1.2330(1.2141) | Loss 2.225777(2.261944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0563 | Time 1.2241(1.2148) | Loss 2.228638(2.259613) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0564 | Time 1.2073(1.2143) | Loss 2.179689(2.254018) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0565 | Time 1.2287(1.2153) | Loss 2.161713(2.247557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0566 | Time 1.2134(1.2152) | Loss 2.263860(2.248698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0567 | Time 1.2098(1.2148) | Loss 2.258872(2.249410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0568 | Time 1.2345(1.2162) | Loss 2.198002(2.245812) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0569 | Time 1.2041(1.2153) | Loss 2.204515(2.242921) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0570 | Time 1.2140(1.2152) | Loss 2.120807(2.234373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0571 | Time 1.2129(1.2151) | Loss 2.359184(2.243110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0572 | Time 1.2261(1.2158) | Loss 2.213163(2.241013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0573 | Time 1.2236(1.2164) | Loss 2.238056(2.240806) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0574 | Time 1.2257(1.2170) | Loss 2.272598(2.243032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0575 | Time 1.2126(1.2167) | Loss 2.334764(2.249453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0576 | Time 1.2067(1.2160) | Loss 2.232550(2.248270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0577 | Time 1.2363(1.2175) | Loss 2.228218(2.246866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0578 | Time 1.2133(1.2172) | Loss 2.285935(2.249601) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0579 | Time 1.2317(1.2182) | Loss 2.315103(2.254186) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0580 | Time 1.2172(1.2181) | Loss 2.193110(2.249911) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0581 | Time 1.2166(1.2180) | Loss 2.315464(2.254500) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0582 | Time 1.2268(1.2186) | Loss 2.259886(2.254877) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0583 | Time 1.2300(1.2194) | Loss 2.261770(2.255359) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0584 | Time 1.2287(1.2201) | Loss 2.263138(2.255904) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0585 | Time 1.2240(1.2203) | Loss 2.323731(2.260652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0586 | Time 1.2420(1.2219) | Loss 2.213857(2.257376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0587 | Time 1.2399(1.2231) | Loss 2.300814(2.260417) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0588 | Time 1.2319(1.2237) | Loss 2.302042(2.263330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0589 | Time 1.2236(1.2237) | Loss 2.201467(2.259000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0590 | Time 1.2268(1.2240) | Loss 2.277641(2.260305) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0591 | Time 1.2720(1.2273) | Loss 2.254415(2.259893) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0592 | Time 1.2232(1.2270) | Loss 2.220004(2.257100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0593 | Time 1.2199(1.2265) | Loss 2.266547(2.257762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0594 | Time 1.2309(1.2268) | Loss 2.197837(2.253567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0595 | Time 1.2476(1.2283) | Loss 2.194015(2.249398) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0596 | Time 1.1965(1.2261) | Loss 2.236650(2.248506) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0597 | Time 1.2197(1.2256) | Loss 2.145682(2.241308) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0598 | Time 1.2116(1.2246) | Loss 2.237809(2.241063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0599 | Time 1.2217(1.2244) | Loss 2.190110(2.237497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0600 | Time 1.1776(1.2211) | Loss 2.185674(2.233869) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 0600 | Test Loss 2.239247 | NFE 20
Skipping vis as data dimension is >2
Iter 0601 | Time 1.2019(1.2198) | Loss 2.176350(2.229843) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0602 | Time 1.2039(1.2187) | Loss 2.251224(2.231339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0603 | Time 1.2225(1.2190) | Loss 2.259612(2.233318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0604 | Time 1.2171(1.2188) | Loss 2.202516(2.231162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0605 | Time 1.2641(1.2220) | Loss 2.234052(2.231365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0606 | Time 1.2797(1.2260) | Loss 2.270654(2.234115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0607 | Time 1.2015(1.2243) | Loss 2.204685(2.232055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0608 | Time 1.2002(1.2226) | Loss 2.322248(2.238368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0609 | Time 1.2128(1.2219) | Loss 2.244308(2.238784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0610 | Time 1.2041(1.2207) | Loss 2.276051(2.241393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0611 | Time 1.2149(1.2203) | Loss 2.283928(2.244370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0612 | Time 1.2080(1.2194) | Loss 2.300471(2.248297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0613 | Time 1.2128(1.2190) | Loss 2.218155(2.246187) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0614 | Time 1.2159(1.2187) | Loss 2.233459(2.245296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0615 | Time 1.2155(1.2185) | Loss 2.237529(2.244753) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0616 | Time 1.2082(1.2178) | Loss 2.265265(2.246189) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0617 | Time 1.2051(1.2169) | Loss 2.298573(2.249855) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0618 | Time 1.1974(1.2155) | Loss 2.259668(2.250542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0619 | Time 1.2141(1.2154) | Loss 2.235966(2.249522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0620 | Time 1.2249(1.2161) | Loss 2.270607(2.250998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0621 | Time 1.2094(1.2156) | Loss 2.294346(2.254032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0622 | Time 1.2136(1.2155) | Loss 2.276548(2.255608) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0623 | Time 1.2102(1.2151) | Loss 2.258530(2.255813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0624 | Time 1.2059(1.2145) | Loss 2.267967(2.256664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0625 | Time 1.2039(1.2137) | Loss 2.267214(2.257402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0626 | Time 1.1935(1.2123) | Loss 2.243127(2.256403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0627 | Time 1.1873(1.2106) | Loss 2.278927(2.257980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0628 | Time 1.1876(1.2090) | Loss 2.218748(2.255233) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0629 | Time 1.2034(1.2086) | Loss 2.242531(2.254344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0630 | Time 1.2155(1.2090) | Loss 2.137790(2.246186) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0631 | Time 1.2153(1.2095) | Loss 2.163775(2.240417) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0632 | Time 1.2220(1.2104) | Loss 2.183203(2.236412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0633 | Time 1.2173(1.2108) | Loss 2.153778(2.230627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0634 | Time 1.2121(1.2109) | Loss 2.174323(2.226686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0635 | Time 1.2117(1.2110) | Loss 2.236437(2.227369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0636 | Time 1.2157(1.2113) | Loss 2.167763(2.223196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0637 | Time 1.2204(1.2120) | Loss 2.212013(2.222413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0638 | Time 1.2123(1.2120) | Loss 2.181239(2.219531) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0639 | Time 1.2731(1.2163) | Loss 2.223373(2.219800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0640 | Time 1.2777(1.2206) | Loss 2.161286(2.215704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0641 | Time 1.2559(1.2230) | Loss 2.151145(2.211185) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0642 | Time 1.2109(1.2222) | Loss 2.065664(2.200998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0643 | Time 1.2354(1.2231) | Loss 2.150566(2.197468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0644 | Time 1.2094(1.2221) | Loss 2.116813(2.191822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0645 | Time 1.2124(1.2215) | Loss 2.093796(2.184961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0646 | Time 1.2158(1.2211) | Loss 2.122827(2.180611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0647 | Time 1.2068(1.2201) | Loss 2.144371(2.178074) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0648 | Time 1.2265(1.2205) | Loss 2.171404(2.177607) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0649 | Time 1.2067(1.2196) | Loss 2.178847(2.177694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0650 | Time 1.2210(1.2197) | Loss 2.200316(2.179278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0651 | Time 1.1994(1.2182) | Loss 2.151418(2.177328) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0652 | Time 1.1961(1.2167) | Loss 2.226899(2.180798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0653 | Time 1.2227(1.2171) | Loss 2.205056(2.182496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0654 | Time 1.2123(1.2168) | Loss 2.255866(2.187632) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0655 | Time 1.1937(1.2152) | Loss 2.170121(2.186406) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0656 | Time 1.1933(1.2136) | Loss 2.273991(2.192537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0657 | Time 1.2031(1.2129) | Loss 2.153345(2.189793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0658 | Time 1.2830(1.2178) | Loss 2.137057(2.186102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0659 | Time 1.2178(1.2178) | Loss 2.137923(2.182729) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0660 | Time 1.2260(1.2184) | Loss 2.186119(2.182967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0661 | Time 1.2080(1.2176) | Loss 2.091640(2.176574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0662 | Time 1.2157(1.2175) | Loss 2.113252(2.172141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0663 | Time 1.2161(1.2174) | Loss 2.103468(2.167334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0664 | Time 1.2044(1.2165) | Loss 2.142558(2.165600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0665 | Time 1.2812(1.2210) | Loss 2.173624(2.166162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0666 | Time 1.2267(1.2214) | Loss 2.161534(2.165838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0667 | Time 1.2177(1.2212) | Loss 2.198739(2.168141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0668 | Time 1.2370(1.2223) | Loss 2.224437(2.172081) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0669 | Time 1.2146(1.2217) | Loss 2.128755(2.169049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0670 | Time 1.2478(1.2236) | Loss 2.200181(2.171228) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0671 | Time 1.2097(1.2226) | Loss 2.205560(2.173631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0672 | Time 1.2062(1.2214) | Loss 2.229126(2.177516) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0673 | Time 1.2333(1.2223) | Loss 2.115583(2.173180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0674 | Time 1.2577(1.2247) | Loss 2.159823(2.172245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0675 | Time 1.2444(1.2261) | Loss 2.137286(2.169798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0676 | Time 1.2143(1.2253) | Loss 2.187742(2.171054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0677 | Time 1.2174(1.2247) | Loss 2.201825(2.173208) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0678 | Time 1.1961(1.2227) | Loss 2.193039(2.174596) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0679 | Time 1.2021(1.2213) | Loss 2.179639(2.174949) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0680 | Time 1.2063(1.2202) | Loss 2.216372(2.177849) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0681 | Time 1.2060(1.2193) | Loss 2.130537(2.174537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0682 | Time 1.2099(1.2186) | Loss 2.119355(2.170674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0683 | Time 1.2151(1.2184) | Loss 2.151640(2.169342) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0684 | Time 1.2301(1.2192) | Loss 2.140207(2.167303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0685 | Time 1.2475(1.2212) | Loss 2.158819(2.166709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0686 | Time 1.2347(1.2221) | Loss 2.161360(2.166334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0687 | Time 1.2255(1.2223) | Loss 2.140603(2.164533) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0688 | Time 1.2124(1.2216) | Loss 2.105386(2.160393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0689 | Time 1.2021(1.2203) | Loss 2.234466(2.165578) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0690 | Time 1.2024(1.2190) | Loss 2.163127(2.165406) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0691 | Time 1.1956(1.2174) | Loss 2.167210(2.165533) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0692 | Time 1.2040(1.2164) | Loss 2.120812(2.162402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0693 | Time 1.2099(1.2160) | Loss 2.144021(2.161115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0694 | Time 1.1947(1.2145) | Loss 2.100925(2.156902) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0695 | Time 1.2114(1.2143) | Loss 2.101471(2.153022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0696 | Time 1.2048(1.2136) | Loss 2.184860(2.155251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0697 | Time 1.2190(1.2140) | Loss 2.140870(2.154244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0698 | Time 1.2037(1.2133) | Loss 2.131249(2.152634) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0699 | Time 1.2255(1.2141) | Loss 2.145639(2.152145) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0700 | Time 1.2048(1.2135) | Loss 2.113882(2.149466) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 0700 | Test Loss 2.201807 | NFE 20
Skipping vis as data dimension is >2
Iter 0701 | Time 1.2120(1.2134) | Loss 2.168889(2.150826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0702 | Time 1.2147(1.2135) | Loss 2.228348(2.156252) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0703 | Time 1.2005(1.2126) | Loss 2.198967(2.159242) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0704 | Time 1.2123(1.2125) | Loss 2.272517(2.167172) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0705 | Time 1.1997(1.2116) | Loss 2.242442(2.172441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0706 | Time 1.2195(1.2122) | Loss 2.271902(2.179403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0707 | Time 1.2269(1.2132) | Loss 2.161602(2.178157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0708 | Time 1.2528(1.2160) | Loss 2.245085(2.182842) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0709 | Time 1.2262(1.2167) | Loss 2.161302(2.181334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0710 | Time 1.2195(1.2169) | Loss 2.229290(2.184691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0711 | Time 1.2370(1.2183) | Loss 2.227311(2.187674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0712 | Time 1.2104(1.2177) | Loss 2.151303(2.185128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0713 | Time 1.2129(1.2174) | Loss 2.227315(2.188081) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0714 | Time 1.2238(1.2179) | Loss 2.138759(2.184629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0715 | Time 1.2227(1.2182) | Loss 2.196989(2.185494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0716 | Time 1.2129(1.2178) | Loss 2.122236(2.181066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0717 | Time 1.2352(1.2190) | Loss 2.094963(2.175039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0718 | Time 1.2305(1.2198) | Loss 2.162161(2.174137) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0719 | Time 1.2251(1.2202) | Loss 2.210127(2.176657) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0720 | Time 1.2125(1.2197) | Loss 2.176656(2.176656) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0721 | Time 1.3040(1.2256) | Loss 2.175712(2.176590) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0722 | Time 1.2222(1.2253) | Loss 2.168225(2.176005) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0723 | Time 1.2273(1.2255) | Loss 2.308480(2.185278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0724 | Time 1.2763(1.2290) | Loss 2.162602(2.183691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0725 | Time 1.2182(1.2283) | Loss 2.144405(2.180941) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0726 | Time 1.2140(1.2273) | Loss 2.176462(2.180627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0727 | Time 1.2103(1.2261) | Loss 2.178349(2.180468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0728 | Time 1.2154(1.2253) | Loss 2.091576(2.174245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0729 | Time 1.2104(1.2243) | Loss 2.209456(2.176710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0730 | Time 1.2112(1.2234) | Loss 2.183561(2.177190) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0731 | Time 1.2143(1.2228) | Loss 2.189459(2.178048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0732 | Time 1.2052(1.2215) | Loss 2.176253(2.177923) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0733 | Time 1.2116(1.2208) | Loss 2.202556(2.179647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0734 | Time 1.2176(1.2206) | Loss 2.109761(2.174755) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0735 | Time 1.2101(1.2199) | Loss 2.142454(2.172494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0736 | Time 1.2164(1.2196) | Loss 2.152923(2.171124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0737 | Time 1.2167(1.2194) | Loss 2.063318(2.163578) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0738 | Time 1.2473(1.2214) | Loss 2.166335(2.163771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0739 | Time 1.2269(1.2217) | Loss 2.163708(2.163766) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0740 | Time 1.2280(1.2222) | Loss 2.113139(2.160222) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0741 | Time 1.2235(1.2223) | Loss 2.186775(2.162081) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0742 | Time 1.2181(1.2220) | Loss 2.127828(2.159683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0743 | Time 1.2185(1.2217) | Loss 2.148855(2.158925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0744 | Time 1.2143(1.2212) | Loss 2.195607(2.161493) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0745 | Time 1.2161(1.2209) | Loss 2.118170(2.158460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0746 | Time 1.2046(1.2197) | Loss 2.120090(2.155775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0747 | Time 1.2094(1.2190) | Loss 2.160872(2.156131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0748 | Time 1.2178(1.2189) | Loss 2.177821(2.157650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0749 | Time 1.2107(1.2183) | Loss 2.104641(2.153939) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0750 | Time 1.2002(1.2171) | Loss 2.168752(2.154976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0751 | Time 1.1916(1.2153) | Loss 2.154360(2.154933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0752 | Time 1.1933(1.2138) | Loss 2.171214(2.156072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0753 | Time 1.2334(1.2151) | Loss 2.224262(2.160846) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0754 | Time 1.1987(1.2140) | Loss 2.132066(2.158831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0755 | Time 1.2044(1.2133) | Loss 2.124855(2.156453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0756 | Time 1.2040(1.2127) | Loss 2.149132(2.155940) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0757 | Time 1.2087(1.2124) | Loss 2.170246(2.156942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0758 | Time 1.2183(1.2128) | Loss 2.181315(2.158648) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0759 | Time 1.2158(1.2130) | Loss 2.063786(2.152008) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0760 | Time 1.2055(1.2125) | Loss 2.104562(2.148686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0761 | Time 1.2159(1.2127) | Loss 2.127725(2.147219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0762 | Time 1.1932(1.2113) | Loss 2.119958(2.145311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0763 | Time 1.2265(1.2124) | Loss 2.109393(2.142797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0764 | Time 1.2042(1.2118) | Loss 2.194206(2.146395) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0765 | Time 1.2382(1.2137) | Loss 2.113302(2.144079) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0766 | Time 1.2163(1.2139) | Loss 2.094086(2.140579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0767 | Time 1.2227(1.2145) | Loss 2.117210(2.138943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0768 | Time 1.2821(1.2192) | Loss 2.107674(2.136754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0769 | Time 1.2231(1.2195) | Loss 2.211052(2.141955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0770 | Time 1.1999(1.2181) | Loss 2.131277(2.141208) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0771 | Time 1.2067(1.2173) | Loss 2.117863(2.139574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0772 | Time 1.2020(1.2162) | Loss 2.060023(2.134005) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0773 | Time 1.2083(1.2157) | Loss 2.095690(2.131323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0774 | Time 1.2081(1.2152) | Loss 2.050591(2.125672) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0775 | Time 1.1844(1.2130) | Loss 2.128333(2.125858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0776 | Time 1.1955(1.2118) | Loss 2.074802(2.122284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0777 | Time 1.1924(1.2104) | Loss 2.073820(2.118892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0778 | Time 1.1922(1.2091) | Loss 1.993167(2.110091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0779 | Time 1.2052(1.2089) | Loss 2.076345(2.107729) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0780 | Time 1.2060(1.2087) | Loss 2.155673(2.111085) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0781 | Time 1.1960(1.2078) | Loss 2.118399(2.111597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0782 | Time 1.1970(1.2070) | Loss 2.145767(2.113989) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0783 | Time 1.2046(1.2069) | Loss 2.150019(2.116511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0784 | Time 1.2120(1.2072) | Loss 2.182031(2.121097) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0785 | Time 1.2175(1.2079) | Loss 2.174649(2.124846) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0786 | Time 1.2197(1.2088) | Loss 2.099717(2.123087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0787 | Time 1.2171(1.2093) | Loss 2.119125(2.122809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0788 | Time 1.1995(1.2087) | Loss 2.103646(2.121468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0789 | Time 1.1991(1.2080) | Loss 2.078820(2.118483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0790 | Time 1.1799(1.2060) | Loss 2.058074(2.114254) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0791 | Time 1.1790(1.2041) | Loss 2.106846(2.113736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0792 | Time 1.1894(1.2031) | Loss 2.136644(2.115339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0793 | Time 1.2291(1.2049) | Loss 2.112248(2.115123) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0794 | Time 1.1982(1.2044) | Loss 2.176899(2.119447) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0795 | Time 1.2214(1.2056) | Loss 2.131229(2.120272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0796 | Time 1.2614(1.2095) | Loss 2.150537(2.122390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0797 | Time 1.2815(1.2146) | Loss 2.152066(2.124468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0798 | Time 1.2740(1.2187) | Loss 2.128324(2.124738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0799 | Time 1.2307(1.2196) | Loss 2.173887(2.128178) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0800 | Time 1.2097(1.2189) | Loss 2.143453(2.129247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 0800 | Test Loss 2.147875 | NFE 20
Skipping vis as data dimension is >2
Iter 0801 | Time 1.2102(1.2183) | Loss 2.169363(2.132055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0802 | Time 1.2096(1.2177) | Loss 2.177525(2.135238) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0803 | Time 1.2104(1.2172) | Loss 2.175188(2.138035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0804 | Time 1.2095(1.2166) | Loss 2.169125(2.140211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0805 | Time 1.2082(1.2160) | Loss 2.096327(2.137139) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0806 | Time 1.2240(1.2166) | Loss 2.100969(2.134607) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0807 | Time 1.2145(1.2164) | Loss 2.032076(2.127430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0808 | Time 1.2076(1.2158) | Loss 2.064403(2.123018) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0809 | Time 1.2165(1.2159) | Loss 2.071478(2.119410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0810 | Time 1.2000(1.2148) | Loss 2.076159(2.116383) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0811 | Time 1.1990(1.2137) | Loss 2.131945(2.117472) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0812 | Time 1.2197(1.2141) | Loss 2.128951(2.118276) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0813 | Time 1.2276(1.2150) | Loss 2.120799(2.118452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0814 | Time 1.2137(1.2149) | Loss 2.108998(2.117790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0815 | Time 1.2100(1.2146) | Loss 2.091043(2.115918) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0816 | Time 1.2194(1.2149) | Loss 2.068926(2.112629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0817 | Time 1.2078(1.2144) | Loss 2.150423(2.115274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0818 | Time 1.2075(1.2139) | Loss 2.165711(2.118805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0819 | Time 1.2263(1.2148) | Loss 2.112982(2.118397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0820 | Time 1.2062(1.2142) | Loss 2.146560(2.120369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0821 | Time 1.2153(1.2143) | Loss 2.203837(2.126211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0822 | Time 1.2097(1.2140) | Loss 2.122095(2.125923) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0823 | Time 1.2002(1.2130) | Loss 2.120258(2.125527) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0824 | Time 1.1954(1.2118) | Loss 2.185781(2.129745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0825 | Time 1.2088(1.2115) | Loss 2.114178(2.128655) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0826 | Time 1.2029(1.2109) | Loss 2.078142(2.125119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0827 | Time 1.1893(1.2094) | Loss 2.065851(2.120970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0828 | Time 1.1900(1.2081) | Loss 2.145942(2.122718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0829 | Time 1.1970(1.2073) | Loss 2.115540(2.122216) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0830 | Time 1.2025(1.2070) | Loss 2.076631(2.119025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0831 | Time 1.1894(1.2057) | Loss 2.112681(2.118581) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0832 | Time 1.1916(1.2047) | Loss 2.152350(2.120945) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0833 | Time 1.1940(1.2040) | Loss 2.153529(2.123226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0834 | Time 1.2010(1.2038) | Loss 2.141770(2.124524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0835 | Time 1.1926(1.2030) | Loss 2.110079(2.123513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0836 | Time 1.2578(1.2068) | Loss 2.132630(2.124151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0837 | Time 1.2259(1.2082) | Loss 2.145133(2.125620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0838 | Time 1.2319(1.2098) | Loss 2.104347(2.124131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0839 | Time 1.2091(1.2098) | Loss 2.097097(2.122238) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0840 | Time 1.1998(1.2091) | Loss 2.169277(2.125531) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0841 | Time 1.1918(1.2079) | Loss 2.165388(2.128321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0842 | Time 1.2122(1.2082) | Loss 2.249160(2.136780) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0843 | Time 1.2114(1.2084) | Loss 2.141466(2.137108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0844 | Time 1.2048(1.2081) | Loss 2.143813(2.137577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0845 | Time 1.2020(1.2077) | Loss 2.142182(2.137899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0846 | Time 1.2272(1.2091) | Loss 2.180936(2.140912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0847 | Time 1.2276(1.2104) | Loss 2.249576(2.148518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0848 | Time 1.3066(1.2171) | Loss 2.178245(2.150599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0849 | Time 1.2101(1.2166) | Loss 2.229121(2.156096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0850 | Time 1.2082(1.2160) | Loss 2.195146(2.158829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0851 | Time 1.2054(1.2153) | Loss 2.143493(2.157756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0852 | Time 1.2099(1.2149) | Loss 2.186814(2.159790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0853 | Time 1.1808(1.2125) | Loss 2.221678(2.164122) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0854 | Time 1.1880(1.2108) | Loss 2.217103(2.167831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0855 | Time 1.1829(1.2089) | Loss 2.240194(2.172896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0856 | Time 1.1819(1.2070) | Loss 2.122032(2.169336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0857 | Time 1.1975(1.2063) | Loss 2.153826(2.168250) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0858 | Time 1.2223(1.2074) | Loss 2.127933(2.165428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0859 | Time 1.2193(1.2083) | Loss 2.153199(2.164572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0860 | Time 1.2266(1.2095) | Loss 2.112295(2.160912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0861 | Time 1.1935(1.2084) | Loss 2.078599(2.155150) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0862 | Time 1.2059(1.2082) | Loss 2.088872(2.150511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0863 | Time 1.2009(1.2077) | Loss 2.148259(2.150353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0864 | Time 1.2401(1.2100) | Loss 2.147578(2.150159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0865 | Time 1.2166(1.2105) | Loss 2.193213(2.153173) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0866 | Time 1.2242(1.2114) | Loss 2.169284(2.154301) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0867 | Time 1.2253(1.2124) | Loss 2.119508(2.151865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0868 | Time 1.2133(1.2125) | Loss 2.118091(2.149501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0869 | Time 1.2073(1.2121) | Loss 2.122315(2.147598) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0870 | Time 1.2171(1.2125) | Loss 2.149642(2.147741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0871 | Time 1.2114(1.2124) | Loss 2.022056(2.138943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0872 | Time 1.2037(1.2118) | Loss 2.113339(2.137151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0873 | Time 1.2223(1.2125) | Loss 2.086488(2.133604) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0874 | Time 1.2072(1.2121) | Loss 2.065774(2.128856) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0875 | Time 1.2462(1.2145) | Loss 2.105003(2.127187) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0876 | Time 1.2050(1.2139) | Loss 2.129917(2.127378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0877 | Time 1.1926(1.2124) | Loss 2.099743(2.125443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0878 | Time 1.1982(1.2114) | Loss 2.182107(2.129410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0879 | Time 1.2094(1.2112) | Loss 2.124780(2.129086) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0880 | Time 1.2124(1.2113) | Loss 2.213809(2.135016) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0881 | Time 1.2000(1.2105) | Loss 2.147928(2.135920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0882 | Time 1.2138(1.2108) | Loss 2.169838(2.138294) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0883 | Time 1.1956(1.2097) | Loss 2.199149(2.142554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0884 | Time 1.2012(1.2091) | Loss 2.264044(2.151059) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0885 | Time 1.1916(1.2079) | Loss 2.174720(2.152715) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0886 | Time 1.1903(1.2066) | Loss 2.228303(2.158006) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0887 | Time 1.2061(1.2066) | Loss 2.203869(2.161216) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0888 | Time 1.1906(1.2055) | Loss 2.168752(2.161744) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0889 | Time 1.1886(1.2043) | Loss 2.095271(2.157091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0890 | Time 1.2008(1.2041) | Loss 2.170650(2.158040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0891 | Time 1.1983(1.2037) | Loss 2.132255(2.156235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0892 | Time 1.1863(1.2024) | Loss 2.220758(2.160752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0893 | Time 1.1956(1.2020) | Loss 2.137957(2.159156) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0894 | Time 1.2039(1.2021) | Loss 2.132021(2.157257) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0895 | Time 1.1919(1.2014) | Loss 2.200030(2.160251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0896 | Time 1.2053(1.2017) | Loss 2.165120(2.160592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0897 | Time 1.2153(1.2026) | Loss 2.199535(2.163318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0898 | Time 1.2065(1.2029) | Loss 2.154606(2.162708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0899 | Time 1.2087(1.2033) | Loss 2.137656(2.160954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0900 | Time 1.2034(1.2033) | Loss 2.187797(2.162833) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 0900 | Test Loss 2.177109 | NFE 20
Skipping vis as data dimension is >2
Iter 0901 | Time 1.2238(1.2047) | Loss 2.096252(2.158172) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0902 | Time 1.2073(1.2049) | Loss 2.147489(2.157425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0903 | Time 1.2055(1.2050) | Loss 2.102329(2.153568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0904 | Time 1.2211(1.2061) | Loss 2.183326(2.155651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0905 | Time 1.2162(1.2068) | Loss 2.118946(2.153082) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0906 | Time 1.2168(1.2075) | Loss 2.051030(2.145938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0907 | Time 1.2123(1.2078) | Loss 2.062849(2.140122) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0908 | Time 1.2033(1.2075) | Loss 2.100764(2.137367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0909 | Time 1.2014(1.2071) | Loss 2.113438(2.135692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0910 | Time 1.1963(1.2063) | Loss 2.164333(2.137697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0911 | Time 1.1972(1.2057) | Loss 2.114238(2.136054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0912 | Time 1.1965(1.2050) | Loss 2.148909(2.136954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0913 | Time 1.2111(1.2055) | Loss 2.100262(2.134386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0914 | Time 1.1974(1.2049) | Loss 2.084341(2.130883) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0915 | Time 1.2109(1.2053) | Loss 2.076005(2.127041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0916 | Time 1.2097(1.2056) | Loss 2.116374(2.126295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0917 | Time 1.1944(1.2048) | Loss 2.147427(2.127774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0918 | Time 1.1999(1.2045) | Loss 2.108587(2.126431) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0919 | Time 1.1983(1.2041) | Loss 2.155405(2.128459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0920 | Time 1.1958(1.2035) | Loss 2.135113(2.128925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0921 | Time 1.2048(1.2036) | Loss 2.171589(2.131911) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0922 | Time 1.1990(1.2033) | Loss 2.167072(2.134373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0923 | Time 1.1919(1.2025) | Loss 2.197623(2.138800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0924 | Time 1.1949(1.2019) | Loss 2.161490(2.140388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0925 | Time 1.2062(1.2022) | Loss 2.104877(2.137903) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0926 | Time 1.1977(1.2019) | Loss 2.041855(2.131179) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0927 | Time 1.2055(1.2022) | Loss 2.168005(2.133757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0928 | Time 1.2204(1.2034) | Loss 2.146963(2.134681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0929 | Time 1.1886(1.2024) | Loss 2.142253(2.135211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0930 | Time 1.1957(1.2019) | Loss 2.203159(2.139968) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0931 | Time 1.2032(1.2020) | Loss 2.191650(2.143586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0932 | Time 1.2097(1.2026) | Loss 2.173703(2.145694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0933 | Time 1.2315(1.2046) | Loss 2.209219(2.150141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0934 | Time 1.2083(1.2049) | Loss 2.126974(2.148519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0935 | Time 1.2139(1.2055) | Loss 2.164612(2.149645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0936 | Time 1.2162(1.2062) | Loss 2.174363(2.151376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0937 | Time 1.2148(1.2068) | Loss 2.152803(2.151476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0938 | Time 1.2085(1.2070) | Loss 2.125583(2.149663) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0939 | Time 1.2024(1.2066) | Loss 2.086639(2.145251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0940 | Time 1.2702(1.2111) | Loss 2.107287(2.142594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0941 | Time 1.2757(1.2156) | Loss 2.093016(2.139123) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0942 | Time 1.2814(1.2202) | Loss 2.043312(2.132417) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0943 | Time 1.2716(1.2238) | Loss 2.152027(2.133789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0944 | Time 1.2824(1.2279) | Loss 2.099318(2.131376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0945 | Time 1.2796(1.2315) | Loss 2.083506(2.128025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0946 | Time 1.2244(1.2310) | Loss 2.091888(2.125496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0947 | Time 1.2117(1.2297) | Loss 2.069058(2.121545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0948 | Time 1.2047(1.2279) | Loss 2.102533(2.120214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0949 | Time 1.1948(1.2256) | Loss 2.132082(2.121045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0950 | Time 1.1966(1.2236) | Loss 2.115877(2.120683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0951 | Time 1.1930(1.2214) | Loss 2.194843(2.125874) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0952 | Time 1.1955(1.2196) | Loss 2.089837(2.123352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0953 | Time 1.2602(1.2225) | Loss 2.150633(2.125261) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0954 | Time 1.2837(1.2268) | Loss 2.134437(2.125904) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0955 | Time 1.2672(1.2296) | Loss 2.167637(2.128825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0956 | Time 1.2866(1.2336) | Loss 2.113292(2.127738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0957 | Time 1.2916(1.2376) | Loss 2.063344(2.123230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0958 | Time 1.2735(1.2401) | Loss 2.168373(2.126390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0959 | Time 1.2389(1.2401) | Loss 2.078589(2.123044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0960 | Time 1.2299(1.2393) | Loss 2.185807(2.127437) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0961 | Time 1.2614(1.2409) | Loss 2.109882(2.126209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0962 | Time 1.2380(1.2407) | Loss 2.105008(2.124725) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0963 | Time 1.1985(1.2377) | Loss 2.115301(2.124065) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0964 | Time 1.2415(1.2380) | Loss 2.095354(2.122055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0965 | Time 1.2024(1.2355) | Loss 2.110025(2.121213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0966 | Time 1.2418(1.2359) | Loss 2.119869(2.121119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0967 | Time 1.2629(1.2378) | Loss 2.157953(2.123697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0968 | Time 1.2077(1.2357) | Loss 2.130295(2.124159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0969 | Time 1.1990(1.2331) | Loss 2.190525(2.128805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0970 | Time 1.2003(1.2308) | Loss 2.141600(2.129700) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0971 | Time 1.1982(1.2286) | Loss 2.214003(2.135602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0972 | Time 1.1843(1.2255) | Loss 2.196287(2.139850) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0973 | Time 1.1921(1.2231) | Loss 2.208736(2.144672) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0974 | Time 1.2049(1.2219) | Loss 2.157593(2.145576) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0975 | Time 1.1920(1.2198) | Loss 2.259245(2.153533) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0976 | Time 1.1976(1.2182) | Loss 2.211830(2.157614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0977 | Time 1.2361(1.2195) | Loss 2.205784(2.160986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0978 | Time 1.1974(1.2179) | Loss 2.167035(2.161409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0979 | Time 1.1915(1.2161) | Loss 2.107601(2.157643) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0980 | Time 1.2048(1.2153) | Loss 2.162838(2.158006) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0981 | Time 1.2272(1.2161) | Loss 2.124103(2.155633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0982 | Time 1.2278(1.2169) | Loss 2.086720(2.150809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0983 | Time 1.2339(1.2181) | Loss 2.069850(2.145142) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0984 | Time 1.2137(1.2178) | Loss 2.111845(2.142811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0985 | Time 1.2134(1.2175) | Loss 2.089821(2.139102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0986 | Time 1.2147(1.2173) | Loss 2.152564(2.140044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0987 | Time 1.2651(1.2207) | Loss 2.095896(2.136954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0988 | Time 1.2652(1.2238) | Loss 2.086572(2.133427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0989 | Time 1.2376(1.2247) | Loss 2.117376(2.132304) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0990 | Time 1.2064(1.2235) | Loss 2.065178(2.127605) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0991 | Time 1.1991(1.2218) | Loss 2.159595(2.129844) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0992 | Time 1.2002(1.2202) | Loss 2.069891(2.125647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0993 | Time 1.2092(1.2195) | Loss 2.039539(2.119620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0994 | Time 1.1949(1.2178) | Loss 2.098289(2.118127) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0995 | Time 1.1969(1.2163) | Loss 2.054858(2.113698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0996 | Time 1.2059(1.2156) | Loss 2.112147(2.113589) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0997 | Time 1.1983(1.2144) | Loss 2.121637(2.114153) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0998 | Time 1.2485(1.2168) | Loss 2.057002(2.110152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 0999 | Time 1.2039(1.2159) | Loss 2.106144(2.109872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1000 | Time 1.2052(1.2151) | Loss 2.148680(2.112588) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1000 | Test Loss 2.072493 | NFE 20
Skipping vis as data dimension is >2
Iter 1001 | Time 1.2011(1.2141) | Loss 2.088662(2.110913) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1002 | Time 1.2248(1.2149) | Loss 2.126623(2.112013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1003 | Time 1.2084(1.2144) | Loss 2.119725(2.112553) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1004 | Time 1.2274(1.2153) | Loss 2.142875(2.114675) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1005 | Time 1.2483(1.2176) | Loss 2.143546(2.116696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1006 | Time 1.2239(1.2181) | Loss 2.110403(2.116256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1007 | Time 1.2246(1.2185) | Loss 2.146310(2.118360) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1008 | Time 1.2457(1.2204) | Loss 2.115755(2.118177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1009 | Time 1.2136(1.2200) | Loss 2.154047(2.120688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1010 | Time 1.2102(1.2193) | Loss 2.190919(2.125604) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1011 | Time 1.2165(1.2191) | Loss 2.115028(2.124864) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1012 | Time 1.2369(1.2203) | Loss 2.129118(2.125162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1013 | Time 1.2038(1.2192) | Loss 2.091846(2.122830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1014 | Time 1.2153(1.2189) | Loss 2.086106(2.120259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1015 | Time 1.2054(1.2180) | Loss 2.125528(2.120628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1016 | Time 1.2079(1.2173) | Loss 2.151949(2.122820) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1017 | Time 1.2100(1.2168) | Loss 2.115669(2.122320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1018 | Time 1.2086(1.2162) | Loss 2.141625(2.123671) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1019 | Time 1.1984(1.2149) | Loss 2.153461(2.125756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1020 | Time 1.2149(1.2149) | Loss 2.169250(2.128801) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1021 | Time 1.2083(1.2145) | Loss 2.160758(2.131038) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1022 | Time 1.2132(1.2144) | Loss 2.091849(2.128295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1023 | Time 1.2452(1.2165) | Loss 2.173542(2.131462) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1024 | Time 1.2073(1.2159) | Loss 2.082899(2.128063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1025 | Time 1.2144(1.2158) | Loss 2.169968(2.130996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1026 | Time 1.2122(1.2155) | Loss 2.099659(2.128802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1027 | Time 1.2459(1.2177) | Loss 2.139966(2.129584) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1028 | Time 1.2036(1.2167) | Loss 2.096599(2.127275) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1029 | Time 1.1994(1.2155) | Loss 2.141771(2.128290) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1030 | Time 1.2067(1.2149) | Loss 2.168650(2.131115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1031 | Time 1.2093(1.2145) | Loss 2.152038(2.132580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1032 | Time 1.2148(1.2145) | Loss 2.051961(2.126936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1033 | Time 1.2218(1.2150) | Loss 2.114895(2.126093) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1034 | Time 1.1999(1.2140) | Loss 2.119250(2.125614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1035 | Time 1.2509(1.2165) | Loss 2.163803(2.128288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1036 | Time 1.2137(1.2163) | Loss 2.155084(2.130163) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1037 | Time 1.2272(1.2171) | Loss 2.038219(2.123727) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1038 | Time 1.2316(1.2181) | Loss 2.108252(2.122644) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1039 | Time 1.2224(1.2184) | Loss 2.109108(2.121696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1040 | Time 1.2124(1.2180) | Loss 2.178046(2.125641) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1041 | Time 1.2230(1.2183) | Loss 2.205302(2.131217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1042 | Time 1.2292(1.2191) | Loss 2.152054(2.132676) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1043 | Time 1.2190(1.2191) | Loss 2.076785(2.128763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1044 | Time 1.2236(1.2194) | Loss 2.065269(2.124319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1045 | Time 1.2179(1.2193) | Loss 2.176768(2.127990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1046 | Time 1.2072(1.2184) | Loss 2.112343(2.126895) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1047 | Time 1.2087(1.2178) | Loss 2.060807(2.122269) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1048 | Time 1.2113(1.2173) | Loss 2.106376(2.121156) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1049 | Time 1.2564(1.2201) | Loss 2.141126(2.122554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1050 | Time 1.2444(1.2218) | Loss 2.125997(2.122795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1051 | Time 1.2102(1.2209) | Loss 2.128730(2.123211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1052 | Time 1.2036(1.2197) | Loss 2.100900(2.121649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1053 | Time 1.2046(1.2187) | Loss 2.159520(2.124300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1054 | Time 1.1994(1.2173) | Loss 2.154096(2.126386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1055 | Time 1.1952(1.2158) | Loss 2.134482(2.126952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1056 | Time 1.2075(1.2152) | Loss 2.182478(2.130839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1057 | Time 1.2396(1.2169) | Loss 2.156366(2.132626) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1058 | Time 1.2204(1.2171) | Loss 2.153282(2.134072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1059 | Time 1.2008(1.2160) | Loss 2.150033(2.135189) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1060 | Time 1.2270(1.2168) | Loss 2.198097(2.139593) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1061 | Time 1.2130(1.2165) | Loss 2.091008(2.136192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1062 | Time 1.2517(1.2190) | Loss 2.105412(2.134037) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1063 | Time 1.2103(1.2184) | Loss 2.078134(2.130124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1064 | Time 1.2205(1.2185) | Loss 2.111253(2.128803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1065 | Time 1.2230(1.2188) | Loss 2.097944(2.126643) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1066 | Time 1.2183(1.2188) | Loss 2.160272(2.128997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1067 | Time 1.2330(1.2198) | Loss 2.145094(2.130124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1068 | Time 1.2282(1.2204) | Loss 2.072673(2.126102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1069 | Time 1.2179(1.2202) | Loss 2.148260(2.127653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1070 | Time 1.2097(1.2195) | Loss 2.173921(2.130892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1071 | Time 1.2168(1.2193) | Loss 2.171734(2.133751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1072 | Time 1.2199(1.2193) | Loss 2.223996(2.140068) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1073 | Time 1.2080(1.2185) | Loss 2.199053(2.144197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1074 | Time 1.2094(1.2179) | Loss 2.175177(2.146366) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1075 | Time 1.2048(1.2170) | Loss 2.162309(2.147482) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1076 | Time 1.2127(1.2167) | Loss 2.233197(2.153482) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1077 | Time 1.2198(1.2169) | Loss 2.172468(2.154811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1078 | Time 1.2181(1.2170) | Loss 2.215303(2.159045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1079 | Time 1.2168(1.2170) | Loss 2.220642(2.163357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1080 | Time 1.2282(1.2178) | Loss 2.195046(2.165575) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1081 | Time 1.2098(1.2172) | Loss 2.178982(2.166514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1082 | Time 1.2169(1.2172) | Loss 2.201118(2.168936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1083 | Time 1.2146(1.2170) | Loss 2.209852(2.171800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1084 | Time 1.2133(1.2167) | Loss 2.234113(2.176162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1085 | Time 1.2073(1.2161) | Loss 2.214919(2.178875) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1086 | Time 1.2147(1.2160) | Loss 2.267993(2.185113) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1087 | Time 1.2128(1.2158) | Loss 2.212626(2.187039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1088 | Time 1.2160(1.2158) | Loss 2.222389(2.189514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1089 | Time 1.2247(1.2164) | Loss 2.226887(2.192130) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1090 | Time 1.2085(1.2159) | Loss 2.221026(2.194153) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1091 | Time 1.2141(1.2157) | Loss 2.157176(2.191564) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1092 | Time 1.2163(1.2158) | Loss 2.227649(2.194090) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1093 | Time 1.2230(1.2163) | Loss 2.179035(2.193036) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1094 | Time 1.2491(1.2186) | Loss 2.229944(2.195620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1095 | Time 1.2296(1.2193) | Loss 2.182722(2.194717) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1096 | Time 1.2390(1.2207) | Loss 2.212567(2.195967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1097 | Time 1.2443(1.2224) | Loss 2.142121(2.192197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1098 | Time 1.2198(1.2222) | Loss 2.153744(2.189506) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1099 | Time 1.2255(1.2224) | Loss 2.142824(2.186238) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1100 | Time 1.2350(1.2233) | Loss 2.148604(2.183604) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1100 | Test Loss 2.085531 | NFE 20
Skipping vis as data dimension is >2
Iter 1101 | Time 1.2680(1.2264) | Loss 2.231928(2.186986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1102 | Time 1.2279(1.2265) | Loss 2.148405(2.184286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1103 | Time 1.2176(1.2259) | Loss 2.154476(2.182199) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1104 | Time 1.2140(1.2251) | Loss 2.174255(2.181643) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1105 | Time 1.2257(1.2251) | Loss 2.152869(2.179629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1106 | Time 1.2218(1.2249) | Loss 2.152911(2.177758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1107 | Time 1.2019(1.2233) | Loss 2.152376(2.175982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1108 | Time 1.2051(1.2220) | Loss 2.187869(2.176814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1109 | Time 1.2028(1.2207) | Loss 2.203324(2.178669) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1110 | Time 1.1998(1.2192) | Loss 2.176381(2.178509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1111 | Time 1.1994(1.2178) | Loss 2.203418(2.180253) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1112 | Time 1.1946(1.2162) | Loss 2.224160(2.183326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1113 | Time 1.2121(1.2159) | Loss 2.178602(2.182996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1114 | Time 1.1940(1.2144) | Loss 2.291957(2.190623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1115 | Time 1.2322(1.2156) | Loss 2.184012(2.190160) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1116 | Time 1.2078(1.2151) | Loss 2.209615(2.191522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1117 | Time 1.2001(1.2140) | Loss 2.152605(2.188798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1118 | Time 1.2009(1.2131) | Loss 2.136859(2.185162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1119 | Time 1.2128(1.2131) | Loss 2.181645(2.184916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1120 | Time 1.1987(1.2121) | Loss 2.254218(2.189767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1121 | Time 1.2063(1.2117) | Loss 2.136142(2.186013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1122 | Time 1.2200(1.2122) | Loss 2.179844(2.185581) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1123 | Time 1.2201(1.2128) | Loss 2.129807(2.181677) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1124 | Time 1.2019(1.2120) | Loss 2.225669(2.184757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1125 | Time 1.2009(1.2113) | Loss 2.126766(2.180697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1126 | Time 1.1971(1.2103) | Loss 2.204113(2.182336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1127 | Time 1.2035(1.2098) | Loss 2.211747(2.184395) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1128 | Time 1.2085(1.2097) | Loss 2.181934(2.184223) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1129 | Time 1.1971(1.2088) | Loss 2.213402(2.186265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1130 | Time 1.1975(1.2080) | Loss 2.163511(2.184673) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1131 | Time 1.2037(1.2077) | Loss 2.205869(2.186156) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1132 | Time 1.2032(1.2074) | Loss 2.155941(2.184041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1133 | Time 1.2113(1.2077) | Loss 2.179897(2.183751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1134 | Time 1.1966(1.2069) | Loss 2.188285(2.184069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1135 | Time 1.2038(1.2067) | Loss 2.169709(2.183063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1136 | Time 1.1982(1.2061) | Loss 2.228766(2.186263) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1137 | Time 1.2154(1.2067) | Loss 2.120872(2.181685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1138 | Time 1.2215(1.2078) | Loss 2.115237(2.177034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1139 | Time 1.2094(1.2079) | Loss 2.130404(2.173770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1140 | Time 1.1989(1.2073) | Loss 2.181023(2.174277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1141 | Time 1.2241(1.2084) | Loss 2.179582(2.174649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1142 | Time 1.1950(1.2075) | Loss 2.120658(2.170869) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1143 | Time 1.2320(1.2092) | Loss 2.147903(2.169262) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1144 | Time 1.2167(1.2097) | Loss 2.201319(2.171506) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1145 | Time 1.2095(1.2097) | Loss 2.165617(2.171094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1146 | Time 1.2174(1.2103) | Loss 2.234435(2.175527) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1147 | Time 1.2214(1.2110) | Loss 2.203866(2.177511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1148 | Time 1.2221(1.2118) | Loss 2.210570(2.179825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1149 | Time 1.2153(1.2121) | Loss 2.205618(2.181631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1150 | Time 1.2006(1.2113) | Loss 2.237274(2.185526) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1151 | Time 1.2104(1.2112) | Loss 2.198293(2.186419) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1152 | Time 1.2132(1.2113) | Loss 2.279280(2.192920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1153 | Time 1.2046(1.2109) | Loss 2.271392(2.198413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1154 | Time 1.1950(1.2098) | Loss 2.244923(2.201668) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1155 | Time 1.2097(1.2097) | Loss 2.215869(2.202663) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1156 | Time 1.2160(1.2102) | Loss 2.227653(2.204412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1157 | Time 1.2182(1.2107) | Loss 2.223001(2.205713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1158 | Time 1.2146(1.2110) | Loss 2.100125(2.198322) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1159 | Time 1.2171(1.2114) | Loss 2.124730(2.193171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1160 | Time 1.2291(1.2127) | Loss 2.128841(2.188667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1161 | Time 1.2257(1.2136) | Loss 2.265435(2.194041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1162 | Time 1.2148(1.2137) | Loss 2.243474(2.197501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1163 | Time 1.2030(1.2129) | Loss 2.246603(2.200939) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1164 | Time 1.1990(1.2120) | Loss 2.119813(2.195260) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1165 | Time 1.2064(1.2116) | Loss 2.150783(2.192146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1166 | Time 1.2125(1.2116) | Loss 2.193837(2.192265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1167 | Time 1.2116(1.2116) | Loss 2.207908(2.193360) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1168 | Time 1.1943(1.2104) | Loss 2.141729(2.189746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1169 | Time 1.2474(1.2130) | Loss 2.201672(2.190580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1170 | Time 1.2602(1.2163) | Loss 2.269385(2.196097) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1171 | Time 1.2529(1.2189) | Loss 2.253911(2.200144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1172 | Time 1.2255(1.2193) | Loss 2.263166(2.204555) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1173 | Time 1.2102(1.2187) | Loss 2.207097(2.204733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1174 | Time 1.2272(1.2193) | Loss 2.279365(2.209957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1175 | Time 1.2157(1.2190) | Loss 2.220596(2.210702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1176 | Time 1.2135(1.2187) | Loss 2.269153(2.214794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1177 | Time 1.2120(1.2182) | Loss 2.280512(2.219394) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1178 | Time 1.2140(1.2179) | Loss 2.239887(2.220829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1179 | Time 1.2390(1.2194) | Loss 2.269892(2.224263) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1180 | Time 1.2119(1.2189) | Loss 2.299293(2.229515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1181 | Time 1.1982(1.2174) | Loss 2.345981(2.237668) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1182 | Time 1.2093(1.2168) | Loss 2.265339(2.239605) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1183 | Time 1.1969(1.2154) | Loss 2.269745(2.241715) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1184 | Time 1.2182(1.2156) | Loss 2.268054(2.243558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1185 | Time 1.2262(1.2164) | Loss 2.358223(2.251585) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1186 | Time 1.1990(1.2152) | Loss 2.354201(2.258768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1187 | Time 1.2177(1.2153) | Loss 2.344624(2.264778) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1188 | Time 1.2054(1.2146) | Loss 2.297884(2.267095) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1189 | Time 1.2200(1.2150) | Loss 2.376711(2.274768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1190 | Time 1.2143(1.2150) | Loss 2.311376(2.277331) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1191 | Time 1.2141(1.2149) | Loss 2.326152(2.280748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1192 | Time 1.2084(1.2145) | Loss 2.331295(2.284287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1193 | Time 1.2099(1.2141) | Loss 2.332814(2.287684) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1194 | Time 1.2149(1.2142) | Loss 2.266889(2.286228) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1195 | Time 1.2270(1.2151) | Loss 2.329853(2.289282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1196 | Time 1.2102(1.2148) | Loss 2.259277(2.287181) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1197 | Time 1.2434(1.2168) | Loss 2.279614(2.286652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1198 | Time 1.2405(1.2184) | Loss 2.187817(2.279733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1199 | Time 1.2308(1.2193) | Loss 2.280636(2.279797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1200 | Time 1.2418(1.2209) | Loss 2.248113(2.277579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1200 | Test Loss 2.227995 | NFE 20
Skipping vis as data dimension is >2
Iter 1201 | Time 1.2237(1.2211) | Loss 2.213867(2.273119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1202 | Time 1.2367(1.2222) | Loss 2.256739(2.271972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1203 | Time 1.2848(1.2265) | Loss 2.218923(2.268259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1204 | Time 1.2405(1.2275) | Loss 2.269109(2.268318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1205 | Time 1.2064(1.2260) | Loss 2.213720(2.264496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1206 | Time 1.2205(1.2257) | Loss 2.158296(2.257062) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1207 | Time 1.2029(1.2241) | Loss 2.304977(2.260416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1208 | Time 1.2271(1.2243) | Loss 2.238871(2.258908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1209 | Time 1.2211(1.2240) | Loss 2.200862(2.254845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1210 | Time 1.2084(1.2230) | Loss 2.214860(2.252046) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1211 | Time 1.2159(1.2225) | Loss 2.152287(2.245063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1212 | Time 1.1970(1.2207) | Loss 2.211854(2.242738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1213 | Time 1.2216(1.2207) | Loss 2.211495(2.240551) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1214 | Time 1.2226(1.2209) | Loss 2.162623(2.235096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1215 | Time 1.2219(1.2209) | Loss 2.191863(2.232070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1216 | Time 1.2270(1.2214) | Loss 2.178219(2.228300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1217 | Time 1.2095(1.2205) | Loss 2.264828(2.230857) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1218 | Time 1.2086(1.2197) | Loss 2.254918(2.232542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1219 | Time 1.2222(1.2199) | Loss 2.247551(2.233592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1220 | Time 1.2056(1.2189) | Loss 2.203600(2.231493) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1221 | Time 1.2100(1.2183) | Loss 2.267839(2.234037) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1222 | Time 1.2073(1.2175) | Loss 2.290579(2.237995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1223 | Time 1.2149(1.2173) | Loss 2.305058(2.242689) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1224 | Time 1.2111(1.2169) | Loss 2.246700(2.242970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1225 | Time 1.2079(1.2162) | Loss 2.226511(2.241818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1226 | Time 1.2009(1.2152) | Loss 2.257849(2.242940) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1227 | Time 1.2015(1.2142) | Loss 2.287900(2.246087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1228 | Time 1.2549(1.2171) | Loss 2.258363(2.246947) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1229 | Time 1.3013(1.2230) | Loss 2.262130(2.248010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1230 | Time 1.2204(1.2228) | Loss 2.285110(2.250607) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1231 | Time 1.2327(1.2235) | Loss 2.311148(2.254844) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1232 | Time 1.2368(1.2244) | Loss 2.246811(2.254282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1233 | Time 1.2547(1.2265) | Loss 2.294833(2.257121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1234 | Time 1.2601(1.2289) | Loss 2.310925(2.260887) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1235 | Time 1.2540(1.2306) | Loss 2.230117(2.258733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1236 | Time 1.2658(1.2331) | Loss 2.311418(2.262421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1237 | Time 1.2174(1.2320) | Loss 2.318177(2.266324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1238 | Time 1.2295(1.2318) | Loss 2.280505(2.267317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1239 | Time 1.2194(1.2309) | Loss 2.268414(2.267393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1240 | Time 1.2221(1.2303) | Loss 2.314123(2.270664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1241 | Time 1.2148(1.2292) | Loss 2.318874(2.274039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1242 | Time 1.2442(1.2303) | Loss 2.325429(2.277636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1243 | Time 1.1948(1.2278) | Loss 2.299156(2.279143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1244 | Time 1.1956(1.2255) | Loss 2.309158(2.281244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1245 | Time 1.1968(1.2235) | Loss 2.276844(2.280936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1246 | Time 1.2053(1.2223) | Loss 2.332356(2.284535) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1247 | Time 1.2188(1.2220) | Loss 2.372884(2.290720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1248 | Time 1.1924(1.2199) | Loss 2.278741(2.289881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1249 | Time 1.1903(1.2179) | Loss 2.342999(2.293599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1250 | Time 1.2303(1.2187) | Loss 2.315285(2.295117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1251 | Time 1.1865(1.2165) | Loss 2.350296(2.298980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1252 | Time 1.2042(1.2156) | Loss 2.274182(2.297244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1253 | Time 1.2062(1.2150) | Loss 2.305393(2.297814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1254 | Time 1.2004(1.2139) | Loss 2.267443(2.295688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1255 | Time 1.2003(1.2130) | Loss 2.384958(2.301937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1256 | Time 1.1891(1.2113) | Loss 2.316689(2.302970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1257 | Time 1.2042(1.2108) | Loss 2.369535(2.307630) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1258 | Time 1.2180(1.2113) | Loss 2.375437(2.312376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1259 | Time 1.1983(1.2104) | Loss 2.361843(2.315839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1260 | Time 1.2066(1.2101) | Loss 2.311765(2.315554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1261 | Time 1.1865(1.2085) | Loss 2.312466(2.315337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1262 | Time 1.2035(1.2081) | Loss 2.318516(2.315560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1263 | Time 1.2055(1.2079) | Loss 2.415987(2.322590) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1264 | Time 1.1965(1.2071) | Loss 2.363228(2.325435) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1265 | Time 1.1981(1.2065) | Loss 2.384602(2.329576) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1266 | Time 1.1981(1.2059) | Loss 2.267489(2.325230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1267 | Time 1.1987(1.2054) | Loss 2.360001(2.327664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1268 | Time 1.2006(1.2051) | Loss 2.267756(2.323471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1269 | Time 1.1992(1.2047) | Loss 2.292581(2.321308) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1270 | Time 1.2206(1.2058) | Loss 2.244835(2.315955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1271 | Time 1.1982(1.2052) | Loss 2.366150(2.319469) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1272 | Time 1.1918(1.2043) | Loss 2.305022(2.318458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1273 | Time 1.2023(1.2042) | Loss 2.329231(2.319212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1274 | Time 1.2006(1.2039) | Loss 2.403689(2.325125) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1275 | Time 1.2041(1.2039) | Loss 2.313785(2.324331) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1276 | Time 1.1838(1.2025) | Loss 2.321970(2.324166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1277 | Time 1.1888(1.2016) | Loss 2.384555(2.328393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1278 | Time 1.2191(1.2028) | Loss 2.356944(2.330392) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1279 | Time 1.2058(1.2030) | Loss 2.417788(2.336509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1280 | Time 1.2063(1.2032) | Loss 2.351841(2.337583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1281 | Time 1.2024(1.2032) | Loss 2.273530(2.333099) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1282 | Time 1.2070(1.2034) | Loss 2.325444(2.332563) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1283 | Time 1.2125(1.2041) | Loss 2.325021(2.332035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1284 | Time 1.2022(1.2039) | Loss 2.284929(2.328738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1285 | Time 1.1984(1.2036) | Loss 2.290087(2.326032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1286 | Time 1.2095(1.2040) | Loss 2.226217(2.319045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1287 | Time 1.2014(1.2038) | Loss 2.264469(2.315225) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1288 | Time 1.1862(1.2026) | Loss 2.299557(2.314128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1289 | Time 1.1830(1.2012) | Loss 2.350499(2.316674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1290 | Time 1.1883(1.2003) | Loss 2.311226(2.316293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1291 | Time 1.1860(1.1993) | Loss 2.376792(2.320528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1292 | Time 1.1893(1.1986) | Loss 2.276674(2.317458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1293 | Time 1.2172(1.1999) | Loss 2.375606(2.321528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1294 | Time 1.2198(1.2013) | Loss 2.390910(2.326385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1295 | Time 1.1996(1.2012) | Loss 2.468657(2.336344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1296 | Time 1.2046(1.2014) | Loss 2.424948(2.342546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1297 | Time 1.1999(1.2013) | Loss 2.417568(2.347798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1298 | Time 1.2017(1.2013) | Loss 2.381427(2.350152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1299 | Time 1.2126(1.2021) | Loss 2.407267(2.354150) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1300 | Time 1.1996(1.2019) | Loss 2.417991(2.358619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1300 | Test Loss 2.401299 | NFE 20
Skipping vis as data dimension is >2
Iter 1301 | Time 1.1994(1.2018) | Loss 2.374873(2.359757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1302 | Time 1.2121(1.2025) | Loss 2.434261(2.364972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1303 | Time 1.1898(1.2016) | Loss 2.399677(2.367401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1304 | Time 1.2218(1.2030) | Loss 2.424343(2.371387) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1305 | Time 1.2174(1.2040) | Loss 2.379621(2.371964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1306 | Time 1.2226(1.2053) | Loss 2.371671(2.371943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1307 | Time 1.2206(1.2064) | Loss 2.387542(2.373035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1308 | Time 1.2003(1.2060) | Loss 2.399138(2.374862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1309 | Time 1.2068(1.2060) | Loss 2.371738(2.374643) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1310 | Time 1.1951(1.2052) | Loss 2.400578(2.376459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1311 | Time 1.1876(1.2040) | Loss 2.409669(2.378784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1312 | Time 1.2031(1.2039) | Loss 2.431846(2.382498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1313 | Time 1.1968(1.2034) | Loss 2.356693(2.380692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1314 | Time 1.1994(1.2032) | Loss 2.452749(2.385736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1315 | Time 1.1895(1.2022) | Loss 2.426198(2.388568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1316 | Time 1.1917(1.2015) | Loss 2.477890(2.394821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1317 | Time 1.1981(1.2012) | Loss 2.393165(2.394705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1318 | Time 1.2051(1.2015) | Loss 2.431839(2.397304) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1319 | Time 1.2024(1.2016) | Loss 2.342515(2.393469) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1320 | Time 1.2065(1.2019) | Loss 2.308785(2.387541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1321 | Time 1.1978(1.2016) | Loss 2.321009(2.382884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1322 | Time 1.2069(1.2020) | Loss 2.318818(2.378399) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1323 | Time 1.2112(1.2026) | Loss 2.289536(2.372179) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1324 | Time 1.2087(1.2031) | Loss 2.340586(2.369967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1325 | Time 1.2033(1.2031) | Loss 2.369346(2.369924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1326 | Time 1.2118(1.2037) | Loss 2.320976(2.366497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1327 | Time 1.1965(1.2032) | Loss 2.375593(2.367134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1328 | Time 1.2323(1.2052) | Loss 2.304828(2.362773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1329 | Time 1.2158(1.2060) | Loss 2.398220(2.365254) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1330 | Time 1.2065(1.2060) | Loss 2.315302(2.361757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1331 | Time 1.1974(1.2054) | Loss 2.325774(2.359239) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1332 | Time 1.2291(1.2071) | Loss 2.431684(2.364310) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1333 | Time 1.2338(1.2089) | Loss 2.385211(2.365773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1334 | Time 1.2201(1.2097) | Loss 2.400531(2.368206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1335 | Time 1.2519(1.2127) | Loss 2.377396(2.368849) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1336 | Time 1.2466(1.2150) | Loss 2.434592(2.373451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1337 | Time 1.2001(1.2140) | Loss 2.406617(2.375773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1338 | Time 1.1961(1.2128) | Loss 2.405253(2.377836) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1339 | Time 1.1973(1.2117) | Loss 2.406116(2.379816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1340 | Time 1.2075(1.2114) | Loss 2.457488(2.385253) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1341 | Time 1.2055(1.2110) | Loss 2.410512(2.387021) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1342 | Time 1.2172(1.2114) | Loss 2.327796(2.382875) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1343 | Time 1.1973(1.2104) | Loss 2.371775(2.382098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1344 | Time 1.2031(1.2099) | Loss 2.349282(2.379801) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1345 | Time 1.1955(1.2089) | Loss 2.343333(2.377248) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1346 | Time 1.2443(1.2114) | Loss 2.282354(2.370606) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1347 | Time 1.2335(1.2129) | Loss 2.365751(2.370266) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1348 | Time 1.2110(1.2128) | Loss 2.377686(2.370785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1349 | Time 1.2254(1.2137) | Loss 2.401006(2.372901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1350 | Time 1.2341(1.2151) | Loss 2.284039(2.366680) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1351 | Time 1.2475(1.2174) | Loss 2.344982(2.365162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1352 | Time 1.2229(1.2178) | Loss 2.367370(2.365316) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1353 | Time 1.2158(1.2176) | Loss 2.368738(2.365556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1354 | Time 1.2247(1.2181) | Loss 2.360661(2.365213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1355 | Time 1.2316(1.2191) | Loss 2.377902(2.366101) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1356 | Time 1.2461(1.2210) | Loss 2.399662(2.368451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1357 | Time 1.2214(1.2210) | Loss 2.374197(2.368853) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1358 | Time 1.2436(1.2226) | Loss 2.374875(2.369274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1359 | Time 1.2390(1.2237) | Loss 2.394604(2.371047) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1360 | Time 1.2022(1.2222) | Loss 2.383006(2.371884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1361 | Time 1.2179(1.2219) | Loss 2.418516(2.375149) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1362 | Time 1.2179(1.2216) | Loss 2.431182(2.379071) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1363 | Time 1.2122(1.2210) | Loss 2.415848(2.381645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1364 | Time 1.2088(1.2201) | Loss 2.436793(2.385506) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1365 | Time 1.2132(1.2196) | Loss 2.434484(2.388934) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1366 | Time 1.2021(1.2184) | Loss 2.400518(2.389745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1367 | Time 1.2047(1.2175) | Loss 2.435292(2.392933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1368 | Time 1.2031(1.2164) | Loss 2.378667(2.391935) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1369 | Time 1.1984(1.2152) | Loss 2.426427(2.394349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1370 | Time 1.2110(1.2149) | Loss 2.320934(2.389210) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1371 | Time 1.2012(1.2139) | Loss 2.407848(2.390515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1372 | Time 1.2102(1.2137) | Loss 2.381436(2.389879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1373 | Time 1.2092(1.2134) | Loss 2.442370(2.393554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1374 | Time 1.2018(1.2125) | Loss 2.449708(2.397484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1375 | Time 1.1922(1.2111) | Loss 2.432908(2.399964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1376 | Time 1.2105(1.2111) | Loss 2.453732(2.403728) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1377 | Time 1.2064(1.2108) | Loss 2.484743(2.409399) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1378 | Time 1.2033(1.2102) | Loss 2.481260(2.414429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1379 | Time 1.2072(1.2100) | Loss 2.452654(2.417105) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1380 | Time 1.2073(1.2098) | Loss 2.510251(2.423625) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1381 | Time 1.2020(1.2093) | Loss 2.553901(2.432744) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1382 | Time 1.2054(1.2090) | Loss 2.513308(2.438384) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1383 | Time 1.2011(1.2085) | Loss 2.560443(2.446928) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1384 | Time 1.1859(1.2069) | Loss 2.575783(2.455948) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1385 | Time 1.2111(1.2072) | Loss 2.591738(2.465453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1386 | Time 1.2282(1.2086) | Loss 2.539589(2.470643) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1387 | Time 1.2187(1.2094) | Loss 2.570326(2.477620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1388 | Time 1.2065(1.2092) | Loss 2.485479(2.478171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1389 | Time 1.1983(1.2084) | Loss 2.441534(2.475606) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1390 | Time 1.2015(1.2079) | Loss 2.557724(2.481354) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1391 | Time 1.2117(1.2082) | Loss 2.543782(2.485724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1392 | Time 1.2208(1.2091) | Loss 2.556502(2.490679) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1393 | Time 1.2214(1.2099) | Loss 2.559974(2.495529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1394 | Time 1.2033(1.2095) | Loss 2.528793(2.497858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1395 | Time 1.2150(1.2098) | Loss 2.571808(2.503034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1396 | Time 1.2206(1.2106) | Loss 2.531828(2.505050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1397 | Time 1.2137(1.2108) | Loss 2.551100(2.508273) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1398 | Time 1.2234(1.2117) | Loss 2.630648(2.516840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1399 | Time 1.2420(1.2138) | Loss 2.587364(2.521776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1400 | Time 1.2092(1.2135) | Loss 2.527323(2.522164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1400 | Test Loss 2.483817 | NFE 20
Skipping vis as data dimension is >2
Iter 1401 | Time 1.2051(1.2129) | Loss 2.572849(2.525712) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1402 | Time 1.2160(1.2131) | Loss 2.547452(2.527234) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1403 | Time 1.1942(1.2118) | Loss 2.555525(2.529214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1404 | Time 1.1943(1.2106) | Loss 2.536190(2.529703) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1405 | Time 1.2235(1.2115) | Loss 2.537841(2.530272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1406 | Time 1.2013(1.2108) | Loss 2.551342(2.531747) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1407 | Time 1.2135(1.2110) | Loss 2.491704(2.528944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1408 | Time 1.2369(1.2128) | Loss 2.510714(2.527668) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1409 | Time 1.1918(1.2113) | Loss 2.499869(2.525722) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1410 | Time 1.1986(1.2104) | Loss 2.470507(2.521857) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1411 | Time 1.2163(1.2108) | Loss 2.478177(2.518800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1412 | Time 1.2291(1.2121) | Loss 2.470775(2.515438) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1413 | Time 1.2337(1.2136) | Loss 2.485131(2.513316) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1414 | Time 1.2109(1.2134) | Loss 2.431822(2.507612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1415 | Time 1.2218(1.2140) | Loss 2.491665(2.506495) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1416 | Time 1.2169(1.2142) | Loss 2.434716(2.501471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1417 | Time 1.2169(1.2144) | Loss 2.523488(2.503012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1418 | Time 1.2214(1.2149) | Loss 2.445657(2.498997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1419 | Time 1.2340(1.2162) | Loss 2.429745(2.494150) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1420 | Time 1.2181(1.2164) | Loss 2.483334(2.493393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1421 | Time 1.2035(1.2155) | Loss 2.512294(2.494716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1422 | Time 1.2081(1.2149) | Loss 2.471491(2.493090) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1423 | Time 1.2107(1.2146) | Loss 2.503209(2.493798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1424 | Time 1.2098(1.2143) | Loss 2.505161(2.494594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1425 | Time 1.2237(1.2150) | Loss 2.518249(2.496249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1426 | Time 1.2120(1.2148) | Loss 2.529821(2.498600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1427 | Time 1.2076(1.2143) | Loss 2.534651(2.501123) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1428 | Time 1.2176(1.2145) | Loss 2.493572(2.500595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1429 | Time 1.2170(1.2147) | Loss 2.566096(2.505180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1430 | Time 1.2320(1.2159) | Loss 2.538485(2.507511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1431 | Time 1.2711(1.2197) | Loss 2.524055(2.508669) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1432 | Time 1.2107(1.2191) | Loss 2.555915(2.511976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1433 | Time 1.2134(1.2187) | Loss 2.579863(2.516728) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1434 | Time 1.2169(1.2186) | Loss 2.558185(2.519630) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1435 | Time 1.2098(1.2180) | Loss 2.578513(2.523752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1436 | Time 1.2295(1.2188) | Loss 2.620033(2.530492) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1437 | Time 1.2047(1.2178) | Loss 2.582263(2.534116) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1438 | Time 1.2182(1.2178) | Loss 2.497565(2.531557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1439 | Time 1.2214(1.2181) | Loss 2.535509(2.531834) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1440 | Time 1.2488(1.2202) | Loss 2.510333(2.530329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1441 | Time 1.2249(1.2205) | Loss 2.487998(2.527366) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1442 | Time 1.2232(1.2207) | Loss 2.504917(2.525794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1443 | Time 1.2118(1.2201) | Loss 2.510662(2.524735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1444 | Time 1.2399(1.2215) | Loss 2.514480(2.524017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1445 | Time 1.2294(1.2220) | Loss 2.571686(2.527354) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1446 | Time 1.2320(1.2227) | Loss 2.524726(2.527170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1447 | Time 1.2259(1.2230) | Loss 2.594327(2.531871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1448 | Time 1.2276(1.2233) | Loss 2.489773(2.528924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1449 | Time 1.2181(1.2229) | Loss 2.568805(2.531716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1450 | Time 1.2052(1.2217) | Loss 2.498306(2.529377) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1451 | Time 1.1986(1.2201) | Loss 2.560958(2.531588) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1452 | Time 1.2211(1.2201) | Loss 2.565922(2.533991) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1453 | Time 1.2250(1.2205) | Loss 2.551239(2.535199) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1454 | Time 1.2452(1.2222) | Loss 2.537532(2.535362) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1455 | Time 1.2175(1.2219) | Loss 2.490330(2.532210) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1456 | Time 1.2242(1.2220) | Loss 2.529428(2.532015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1457 | Time 1.2286(1.2225) | Loss 2.546115(2.533002) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1458 | Time 1.2112(1.2217) | Loss 2.556963(2.534679) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1459 | Time 1.2032(1.2204) | Loss 2.526050(2.534075) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1460 | Time 1.2164(1.2201) | Loss 2.555485(2.535574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1461 | Time 1.2149(1.2198) | Loss 2.521630(2.534598) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1462 | Time 1.2023(1.2185) | Loss 2.552909(2.535880) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1463 | Time 1.2156(1.2183) | Loss 2.567247(2.538075) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1464 | Time 1.2126(1.2179) | Loss 2.530691(2.537558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1465 | Time 1.2102(1.2174) | Loss 2.535790(2.537435) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1466 | Time 1.2286(1.2182) | Loss 2.547338(2.538128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1467 | Time 1.2614(1.2212) | Loss 2.561838(2.539788) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1468 | Time 1.2207(1.2212) | Loss 2.535447(2.539484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1469 | Time 1.2214(1.2212) | Loss 2.539962(2.539517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1470 | Time 1.2240(1.2214) | Loss 2.589872(2.543042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1471 | Time 1.2185(1.2212) | Loss 2.539432(2.542789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1472 | Time 1.2193(1.2211) | Loss 2.570899(2.544757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1473 | Time 1.2057(1.2200) | Loss 2.597510(2.548450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1474 | Time 1.2115(1.2194) | Loss 2.612123(2.552907) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1475 | Time 1.2440(1.2211) | Loss 2.571887(2.554235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1476 | Time 1.2131(1.2205) | Loss 2.544433(2.553549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1477 | Time 1.2160(1.2202) | Loss 2.590731(2.556152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1478 | Time 1.2744(1.2240) | Loss 2.621798(2.560747) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1479 | Time 1.2042(1.2226) | Loss 2.652828(2.567193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1480 | Time 1.2335(1.2234) | Loss 2.554611(2.566312) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1481 | Time 1.2399(1.2245) | Loss 2.537194(2.564274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1482 | Time 1.2116(1.2236) | Loss 2.563596(2.564226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1483 | Time 1.2363(1.2245) | Loss 2.531507(2.561936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1484 | Time 1.2149(1.2239) | Loss 2.535602(2.560093) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1485 | Time 1.2221(1.2237) | Loss 2.557570(2.559916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1486 | Time 1.2128(1.2230) | Loss 2.555350(2.559597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1487 | Time 1.2319(1.2236) | Loss 2.641038(2.565297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1488 | Time 1.2486(1.2253) | Loss 2.546955(2.564013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1489 | Time 1.1974(1.2234) | Loss 2.558818(2.563650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1490 | Time 1.2049(1.2221) | Loss 2.544645(2.562319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1491 | Time 1.2078(1.2211) | Loss 2.508713(2.558567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1492 | Time 1.2214(1.2211) | Loss 2.565523(2.559054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1493 | Time 1.2010(1.2197) | Loss 2.556373(2.558866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1494 | Time 1.2153(1.2194) | Loss 2.561966(2.559083) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1495 | Time 1.2105(1.2188) | Loss 2.590575(2.561288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1496 | Time 1.2100(1.2182) | Loss 2.520428(2.558427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1497 | Time 1.2109(1.2176) | Loss 2.551359(2.557933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1498 | Time 1.2415(1.2193) | Loss 2.565897(2.558490) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1499 | Time 1.2009(1.2180) | Loss 2.518120(2.555664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1500 | Time 1.2019(1.2169) | Loss 2.586740(2.557840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1500 | Test Loss 2.550607 | NFE 20
Skipping vis as data dimension is >2
Iter 1501 | Time 1.2097(1.2164) | Loss 2.590336(2.560114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1502 | Time 1.2257(1.2170) | Loss 2.629193(2.564950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1503 | Time 1.2227(1.2174) | Loss 2.598337(2.567287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1504 | Time 1.1951(1.2159) | Loss 2.547820(2.565924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1505 | Time 1.1966(1.2145) | Loss 2.555664(2.565206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1506 | Time 1.1816(1.2122) | Loss 2.529792(2.562727) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1507 | Time 1.1877(1.2105) | Loss 2.563206(2.562761) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1508 | Time 1.2004(1.2098) | Loss 2.550534(2.561905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1509 | Time 1.1903(1.2084) | Loss 2.546276(2.560811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1510 | Time 1.1907(1.2072) | Loss 2.526792(2.558429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1511 | Time 1.1936(1.2062) | Loss 2.520499(2.555774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1512 | Time 1.2167(1.2070) | Loss 2.547452(2.555192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1513 | Time 1.2018(1.2066) | Loss 2.544945(2.554474) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1514 | Time 1.1902(1.2055) | Loss 2.524262(2.552360) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1515 | Time 1.2090(1.2057) | Loss 2.560999(2.552964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1516 | Time 1.2058(1.2057) | Loss 2.513242(2.550184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1517 | Time 1.2075(1.2058) | Loss 2.572338(2.551735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1518 | Time 1.2037(1.2057) | Loss 2.500602(2.548155) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1519 | Time 1.1884(1.2045) | Loss 2.544456(2.547896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1520 | Time 1.2350(1.2066) | Loss 2.551653(2.548159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1521 | Time 1.2215(1.2077) | Loss 2.486248(2.543826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1522 | Time 1.2055(1.2075) | Loss 2.544087(2.543844) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1523 | Time 1.2524(1.2107) | Loss 2.550996(2.544344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1524 | Time 1.2056(1.2103) | Loss 2.502656(2.541426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1525 | Time 1.1979(1.2094) | Loss 2.454847(2.535366) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1526 | Time 1.2479(1.2121) | Loss 2.453066(2.529605) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1527 | Time 1.2153(1.2123) | Loss 2.509580(2.528203) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1528 | Time 1.2019(1.2116) | Loss 2.457348(2.523243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1529 | Time 1.2221(1.2124) | Loss 2.494747(2.521248) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1530 | Time 1.2214(1.2130) | Loss 2.466583(2.517422) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1531 | Time 1.2153(1.2131) | Loss 2.502264(2.516361) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1532 | Time 1.2050(1.2126) | Loss 2.541640(2.518130) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1533 | Time 1.2051(1.2121) | Loss 2.538537(2.519559) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1534 | Time 1.2365(1.2138) | Loss 2.525295(2.519960) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1535 | Time 1.1879(1.2120) | Loss 2.580169(2.524175) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1536 | Time 1.1926(1.2106) | Loss 2.585463(2.528465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1537 | Time 1.1910(1.2092) | Loss 2.561061(2.530747) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1538 | Time 1.1916(1.2080) | Loss 2.529687(2.530673) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1539 | Time 1.2205(1.2089) | Loss 2.616409(2.536674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1540 | Time 1.1999(1.2082) | Loss 2.614820(2.542144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1541 | Time 1.2406(1.2105) | Loss 2.697819(2.553042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1542 | Time 1.2221(1.2113) | Loss 2.626076(2.558154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1543 | Time 1.1994(1.2105) | Loss 2.633412(2.563422) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1544 | Time 1.1982(1.2096) | Loss 2.597972(2.565841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1545 | Time 1.1866(1.2080) | Loss 2.572379(2.566298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1546 | Time 1.1867(1.2065) | Loss 2.564547(2.566176) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1547 | Time 1.2010(1.2061) | Loss 2.573553(2.566692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1548 | Time 1.2498(1.2092) | Loss 2.592283(2.568483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1549 | Time 1.2288(1.2106) | Loss 2.616736(2.571861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1550 | Time 1.2218(1.2113) | Loss 2.617988(2.575090) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1551 | Time 1.2119(1.2114) | Loss 2.513917(2.570808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1552 | Time 1.2160(1.2117) | Loss 2.519355(2.567206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1553 | Time 1.2359(1.2134) | Loss 2.522654(2.564088) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1554 | Time 1.2280(1.2144) | Loss 2.548954(2.563028) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1555 | Time 1.2309(1.2156) | Loss 2.622169(2.567168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1556 | Time 1.2357(1.2170) | Loss 2.595603(2.569159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1557 | Time 1.2287(1.2178) | Loss 2.570737(2.569269) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1558 | Time 1.2259(1.2184) | Loss 2.582915(2.570224) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1559 | Time 1.2349(1.2195) | Loss 2.609761(2.572992) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1560 | Time 1.2025(1.2183) | Loss 2.693368(2.581418) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1561 | Time 1.2170(1.2182) | Loss 2.653185(2.586442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1562 | Time 1.2286(1.2190) | Loss 2.671647(2.592406) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1563 | Time 1.2203(1.2191) | Loss 2.622938(2.594543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1564 | Time 1.2232(1.2194) | Loss 2.605706(2.595325) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1565 | Time 1.2205(1.2194) | Loss 2.635138(2.598112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1566 | Time 1.2249(1.2198) | Loss 2.659503(2.602409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1567 | Time 1.2160(1.2196) | Loss 2.615464(2.603323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1568 | Time 1.2213(1.2197) | Loss 2.645360(2.606265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1569 | Time 1.2190(1.2196) | Loss 2.617891(2.607079) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1570 | Time 1.1982(1.2181) | Loss 2.649629(2.610058) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1571 | Time 1.2083(1.2174) | Loss 2.722374(2.617920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1572 | Time 1.2086(1.2168) | Loss 2.679250(2.622213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1573 | Time 1.2217(1.2172) | Loss 2.748155(2.631029) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1574 | Time 1.2848(1.2219) | Loss 2.728478(2.637850) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1575 | Time 1.2478(1.2237) | Loss 2.705728(2.642602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1576 | Time 1.2357(1.2245) | Loss 2.717242(2.647827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1577 | Time 1.2190(1.2242) | Loss 2.724637(2.653203) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1578 | Time 1.2231(1.2241) | Loss 2.651623(2.653093) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1579 | Time 1.2153(1.2235) | Loss 2.696896(2.656159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1580 | Time 1.2123(1.2227) | Loss 2.603057(2.652442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1581 | Time 1.2250(1.2228) | Loss 2.689426(2.655031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1582 | Time 1.2175(1.2225) | Loss 2.635815(2.653686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1583 | Time 1.2014(1.2210) | Loss 2.716152(2.658058) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1584 | Time 1.2131(1.2204) | Loss 2.661478(2.658298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1585 | Time 1.2118(1.2198) | Loss 2.747259(2.664525) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1586 | Time 1.2131(1.2194) | Loss 2.743714(2.670068) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1587 | Time 1.2219(1.2195) | Loss 2.628095(2.667130) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1588 | Time 1.2140(1.2192) | Loss 2.765264(2.673999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1589 | Time 1.2116(1.2186) | Loss 2.743194(2.678843) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1590 | Time 1.2329(1.2196) | Loss 2.702960(2.680531) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1591 | Time 1.2271(1.2202) | Loss 2.688552(2.681093) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1592 | Time 1.2186(1.2200) | Loss 2.750876(2.685977) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1593 | Time 1.2256(1.2204) | Loss 2.743294(2.689990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1594 | Time 1.2110(1.2198) | Loss 2.708790(2.691306) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1595 | Time 1.2171(1.2196) | Loss 2.699459(2.691876) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1596 | Time 1.2452(1.2214) | Loss 2.716270(2.693584) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1597 | Time 1.2132(1.2208) | Loss 2.683031(2.692845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1598 | Time 1.2190(1.2207) | Loss 2.721106(2.694823) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1599 | Time 1.2196(1.2206) | Loss 2.735532(2.697673) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1600 | Time 1.2616(1.2235) | Loss 2.722820(2.699433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1600 | Test Loss 2.766642 | NFE 20
Skipping vis as data dimension is >2
Iter 1601 | Time 1.2858(1.2278) | Loss 2.645529(2.695660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1602 | Time 1.2441(1.2290) | Loss 2.685908(2.694977) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1603 | Time 1.2517(1.2306) | Loss 2.722859(2.696929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1604 | Time 1.2490(1.2319) | Loss 2.701721(2.697265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1605 | Time 1.2344(1.2320) | Loss 2.719344(2.698810) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1606 | Time 1.2468(1.2331) | Loss 2.754511(2.702709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1607 | Time 1.2457(1.2340) | Loss 2.688020(2.701681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1608 | Time 1.2486(1.2350) | Loss 2.738345(2.704247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1609 | Time 1.2177(1.2338) | Loss 2.712011(2.704791) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1610 | Time 1.2187(1.2327) | Loss 2.829767(2.713539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1611 | Time 1.2236(1.2321) | Loss 2.798296(2.719472) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1612 | Time 1.2303(1.2320) | Loss 2.744898(2.721252) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1613 | Time 1.2464(1.2330) | Loss 2.780591(2.725406) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1614 | Time 1.2132(1.2316) | Loss 2.742987(2.726636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1615 | Time 1.2173(1.2306) | Loss 2.730659(2.726918) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1616 | Time 1.2197(1.2298) | Loss 2.775151(2.730294) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1617 | Time 1.2208(1.2292) | Loss 2.748870(2.731595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1618 | Time 1.2231(1.2288) | Loss 2.727355(2.731298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1619 | Time 1.1968(1.2265) | Loss 2.753760(2.732870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1620 | Time 1.2198(1.2261) | Loss 2.704792(2.730905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1621 | Time 1.2174(1.2254) | Loss 2.753184(2.732464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1622 | Time 1.2135(1.2246) | Loss 2.737659(2.732828) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1623 | Time 1.2055(1.2233) | Loss 2.706719(2.731000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1624 | Time 1.2056(1.2220) | Loss 2.735481(2.731314) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1625 | Time 1.2150(1.2215) | Loss 2.737496(2.731747) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1626 | Time 1.2038(1.2203) | Loss 2.730949(2.731691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1627 | Time 1.2141(1.2199) | Loss 2.738115(2.732141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1628 | Time 1.2518(1.2221) | Loss 2.681850(2.728620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1629 | Time 1.2319(1.2228) | Loss 2.638134(2.722286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1630 | Time 1.2591(1.2253) | Loss 2.768850(2.725546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1631 | Time 1.2459(1.2268) | Loss 2.722432(2.725328) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1632 | Time 1.2374(1.2275) | Loss 2.653270(2.720284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1633 | Time 1.2384(1.2283) | Loss 2.763566(2.723313) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1634 | Time 1.2305(1.2284) | Loss 2.744032(2.724764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1635 | Time 1.2233(1.2281) | Loss 2.708097(2.723597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1636 | Time 1.2150(1.2272) | Loss 2.725632(2.723739) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1637 | Time 1.2316(1.2275) | Loss 2.738351(2.724762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1638 | Time 1.2241(1.2272) | Loss 2.719457(2.724391) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1639 | Time 1.2337(1.2277) | Loss 2.741381(2.725580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1640 | Time 1.2230(1.2274) | Loss 2.757753(2.727832) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1641 | Time 1.2206(1.2269) | Loss 2.667772(2.723628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1642 | Time 1.2135(1.2259) | Loss 2.733732(2.724335) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1643 | Time 1.2107(1.2249) | Loss 2.715138(2.723692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1644 | Time 1.2057(1.2235) | Loss 2.706261(2.722471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1645 | Time 1.2143(1.2229) | Loss 2.694048(2.720482) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1646 | Time 1.2161(1.2224) | Loss 2.695691(2.718746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1647 | Time 1.2193(1.2222) | Loss 2.729869(2.719525) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1648 | Time 1.2192(1.2220) | Loss 2.811578(2.725969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1649 | Time 1.2220(1.2220) | Loss 2.719489(2.725515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1650 | Time 1.2359(1.2230) | Loss 2.779848(2.729318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1651 | Time 1.2370(1.2239) | Loss 2.752381(2.730933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1652 | Time 1.2401(1.2251) | Loss 2.782606(2.734550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1653 | Time 1.1961(1.2230) | Loss 2.792575(2.738612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1654 | Time 1.2048(1.2218) | Loss 2.759018(2.740040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1655 | Time 1.2228(1.2218) | Loss 2.744727(2.740368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1656 | Time 1.2185(1.2216) | Loss 2.670736(2.735494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1657 | Time 1.2228(1.2217) | Loss 2.686941(2.732095) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1658 | Time 1.2475(1.2235) | Loss 2.696109(2.729576) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1659 | Time 1.2237(1.2235) | Loss 2.763824(2.731974) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1660 | Time 1.2208(1.2233) | Loss 2.703893(2.730008) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1661 | Time 1.2282(1.2237) | Loss 2.705431(2.728287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1662 | Time 1.2251(1.2238) | Loss 2.667181(2.724010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1663 | Time 1.2322(1.2243) | Loss 2.667157(2.720030) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1664 | Time 1.2357(1.2251) | Loss 2.645088(2.714784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1665 | Time 1.2319(1.2256) | Loss 2.655373(2.710626) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1666 | Time 1.2164(1.2250) | Loss 2.635277(2.705351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1667 | Time 1.1863(1.2223) | Loss 2.719427(2.706337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1668 | Time 1.2050(1.2211) | Loss 2.721220(2.707378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1669 | Time 1.2207(1.2210) | Loss 2.778734(2.712373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1670 | Time 1.2064(1.2200) | Loss 2.714393(2.712515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1671 | Time 1.2201(1.2200) | Loss 2.748296(2.715019) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1672 | Time 1.2169(1.2198) | Loss 2.809928(2.721663) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1673 | Time 1.2192(1.2198) | Loss 2.690254(2.719464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1674 | Time 1.2206(1.2198) | Loss 2.714937(2.719147) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1675 | Time 1.2749(1.2237) | Loss 2.719387(2.719164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1676 | Time 1.2568(1.2260) | Loss 2.736130(2.720352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1677 | Time 1.2311(1.2263) | Loss 2.756444(2.722878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1678 | Time 1.2338(1.2269) | Loss 2.796201(2.728011) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1679 | Time 1.2361(1.2275) | Loss 2.749883(2.729542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1680 | Time 1.2236(1.2272) | Loss 2.694394(2.727081) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1681 | Time 1.2363(1.2279) | Loss 2.773769(2.730350) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1682 | Time 1.2386(1.2286) | Loss 2.845942(2.738441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1683 | Time 1.2226(1.2282) | Loss 2.770135(2.740660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1684 | Time 1.2300(1.2283) | Loss 2.878324(2.750296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1685 | Time 1.2433(1.2294) | Loss 2.847053(2.757069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1686 | Time 1.2348(1.2298) | Loss 2.820203(2.761488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1687 | Time 1.2500(1.2312) | Loss 2.843744(2.767246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1688 | Time 1.2305(1.2311) | Loss 2.841476(2.772442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1689 | Time 1.2230(1.2306) | Loss 2.810354(2.775096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1690 | Time 1.2336(1.2308) | Loss 2.860819(2.781097) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1691 | Time 1.2295(1.2307) | Loss 2.843677(2.785477) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1692 | Time 1.2219(1.2301) | Loss 2.909995(2.794194) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1693 | Time 1.2231(1.2296) | Loss 2.834190(2.796993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1694 | Time 1.2122(1.2284) | Loss 2.833850(2.799573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1695 | Time 1.2261(1.2282) | Loss 2.881941(2.805339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1696 | Time 1.2110(1.2270) | Loss 2.915019(2.813017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1697 | Time 1.1898(1.2244) | Loss 2.915934(2.820221) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1698 | Time 1.1920(1.2221) | Loss 2.829558(2.820874) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1699 | Time 1.2198(1.2220) | Loss 2.878429(2.824903) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1700 | Time 1.2020(1.2206) | Loss 2.897765(2.830004) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1700 | Test Loss 2.834879 | NFE 20
Skipping vis as data dimension is >2
Iter 1701 | Time 1.1946(1.2187) | Loss 2.780898(2.826566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1702 | Time 1.2050(1.2178) | Loss 2.860522(2.828943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1703 | Time 1.2125(1.2174) | Loss 2.852572(2.830597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1704 | Time 1.1978(1.2160) | Loss 2.802055(2.828599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1705 | Time 1.2078(1.2155) | Loss 2.831811(2.828824) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1706 | Time 1.2164(1.2155) | Loss 2.808630(2.827410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1707 | Time 1.2150(1.2155) | Loss 2.680547(2.817130) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1708 | Time 1.2456(1.2176) | Loss 2.779831(2.814519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1709 | Time 1.2764(1.2217) | Loss 2.800987(2.813572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1710 | Time 1.2218(1.2217) | Loss 2.668892(2.803444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1711 | Time 1.2181(1.2215) | Loss 2.684806(2.795140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1712 | Time 1.2195(1.2213) | Loss 2.673052(2.786594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1713 | Time 1.2558(1.2237) | Loss 2.662176(2.777884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1714 | Time 1.2044(1.2224) | Loss 2.751656(2.776048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1715 | Time 1.2429(1.2238) | Loss 2.720556(2.772164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1716 | Time 1.2032(1.2224) | Loss 2.707415(2.767631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1717 | Time 1.2021(1.2210) | Loss 2.728831(2.764915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1718 | Time 1.2027(1.2197) | Loss 2.706708(2.760841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1719 | Time 1.2013(1.2184) | Loss 2.721659(2.758098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1720 | Time 1.2193(1.2185) | Loss 2.679455(2.752593) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1721 | Time 1.2460(1.2204) | Loss 2.687504(2.748037) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1722 | Time 1.2086(1.2196) | Loss 2.729401(2.746732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1723 | Time 1.2218(1.2197) | Loss 2.716156(2.744592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1724 | Time 1.2100(1.2190) | Loss 2.686616(2.740534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1725 | Time 1.1886(1.2169) | Loss 2.798545(2.744595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1726 | Time 1.2223(1.2173) | Loss 2.731256(2.743661) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1727 | Time 1.2096(1.2167) | Loss 2.778223(2.746080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1728 | Time 1.2074(1.2161) | Loss 2.841452(2.752756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1729 | Time 1.2114(1.2158) | Loss 2.696766(2.748837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1730 | Time 1.2139(1.2156) | Loss 2.850202(2.755932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1731 | Time 1.2197(1.2159) | Loss 2.837283(2.761627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1732 | Time 1.2214(1.2163) | Loss 2.853270(2.768042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1733 | Time 1.1895(1.2144) | Loss 2.841647(2.773194) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1734 | Time 1.1928(1.2129) | Loss 2.813985(2.776050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1735 | Time 1.2339(1.2144) | Loss 2.877918(2.783181) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1736 | Time 1.2244(1.2151) | Loss 2.808829(2.784976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1737 | Time 1.2197(1.2154) | Loss 2.851187(2.789611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1738 | Time 1.2487(1.2177) | Loss 2.765075(2.787893) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1739 | Time 1.2270(1.2184) | Loss 2.819205(2.790085) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1740 | Time 1.2345(1.2195) | Loss 2.808965(2.791407) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1741 | Time 1.2276(1.2201) | Loss 2.800500(2.792043) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1742 | Time 1.2346(1.2211) | Loss 2.822395(2.794168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1743 | Time 1.2067(1.2201) | Loss 2.770743(2.792528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1744 | Time 1.2076(1.2192) | Loss 2.793932(2.792626) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1745 | Time 1.1934(1.2174) | Loss 2.823417(2.794782) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1746 | Time 1.2153(1.2173) | Loss 2.796123(2.794876) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1747 | Time 1.2026(1.2162) | Loss 2.792742(2.794726) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1748 | Time 1.2222(1.2167) | Loss 2.842794(2.798091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1749 | Time 1.2049(1.2158) | Loss 2.807070(2.798719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1750 | Time 1.1958(1.2144) | Loss 2.659978(2.789008) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1751 | Time 1.1920(1.2129) | Loss 2.750216(2.786292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1752 | Time 1.2235(1.2136) | Loss 2.770166(2.785163) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1753 | Time 1.1965(1.2124) | Loss 2.726316(2.781044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1754 | Time 1.2052(1.2119) | Loss 2.857895(2.786424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1755 | Time 1.2085(1.2117) | Loss 2.787652(2.786510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1756 | Time 1.2192(1.2122) | Loss 2.798510(2.787350) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1757 | Time 1.2224(1.2129) | Loss 2.820020(2.789637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1758 | Time 1.2148(1.2130) | Loss 2.823179(2.791984) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1759 | Time 1.2251(1.2139) | Loss 2.787734(2.791687) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1760 | Time 1.2189(1.2142) | Loss 2.891905(2.798702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1761 | Time 1.1975(1.2131) | Loss 2.860121(2.803002) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1762 | Time 1.2679(1.2169) | Loss 2.923791(2.811457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1763 | Time 1.2653(1.2203) | Loss 2.915265(2.818723) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1764 | Time 1.2546(1.2227) | Loss 2.910988(2.825182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1765 | Time 1.2277(1.2230) | Loss 3.020044(2.838822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1766 | Time 1.2158(1.2225) | Loss 2.889612(2.842377) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1767 | Time 1.2127(1.2218) | Loss 2.997494(2.853236) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1768 | Time 1.2510(1.2239) | Loss 2.973397(2.861647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1769 | Time 1.2109(1.2230) | Loss 2.995078(2.870987) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1770 | Time 1.2238(1.2230) | Loss 2.921736(2.874540) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1771 | Time 1.2096(1.2221) | Loss 2.930774(2.878476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1772 | Time 1.2079(1.2211) | Loss 2.981543(2.885691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1773 | Time 1.2144(1.2206) | Loss 2.936123(2.889221) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1774 | Time 1.2056(1.2196) | Loss 2.945082(2.893131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1775 | Time 1.2144(1.2192) | Loss 2.938979(2.896341) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1776 | Time 1.1896(1.2171) | Loss 2.946326(2.899839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1777 | Time 1.2116(1.2168) | Loss 2.999053(2.906784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1778 | Time 1.1995(1.2156) | Loss 2.888968(2.905537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1779 | Time 1.2045(1.2148) | Loss 2.975961(2.910467) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1780 | Time 1.2059(1.2142) | Loss 2.979224(2.915280) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1781 | Time 1.2011(1.2132) | Loss 3.034522(2.923627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1782 | Time 1.1972(1.2121) | Loss 2.940326(2.924796) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1783 | Time 1.2088(1.2119) | Loss 2.948060(2.926424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1784 | Time 1.2305(1.2132) | Loss 2.910086(2.925281) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1785 | Time 1.2158(1.2134) | Loss 2.971879(2.928543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1786 | Time 1.2113(1.2132) | Loss 2.937606(2.929177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1787 | Time 1.1921(1.2117) | Loss 2.985847(2.933144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1788 | Time 1.1971(1.2107) | Loss 2.967224(2.935530) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1789 | Time 1.2427(1.2130) | Loss 2.992920(2.939547) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1790 | Time 1.2392(1.2148) | Loss 2.905099(2.937136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1791 | Time 1.2633(1.2182) | Loss 2.994429(2.941146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1792 | Time 1.2203(1.2183) | Loss 3.006594(2.945727) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1793 | Time 1.2179(1.2183) | Loss 2.983231(2.948353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1794 | Time 1.2139(1.2180) | Loss 2.945159(2.948129) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1795 | Time 1.2044(1.2171) | Loss 3.016750(2.952933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1796 | Time 1.2152(1.2169) | Loss 2.972414(2.954296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1797 | Time 1.2146(1.2168) | Loss 3.056378(2.961442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1798 | Time 1.1989(1.2155) | Loss 3.046252(2.967379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1799 | Time 1.1860(1.2134) | Loss 3.039410(2.972421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1800 | Time 1.1869(1.2116) | Loss 3.001940(2.974487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1800 | Test Loss 3.015001 | NFE 20
Skipping vis as data dimension is >2
Iter 1801 | Time 1.1961(1.2105) | Loss 3.060640(2.980518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1802 | Time 1.1970(1.2096) | Loss 2.977849(2.980331) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1803 | Time 1.1979(1.2087) | Loss 3.003387(2.981945) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1804 | Time 1.1983(1.2080) | Loss 2.928223(2.978184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1805 | Time 1.2015(1.2076) | Loss 2.949801(2.976198) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1806 | Time 1.2036(1.2073) | Loss 2.937229(2.973470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1807 | Time 1.2129(1.2077) | Loss 2.935385(2.970804) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1808 | Time 1.2160(1.2083) | Loss 3.010834(2.973606) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1809 | Time 1.2101(1.2084) | Loss 2.966289(2.973094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1810 | Time 1.1847(1.2067) | Loss 2.905361(2.968352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1811 | Time 1.2057(1.2067) | Loss 2.948428(2.966958) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1812 | Time 1.2095(1.2069) | Loss 2.939811(2.965057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1813 | Time 1.1954(1.2061) | Loss 2.884965(2.959451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1814 | Time 1.1995(1.2056) | Loss 3.024194(2.963983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1815 | Time 1.1924(1.2047) | Loss 2.927371(2.961420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1816 | Time 1.2330(1.2067) | Loss 2.898632(2.957025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1817 | Time 1.2087(1.2068) | Loss 2.962271(2.957392) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1818 | Time 1.2058(1.2067) | Loss 2.954766(2.957208) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1819 | Time 1.2090(1.2069) | Loss 2.876699(2.951573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1820 | Time 1.2126(1.2073) | Loss 2.945488(2.951147) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1821 | Time 1.2157(1.2079) | Loss 2.970178(2.952479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1822 | Time 1.2091(1.2080) | Loss 2.928220(2.950781) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1823 | Time 1.2136(1.2084) | Loss 2.986606(2.953289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1824 | Time 1.1949(1.2074) | Loss 2.998444(2.956449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1825 | Time 1.2173(1.2081) | Loss 3.006350(2.959943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1826 | Time 1.1853(1.2065) | Loss 2.927643(2.957682) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1827 | Time 1.1866(1.2051) | Loss 3.013457(2.961586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1828 | Time 1.1897(1.2040) | Loss 2.918735(2.958586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1829 | Time 1.1927(1.2032) | Loss 3.014824(2.962523) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1830 | Time 1.1941(1.2026) | Loss 2.903539(2.958394) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1831 | Time 1.2071(1.2029) | Loss 2.901041(2.954379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1832 | Time 1.1853(1.2017) | Loss 2.955940(2.954489) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1833 | Time 1.2055(1.2020) | Loss 2.982250(2.956432) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1834 | Time 1.1847(1.2007) | Loss 2.959188(2.956625) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1835 | Time 1.1868(1.1998) | Loss 2.885256(2.951629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1836 | Time 1.1908(1.1991) | Loss 2.867468(2.945738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1837 | Time 1.1870(1.1983) | Loss 2.897402(2.942354) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1838 | Time 1.1864(1.1975) | Loss 2.910408(2.940118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1839 | Time 1.1938(1.1972) | Loss 2.934134(2.939699) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1840 | Time 1.1860(1.1964) | Loss 2.931301(2.939111) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1841 | Time 1.1735(1.1948) | Loss 2.953625(2.940127) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1842 | Time 1.1764(1.1935) | Loss 2.884827(2.936256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1843 | Time 1.2060(1.1944) | Loss 2.915947(2.934835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1844 | Time 1.1754(1.1931) | Loss 2.978943(2.937922) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1845 | Time 1.1843(1.1924) | Loss 2.844552(2.931386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1846 | Time 1.2072(1.1935) | Loss 2.983496(2.935034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1847 | Time 1.1941(1.1935) | Loss 2.922788(2.934177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1848 | Time 1.1714(1.1920) | Loss 2.945618(2.934978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1849 | Time 1.1710(1.1905) | Loss 2.945789(2.935734) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1850 | Time 1.1621(1.1885) | Loss 2.944355(2.936338) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1851 | Time 1.1838(1.1882) | Loss 2.979527(2.939361) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1852 | Time 1.1844(1.1879) | Loss 2.947024(2.939897) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1853 | Time 1.1876(1.1879) | Loss 2.981330(2.942798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1854 | Time 1.2593(1.1929) | Loss 2.987474(2.945925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1855 | Time 1.2566(1.1974) | Loss 3.014883(2.950752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1856 | Time 1.2093(1.1982) | Loss 2.970255(2.952117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1857 | Time 1.2009(1.1984) | Loss 2.980258(2.954087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1858 | Time 1.1967(1.1983) | Loss 2.932230(2.952557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1859 | Time 1.2396(1.2012) | Loss 2.997840(2.955727) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1860 | Time 1.2011(1.2011) | Loss 2.908925(2.952451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1861 | Time 1.2269(1.2030) | Loss 2.937020(2.951371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1862 | Time 1.1986(1.2026) | Loss 2.964386(2.952282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1863 | Time 1.2220(1.2040) | Loss 2.995576(2.955312) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1864 | Time 1.1965(1.2035) | Loss 2.939587(2.954211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1865 | Time 1.2160(1.2044) | Loss 2.941886(2.953349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1866 | Time 1.1994(1.2040) | Loss 2.883238(2.948441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1867 | Time 1.1981(1.2036) | Loss 2.955432(2.948930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1868 | Time 1.2064(1.2038) | Loss 2.985624(2.951499) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1869 | Time 1.2048(1.2039) | Loss 2.923620(2.949547) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1870 | Time 1.2008(1.2037) | Loss 2.949489(2.949543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1871 | Time 1.2776(1.2088) | Loss 2.922173(2.947627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1872 | Time 1.2275(1.2101) | Loss 2.920549(2.945732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1873 | Time 1.2051(1.2098) | Loss 2.907620(2.943064) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1874 | Time 1.2077(1.2096) | Loss 2.980766(2.945703) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1875 | Time 1.2121(1.2098) | Loss 2.963504(2.946949) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1876 | Time 1.2333(1.2115) | Loss 2.941009(2.946533) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1877 | Time 1.2252(1.2124) | Loss 2.922212(2.944831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1878 | Time 1.1997(1.2115) | Loss 2.916775(2.942867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1879 | Time 1.2092(1.2114) | Loss 2.997641(2.946701) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1880 | Time 1.1943(1.2102) | Loss 2.993809(2.949999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1881 | Time 1.2038(1.2097) | Loss 3.051842(2.957128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1882 | Time 1.1865(1.2081) | Loss 3.027266(2.962037) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1883 | Time 1.1928(1.2070) | Loss 2.993304(2.964226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1884 | Time 1.2053(1.2069) | Loss 3.054249(2.970528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1885 | Time 1.1880(1.2056) | Loss 3.075827(2.977899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1886 | Time 1.1933(1.2047) | Loss 3.081621(2.985159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1887 | Time 1.1995(1.2044) | Loss 3.055974(2.990116) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1888 | Time 1.2032(1.2043) | Loss 3.123645(2.999463) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1889 | Time 1.2106(1.2047) | Loss 3.118675(3.007808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1890 | Time 1.1933(1.2039) | Loss 3.094011(3.013842) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1891 | Time 1.1967(1.2034) | Loss 3.078667(3.018380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1892 | Time 1.1882(1.2024) | Loss 3.057317(3.021106) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1893 | Time 1.2110(1.2030) | Loss 3.026992(3.021518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1894 | Time 1.2025(1.2029) | Loss 3.064980(3.024560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1895 | Time 1.2084(1.2033) | Loss 3.003770(3.023105) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1896 | Time 1.2130(1.2040) | Loss 2.961338(3.018781) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1897 | Time 1.1921(1.2032) | Loss 3.111254(3.025254) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1898 | Time 1.2047(1.2033) | Loss 3.026251(3.025324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1899 | Time 1.2118(1.2039) | Loss 3.057171(3.027553) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1900 | Time 1.2065(1.2040) | Loss 2.910077(3.019330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 1900 | Test Loss 2.970748 | NFE 20
Skipping vis as data dimension is >2
Iter 1901 | Time 1.1948(1.2034) | Loss 2.980599(3.016619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1902 | Time 1.2144(1.2042) | Loss 3.023227(3.017081) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1903 | Time 1.2657(1.2085) | Loss 2.997426(3.015705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1904 | Time 1.2736(1.2130) | Loss 3.008369(3.015192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1905 | Time 1.2180(1.2134) | Loss 3.003619(3.014382) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1906 | Time 1.2107(1.2132) | Loss 3.059390(3.017532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1907 | Time 1.2053(1.2126) | Loss 3.023196(3.017929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1908 | Time 1.2106(1.2125) | Loss 3.008662(3.017280) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1909 | Time 1.2080(1.2122) | Loss 3.003684(3.016328) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1910 | Time 1.2088(1.2119) | Loss 3.069017(3.020017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1911 | Time 1.1934(1.2106) | Loss 3.016784(3.019790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1912 | Time 1.2076(1.2104) | Loss 3.042896(3.021408) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1913 | Time 1.2083(1.2103) | Loss 3.083839(3.025778) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1914 | Time 1.2224(1.2111) | Loss 3.157666(3.035010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1915 | Time 1.2146(1.2114) | Loss 3.091877(3.038991) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1916 | Time 1.1868(1.2097) | Loss 3.003091(3.036478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1917 | Time 1.2270(1.2109) | Loss 3.074639(3.039149) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1918 | Time 1.2085(1.2107) | Loss 3.109525(3.044075) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1919 | Time 1.2015(1.2100) | Loss 3.132213(3.050245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1920 | Time 1.1965(1.2091) | Loss 3.078866(3.052249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1921 | Time 1.2135(1.2094) | Loss 3.113695(3.056550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1922 | Time 1.2662(1.2134) | Loss 3.074627(3.057815) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1923 | Time 1.2568(1.2164) | Loss 3.087187(3.059871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1924 | Time 1.2328(1.2176) | Loss 3.051744(3.059302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1925 | Time 1.2097(1.2170) | Loss 3.067861(3.059901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1926 | Time 1.2151(1.2169) | Loss 3.049918(3.059203) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1927 | Time 1.2148(1.2167) | Loss 3.131909(3.064292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1928 | Time 1.2316(1.2178) | Loss 3.053006(3.063502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1929 | Time 1.2128(1.2174) | Loss 3.065902(3.063670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1930 | Time 1.1995(1.2162) | Loss 3.025160(3.060974) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1931 | Time 1.2649(1.2196) | Loss 3.069560(3.061575) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1932 | Time 1.2280(1.2202) | Loss 3.102371(3.064431) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1933 | Time 1.2110(1.2195) | Loss 3.029882(3.062013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1934 | Time 1.1960(1.2179) | Loss 3.136860(3.067252) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1935 | Time 1.2002(1.2166) | Loss 3.047623(3.065878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1936 | Time 1.1983(1.2154) | Loss 3.069778(3.066151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1937 | Time 1.1784(1.2128) | Loss 3.036097(3.064047) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1938 | Time 1.1798(1.2105) | Loss 3.019748(3.060946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1939 | Time 1.1838(1.2086) | Loss 2.950494(3.053215) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1940 | Time 1.1969(1.2078) | Loss 2.999838(3.049478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1941 | Time 1.2095(1.2079) | Loss 2.927230(3.040921) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1942 | Time 1.2010(1.2074) | Loss 3.008677(3.038664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1943 | Time 1.1870(1.2060) | Loss 2.925784(3.030762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1944 | Time 1.1886(1.2048) | Loss 2.971546(3.026617) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1945 | Time 1.2119(1.2053) | Loss 2.932512(3.020030) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1946 | Time 1.2048(1.2052) | Loss 2.980605(3.017270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1947 | Time 1.2001(1.2049) | Loss 2.927371(3.010977) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1948 | Time 1.1911(1.2039) | Loss 2.937429(3.005829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1949 | Time 1.2159(1.2047) | Loss 2.926734(3.000292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1950 | Time 1.2092(1.2051) | Loss 2.917627(2.994506) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1951 | Time 1.2001(1.2047) | Loss 2.899036(2.987823) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1952 | Time 1.2256(1.2062) | Loss 3.041718(2.991595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1953 | Time 1.1980(1.2056) | Loss 2.972968(2.990291) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1954 | Time 1.2360(1.2077) | Loss 3.053527(2.994718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1955 | Time 1.2342(1.2096) | Loss 3.041779(2.998012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1956 | Time 1.2178(1.2102) | Loss 2.994571(2.997771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1957 | Time 1.2157(1.2105) | Loss 3.081574(3.003638) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1958 | Time 1.2062(1.2102) | Loss 3.070205(3.008297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1959 | Time 1.2004(1.2096) | Loss 2.955833(3.004625) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1960 | Time 1.2070(1.2094) | Loss 2.978439(3.002792) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1961 | Time 1.2335(1.2111) | Loss 2.978060(3.001061) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1962 | Time 1.2088(1.2109) | Loss 2.989283(3.000236) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1963 | Time 1.1815(1.2088) | Loss 2.990353(2.999544) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1964 | Time 1.1804(1.2069) | Loss 2.931780(2.994801) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1965 | Time 1.1913(1.2058) | Loss 2.943367(2.991200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1966 | Time 1.2224(1.2069) | Loss 2.942176(2.987769) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1967 | Time 1.2097(1.2071) | Loss 2.986544(2.987683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1968 | Time 1.1879(1.2058) | Loss 3.024955(2.990292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1969 | Time 1.2067(1.2058) | Loss 2.949095(2.987408) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1970 | Time 1.1760(1.2038) | Loss 2.970599(2.986231) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1971 | Time 1.1912(1.2029) | Loss 3.025374(2.988971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1972 | Time 1.1868(1.2018) | Loss 2.976758(2.988116) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1973 | Time 1.2050(1.2020) | Loss 2.979845(2.987537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1974 | Time 1.1939(1.2014) | Loss 3.087979(2.994568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1975 | Time 1.2247(1.2031) | Loss 3.032408(2.997217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1976 | Time 1.2404(1.2057) | Loss 3.003127(2.997631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1977 | Time 1.2546(1.2091) | Loss 3.033125(3.000115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1978 | Time 1.2946(1.2151) | Loss 3.072634(3.005192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1979 | Time 1.2552(1.2179) | Loss 3.122981(3.013437) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1980 | Time 1.2084(1.2172) | Loss 3.107403(3.020015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1981 | Time 1.2289(1.2180) | Loss 3.118359(3.026899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1982 | Time 1.2402(1.2196) | Loss 3.124933(3.033761) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1983 | Time 1.2024(1.2184) | Loss 3.110316(3.039120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1984 | Time 1.1960(1.2168) | Loss 3.082750(3.042174) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1985 | Time 1.2088(1.2163) | Loss 3.084916(3.045166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1986 | Time 1.2169(1.2163) | Loss 3.088685(3.048212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1987 | Time 1.2088(1.2158) | Loss 3.094625(3.051461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1988 | Time 1.2139(1.2157) | Loss 3.030490(3.049993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1989 | Time 1.2107(1.2153) | Loss 3.014233(3.047490) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1990 | Time 1.2071(1.2147) | Loss 3.010727(3.044917) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1991 | Time 1.1997(1.2137) | Loss 3.055505(3.045658) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1992 | Time 1.1756(1.2110) | Loss 3.037488(3.045086) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1993 | Time 1.1922(1.2097) | Loss 2.973140(3.040050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1994 | Time 1.2013(1.2091) | Loss 2.971881(3.035278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1995 | Time 1.2234(1.2101) | Loss 2.994896(3.032451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1996 | Time 1.2368(1.2120) | Loss 3.005251(3.030547) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1997 | Time 1.1790(1.2097) | Loss 2.988389(3.027596) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1998 | Time 1.2094(1.2097) | Loss 2.958566(3.022764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 1999 | Time 1.2137(1.2099) | Loss 2.982744(3.019963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2000 | Time 1.2002(1.2093) | Loss 2.908211(3.012140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2000 | Test Loss 3.019896 | NFE 20
Skipping vis as data dimension is >2
Iter 2001 | Time 1.2179(1.2099) | Loss 2.927316(3.006202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2002 | Time 1.2091(1.2098) | Loss 2.999863(3.005759) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2003 | Time 1.2163(1.2103) | Loss 2.984518(3.004272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2004 | Time 1.2015(1.2096) | Loss 2.951518(3.000579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2005 | Time 1.2046(1.2093) | Loss 2.915087(2.994595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2006 | Time 1.2310(1.2108) | Loss 2.969944(2.992869) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2007 | Time 1.2175(1.2113) | Loss 3.029499(2.995433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2008 | Time 1.2007(1.2105) | Loss 2.892681(2.988241) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2009 | Time 1.2091(1.2104) | Loss 2.978831(2.987582) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2010 | Time 1.2060(1.2101) | Loss 2.932922(2.983756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2011 | Time 1.2068(1.2099) | Loss 2.984963(2.983840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2012 | Time 1.1951(1.2089) | Loss 2.980659(2.983618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2013 | Time 1.1985(1.2081) | Loss 2.969210(2.982609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2014 | Time 1.1943(1.2072) | Loss 3.032229(2.986082) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2015 | Time 1.2544(1.2105) | Loss 3.012132(2.987906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2016 | Time 1.2555(1.2136) | Loss 3.024811(2.990489) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2017 | Time 1.2149(1.2137) | Loss 3.121567(2.999665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2018 | Time 1.2024(1.2129) | Loss 3.045414(3.002867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2019 | Time 1.2136(1.2130) | Loss 3.125877(3.011478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2020 | Time 1.1904(1.2114) | Loss 3.062657(3.015060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2021 | Time 1.2117(1.2114) | Loss 3.081768(3.019730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2022 | Time 1.1952(1.2103) | Loss 3.107954(3.025906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2023 | Time 1.1859(1.2086) | Loss 3.114903(3.032135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2024 | Time 1.2078(1.2085) | Loss 3.027442(3.031807) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2025 | Time 1.2069(1.2084) | Loss 3.101470(3.036683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2026 | Time 1.1909(1.2072) | Loss 3.150303(3.044637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2027 | Time 1.1892(1.2059) | Loss 3.116260(3.049650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2028 | Time 1.1882(1.2047) | Loss 3.153603(3.056927) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2029 | Time 1.2194(1.2057) | Loss 3.160979(3.064211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2030 | Time 1.2265(1.2072) | Loss 3.218071(3.074981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2031 | Time 1.3014(1.2138) | Loss 3.135924(3.079247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2032 | Time 1.2330(1.2151) | Loss 3.185991(3.086719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2033 | Time 1.2314(1.2163) | Loss 3.109535(3.088316) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2034 | Time 1.2434(1.2182) | Loss 3.174709(3.094364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2035 | Time 1.2003(1.2169) | Loss 3.107841(3.095307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2036 | Time 1.2229(1.2173) | Loss 3.083116(3.094454) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2037 | Time 1.2283(1.2181) | Loss 3.137288(3.097452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2038 | Time 1.1997(1.2168) | Loss 3.062953(3.095037) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2039 | Time 1.2000(1.2156) | Loss 3.060088(3.092591) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2040 | Time 1.2032(1.2148) | Loss 3.031759(3.088332) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2041 | Time 1.2050(1.2141) | Loss 3.003608(3.082402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2042 | Time 1.1983(1.2130) | Loss 3.124194(3.085327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2043 | Time 1.2209(1.2135) | Loss 3.077067(3.084749) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2044 | Time 1.1965(1.2123) | Loss 3.079392(3.084374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2045 | Time 1.1963(1.2112) | Loss 3.046503(3.081723) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2046 | Time 1.2019(1.2106) | Loss 3.047463(3.079325) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2047 | Time 1.2143(1.2108) | Loss 3.038023(3.076434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2048 | Time 1.2101(1.2108) | Loss 3.049388(3.074540) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2049 | Time 1.2084(1.2106) | Loss 3.069871(3.074214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2050 | Time 1.2127(1.2108) | Loss 3.030117(3.071127) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2051 | Time 1.1961(1.2097) | Loss 3.135850(3.075657) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2052 | Time 1.2025(1.2092) | Loss 3.051998(3.074001) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2053 | Time 1.2018(1.2087) | Loss 3.147656(3.079157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2054 | Time 1.1880(1.2073) | Loss 3.182769(3.086410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2055 | Time 1.1904(1.2061) | Loss 3.062332(3.084724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2056 | Time 1.2076(1.2062) | Loss 3.141553(3.088702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2057 | Time 1.2040(1.2060) | Loss 3.102484(3.089667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2058 | Time 1.2086(1.2062) | Loss 3.080748(3.089043) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2059 | Time 1.1917(1.2052) | Loss 3.068723(3.087620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2060 | Time 1.2137(1.2058) | Loss 3.091185(3.087870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2061 | Time 1.2024(1.2056) | Loss 3.128585(3.090720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2062 | Time 1.2017(1.2053) | Loss 3.106955(3.091856) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2063 | Time 1.1739(1.2031) | Loss 3.075933(3.090742) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2064 | Time 1.1858(1.2019) | Loss 3.118306(3.092671) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2065 | Time 1.2019(1.2019) | Loss 3.123749(3.094847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2066 | Time 1.2019(1.2019) | Loss 3.159897(3.099400) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2067 | Time 1.2011(1.2018) | Loss 3.191482(3.105846) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2068 | Time 1.2289(1.2037) | Loss 3.125054(3.107190) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2069 | Time 1.1977(1.2033) | Loss 3.129340(3.108741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2070 | Time 1.2015(1.2032) | Loss 3.120527(3.109566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2071 | Time 1.2485(1.2063) | Loss 3.228409(3.117885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2072 | Time 1.2245(1.2076) | Loss 3.147625(3.119967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2073 | Time 1.2266(1.2089) | Loss 3.190110(3.124877) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2074 | Time 1.1930(1.2078) | Loss 3.125411(3.124914) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2075 | Time 1.1909(1.2066) | Loss 3.240764(3.133024) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2076 | Time 1.1821(1.2049) | Loss 3.144641(3.133837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2077 | Time 1.1815(1.2033) | Loss 3.180063(3.137073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2078 | Time 1.1844(1.2020) | Loss 3.224172(3.143170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2079 | Time 1.1834(1.2007) | Loss 3.139054(3.142882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2080 | Time 1.2164(1.2018) | Loss 3.077970(3.138338) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2081 | Time 1.2023(1.2018) | Loss 3.154266(3.139453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2082 | Time 1.2040(1.2020) | Loss 3.116937(3.137877) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2083 | Time 1.2058(1.2022) | Loss 3.192782(3.141720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2084 | Time 1.2093(1.2027) | Loss 3.150568(3.142339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2085 | Time 1.1807(1.2012) | Loss 3.220951(3.147842) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2086 | Time 1.2014(1.2012) | Loss 3.253037(3.155206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2087 | Time 1.1973(1.2009) | Loss 3.227259(3.160249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2088 | Time 1.2341(1.2033) | Loss 3.265038(3.167585) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2089 | Time 1.2147(1.2041) | Loss 3.221007(3.171324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2090 | Time 1.1988(1.2037) | Loss 3.225250(3.175099) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2091 | Time 1.2192(1.2048) | Loss 3.305815(3.184249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2092 | Time 1.2070(1.2049) | Loss 3.250087(3.188858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2093 | Time 1.2208(1.2060) | Loss 3.291378(3.196034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2094 | Time 1.2090(1.2063) | Loss 3.339441(3.206073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2095 | Time 1.2383(1.2085) | Loss 3.236698(3.208216) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2096 | Time 1.2613(1.2122) | Loss 3.209215(3.208286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2097 | Time 1.1953(1.2110) | Loss 3.175803(3.206013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2098 | Time 1.1822(1.2090) | Loss 3.216012(3.206713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2099 | Time 1.1816(1.2071) | Loss 3.201408(3.206341) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2100 | Time 1.1977(1.2064) | Loss 3.292032(3.212340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2100 | Test Loss 3.230696 | NFE 20
Skipping vis as data dimension is >2
Iter 2101 | Time 1.2007(1.2060) | Loss 3.242585(3.214457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2102 | Time 1.1777(1.2040) | Loss 3.236283(3.215985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2103 | Time 1.1903(1.2031) | Loss 3.179563(3.213435) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2104 | Time 1.2055(1.2032) | Loss 3.181556(3.211204) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2105 | Time 1.2081(1.2036) | Loss 3.197443(3.210240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2106 | Time 1.2084(1.2039) | Loss 3.197363(3.209339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2107 | Time 1.2219(1.2052) | Loss 3.162744(3.206077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2108 | Time 1.2424(1.2078) | Loss 3.173028(3.203764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2109 | Time 1.2097(1.2079) | Loss 3.160161(3.200712) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2110 | Time 1.2152(1.2084) | Loss 3.193534(3.200209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2111 | Time 1.2147(1.2089) | Loss 3.220519(3.201631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2112 | Time 1.2159(1.2094) | Loss 3.139469(3.197280) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2113 | Time 1.2094(1.2094) | Loss 3.228808(3.199487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2114 | Time 1.2435(1.2118) | Loss 3.180388(3.198150) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2115 | Time 1.2505(1.2145) | Loss 3.167789(3.196024) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2116 | Time 1.2534(1.2172) | Loss 3.191168(3.195685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2117 | Time 1.2275(1.2179) | Loss 3.192397(3.195454) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2118 | Time 1.2247(1.2184) | Loss 3.191771(3.195197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2119 | Time 1.2227(1.2187) | Loss 3.223001(3.197143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2120 | Time 1.2236(1.2190) | Loss 3.269543(3.202211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2121 | Time 1.2051(1.2181) | Loss 3.265749(3.206658) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2122 | Time 1.2142(1.2178) | Loss 3.275587(3.211483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2123 | Time 1.2047(1.2169) | Loss 3.257462(3.214702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2124 | Time 1.2001(1.2157) | Loss 3.381728(3.226394) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2125 | Time 1.1961(1.2143) | Loss 3.288089(3.230712) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2126 | Time 1.1951(1.2130) | Loss 3.427990(3.244522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2127 | Time 1.2497(1.2156) | Loss 3.395377(3.255082) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2128 | Time 1.2231(1.2161) | Loss 3.386336(3.264270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2129 | Time 1.2177(1.2162) | Loss 3.312482(3.267644) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2130 | Time 1.1841(1.2139) | Loss 3.326797(3.271785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2131 | Time 1.1875(1.2121) | Loss 3.416195(3.281894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2132 | Time 1.1835(1.2101) | Loss 3.308735(3.283773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2133 | Time 1.1964(1.2091) | Loss 3.307043(3.285402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2134 | Time 1.1959(1.2082) | Loss 3.412947(3.294330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2135 | Time 1.1943(1.2072) | Loss 3.252093(3.291373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2136 | Time 1.2461(1.2100) | Loss 3.334157(3.294368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2137 | Time 1.2031(1.2095) | Loss 3.234542(3.290180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2138 | Time 1.2097(1.2095) | Loss 3.288591(3.290069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2139 | Time 1.1964(1.2086) | Loss 3.368118(3.295532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2140 | Time 1.1922(1.2074) | Loss 3.277493(3.294270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2141 | Time 1.2159(1.2080) | Loss 3.244957(3.290818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2142 | Time 1.2073(1.2080) | Loss 3.348446(3.294852) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2143 | Time 1.1989(1.2073) | Loss 3.323200(3.296836) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2144 | Time 1.2048(1.2072) | Loss 3.412751(3.304950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2145 | Time 1.2062(1.2071) | Loss 3.354587(3.308425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2146 | Time 1.2162(1.2077) | Loss 3.374650(3.313060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2147 | Time 1.1974(1.2070) | Loss 3.279301(3.310697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2148 | Time 1.1932(1.2060) | Loss 3.268149(3.307719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2149 | Time 1.1981(1.2055) | Loss 3.297564(3.307008) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2150 | Time 1.1883(1.2043) | Loss 3.343021(3.309529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2151 | Time 1.1965(1.2037) | Loss 3.264803(3.306398) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2152 | Time 1.1995(1.2034) | Loss 3.248892(3.302373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2153 | Time 1.2156(1.2043) | Loss 3.243060(3.298221) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2154 | Time 1.2536(1.2077) | Loss 3.259382(3.295502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2155 | Time 1.1961(1.2069) | Loss 3.311488(3.296621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2156 | Time 1.1933(1.2060) | Loss 3.294425(3.296467) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2157 | Time 1.1917(1.2050) | Loss 3.366651(3.301380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2158 | Time 1.1871(1.2037) | Loss 3.308193(3.301857) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2159 | Time 1.1912(1.2028) | Loss 3.335274(3.304196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2160 | Time 1.1939(1.2022) | Loss 3.404656(3.311228) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2161 | Time 1.1935(1.2016) | Loss 3.376149(3.315773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2162 | Time 1.1956(1.2012) | Loss 3.317310(3.315881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2163 | Time 1.1962(1.2008) | Loss 3.234154(3.310160) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2164 | Time 1.2034(1.2010) | Loss 3.269057(3.307282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2165 | Time 1.1978(1.2008) | Loss 3.327851(3.308722) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2166 | Time 1.2025(1.2009) | Loss 3.268553(3.305910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2167 | Time 1.1944(1.2005) | Loss 3.328867(3.307517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2168 | Time 1.2015(1.2005) | Loss 3.392033(3.313434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2169 | Time 1.2191(1.2018) | Loss 3.352440(3.316164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2170 | Time 1.1963(1.2014) | Loss 3.339511(3.317798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2171 | Time 1.2239(1.2030) | Loss 3.332085(3.318798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2172 | Time 1.1896(1.2021) | Loss 3.335518(3.319969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2173 | Time 1.2625(1.2063) | Loss 3.316406(3.319719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2174 | Time 1.2132(1.2068) | Loss 3.289604(3.317611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2175 | Time 1.1929(1.2058) | Loss 3.378659(3.321885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2176 | Time 1.2110(1.2062) | Loss 3.327535(3.322280) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2177 | Time 1.1961(1.2055) | Loss 3.439940(3.330516) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2178 | Time 1.2134(1.2060) | Loss 3.276198(3.326714) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2179 | Time 1.1885(1.2048) | Loss 3.384529(3.330761) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2180 | Time 1.2053(1.2048) | Loss 3.421778(3.337132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2181 | Time 1.1862(1.2035) | Loss 3.447963(3.344890) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2182 | Time 1.1856(1.2023) | Loss 3.373341(3.346882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2183 | Time 1.2056(1.2025) | Loss 3.430536(3.352738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2184 | Time 1.1922(1.2018) | Loss 3.440319(3.358868) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2185 | Time 1.1837(1.2005) | Loss 3.356392(3.358695) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2186 | Time 1.2031(1.2007) | Loss 3.403613(3.361839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2187 | Time 1.1925(1.2001) | Loss 3.343245(3.360538) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2188 | Time 1.2018(1.2002) | Loss 3.369234(3.361146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2189 | Time 1.1916(1.1996) | Loss 3.355302(3.360737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2190 | Time 1.1917(1.1991) | Loss 3.293419(3.356025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2191 | Time 1.1963(1.1989) | Loss 3.344151(3.355194) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2192 | Time 1.2062(1.1994) | Loss 3.337500(3.353955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2193 | Time 1.1971(1.1992) | Loss 3.313513(3.351124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2194 | Time 1.1861(1.1983) | Loss 3.287667(3.346682) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2195 | Time 1.1974(1.1983) | Loss 3.267138(3.341114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2196 | Time 1.2029(1.1986) | Loss 3.290266(3.337555) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2197 | Time 1.2191(1.2000) | Loss 3.261260(3.332214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2198 | Time 1.2077(1.2006) | Loss 3.230777(3.325114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2199 | Time 1.2025(1.2007) | Loss 3.239866(3.319146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2200 | Time 1.1994(1.2006) | Loss 3.295964(3.317523) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2200 | Test Loss 3.309720 | NFE 20
Skipping vis as data dimension is >2
Iter 2201 | Time 1.1962(1.2003) | Loss 3.292291(3.315757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2202 | Time 1.1885(1.1995) | Loss 3.221683(3.309172) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2203 | Time 1.2034(1.1997) | Loss 3.269394(3.306388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2204 | Time 1.2016(1.1999) | Loss 3.218009(3.300201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2205 | Time 1.2042(1.2002) | Loss 3.241712(3.296107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2206 | Time 1.1942(1.1998) | Loss 3.322918(3.297984) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2207 | Time 1.1964(1.1995) | Loss 3.268796(3.295941) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2208 | Time 1.2057(1.2000) | Loss 3.312677(3.297112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2209 | Time 1.2020(1.2001) | Loss 3.263907(3.294788) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2210 | Time 1.1889(1.1993) | Loss 3.260615(3.292396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2211 | Time 1.1798(1.1980) | Loss 3.224772(3.287662) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2212 | Time 1.1868(1.1972) | Loss 3.278059(3.286990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2213 | Time 1.1837(1.1962) | Loss 3.229964(3.282998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2214 | Time 1.1988(1.1964) | Loss 3.293368(3.283724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2215 | Time 1.1913(1.1961) | Loss 3.324580(3.286584) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2216 | Time 1.1789(1.1949) | Loss 3.205853(3.280933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2217 | Time 1.1849(1.1942) | Loss 3.161536(3.272575) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2218 | Time 1.1719(1.1926) | Loss 3.193417(3.267034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2219 | Time 1.1962(1.1929) | Loss 3.387151(3.275442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2220 | Time 1.1862(1.1924) | Loss 3.220738(3.271613) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2221 | Time 1.1901(1.1922) | Loss 3.274217(3.271795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2222 | Time 1.1831(1.1916) | Loss 3.278779(3.272284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2223 | Time 1.2160(1.1933) | Loss 3.294609(3.273847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2224 | Time 1.2597(1.1979) | Loss 3.357140(3.279677) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2225 | Time 1.2013(1.1982) | Loss 3.336707(3.283669) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2226 | Time 1.2060(1.1987) | Loss 3.360169(3.289024) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2227 | Time 1.2134(1.1998) | Loss 3.279340(3.288346) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2228 | Time 1.2178(1.2010) | Loss 3.416068(3.297287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2229 | Time 1.1927(1.2004) | Loss 3.375356(3.302752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2230 | Time 1.2100(1.2011) | Loss 3.393979(3.309138) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2231 | Time 1.2065(1.2015) | Loss 3.321663(3.310015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2232 | Time 1.2125(1.2022) | Loss 3.333406(3.311652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2233 | Time 1.2068(1.2026) | Loss 3.327752(3.312779) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2234 | Time 1.2556(1.2063) | Loss 3.334793(3.314320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2235 | Time 1.2626(1.2102) | Loss 3.314504(3.314333) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2236 | Time 1.1938(1.2091) | Loss 3.340358(3.316155) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2237 | Time 1.1971(1.2082) | Loss 3.271413(3.313023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2238 | Time 1.1846(1.2066) | Loss 3.327587(3.314042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2239 | Time 1.1960(1.2058) | Loss 3.423461(3.321702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2240 | Time 1.1949(1.2051) | Loss 3.356687(3.324150) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2241 | Time 1.2066(1.2052) | Loss 3.391579(3.328870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2242 | Time 1.1957(1.2045) | Loss 3.301188(3.326933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2243 | Time 1.1822(1.2030) | Loss 3.284128(3.323936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2244 | Time 1.2068(1.2032) | Loss 3.285456(3.321243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2245 | Time 1.2174(1.2042) | Loss 3.382579(3.325536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2246 | Time 1.2079(1.2045) | Loss 3.364777(3.328283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2247 | Time 1.1920(1.2036) | Loss 3.276328(3.324646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2248 | Time 1.2039(1.2036) | Loss 3.388531(3.329118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2249 | Time 1.2079(1.2039) | Loss 3.388234(3.333256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2250 | Time 1.2107(1.2044) | Loss 3.396074(3.337654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2251 | Time 1.2300(1.2062) | Loss 3.331736(3.337239) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2252 | Time 1.1985(1.2057) | Loss 3.420109(3.343040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2253 | Time 1.2842(1.2112) | Loss 3.319470(3.341390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2254 | Time 1.2331(1.2127) | Loss 3.327934(3.340448) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2255 | Time 1.2181(1.2131) | Loss 3.361819(3.341944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2256 | Time 1.2115(1.2130) | Loss 3.340356(3.341833) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2257 | Time 1.2259(1.2139) | Loss 3.309059(3.339539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2258 | Time 1.2236(1.2145) | Loss 3.318008(3.338032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2259 | Time 1.2685(1.2183) | Loss 3.368851(3.340189) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2260 | Time 1.2030(1.2172) | Loss 3.496410(3.351125) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2261 | Time 1.2057(1.2164) | Loss 3.435559(3.357035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2262 | Time 1.2018(1.2154) | Loss 3.417535(3.361270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2263 | Time 1.1992(1.2143) | Loss 3.429480(3.366045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2264 | Time 1.1942(1.2129) | Loss 3.378752(3.366934) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2265 | Time 1.2035(1.2122) | Loss 3.357112(3.366247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2266 | Time 1.2075(1.2119) | Loss 3.420172(3.370021) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2267 | Time 1.2084(1.2116) | Loss 3.360397(3.369348) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2268 | Time 1.2683(1.2156) | Loss 3.312013(3.365334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2269 | Time 1.2117(1.2153) | Loss 3.416572(3.368921) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2270 | Time 1.2039(1.2145) | Loss 3.382961(3.369904) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2271 | Time 1.2027(1.2137) | Loss 3.395499(3.371695) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2272 | Time 1.1872(1.2119) | Loss 3.355461(3.370559) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2273 | Time 1.1816(1.2097) | Loss 3.426692(3.374488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2274 | Time 1.1799(1.2076) | Loss 3.431437(3.378475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2275 | Time 1.1849(1.2060) | Loss 3.358553(3.377080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2276 | Time 1.1884(1.2048) | Loss 3.445449(3.381866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2277 | Time 1.2247(1.2062) | Loss 3.408092(3.383702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2278 | Time 1.2316(1.2080) | Loss 3.289433(3.377103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2279 | Time 1.2068(1.2079) | Loss 3.356601(3.375668) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2280 | Time 1.2004(1.2074) | Loss 3.332609(3.372654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2281 | Time 1.2093(1.2075) | Loss 3.327575(3.369498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2282 | Time 1.1901(1.2063) | Loss 3.312657(3.365519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2283 | Time 1.2064(1.2063) | Loss 3.367343(3.365647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2284 | Time 1.2129(1.2068) | Loss 3.458830(3.372170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2285 | Time 1.1940(1.2059) | Loss 3.492118(3.380566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2286 | Time 1.1823(1.2042) | Loss 3.532382(3.391193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2287 | Time 1.1931(1.2034) | Loss 3.431717(3.394030) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2288 | Time 1.1881(1.2024) | Loss 3.528557(3.403447) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2289 | Time 1.1844(1.2011) | Loss 3.463652(3.407661) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2290 | Time 1.1886(1.2002) | Loss 3.569048(3.418958) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2291 | Time 1.1907(1.1996) | Loss 3.549655(3.428107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2292 | Time 1.1922(1.1991) | Loss 3.418883(3.427461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2293 | Time 1.1972(1.1989) | Loss 3.425493(3.427324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2294 | Time 1.1945(1.1986) | Loss 3.450528(3.428948) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2295 | Time 1.2059(1.1991) | Loss 3.471816(3.431949) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2296 | Time 1.2107(1.1999) | Loss 3.412725(3.430603) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2297 | Time 1.1840(1.1988) | Loss 3.454798(3.432297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2298 | Time 1.1863(1.1980) | Loss 3.456168(3.433968) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2299 | Time 1.1928(1.1976) | Loss 3.503165(3.438811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2300 | Time 1.1905(1.1971) | Loss 3.476540(3.441452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2300 | Test Loss 3.435807 | NFE 20
Skipping vis as data dimension is >2
Iter 2301 | Time 1.2200(1.1987) | Loss 3.374216(3.436746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2302 | Time 1.2067(1.1993) | Loss 3.436821(3.436751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2303 | Time 1.1992(1.1993) | Loss 3.413567(3.435128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2304 | Time 1.2054(1.1997) | Loss 3.418656(3.433975) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2305 | Time 1.2258(1.2015) | Loss 3.379944(3.430193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2306 | Time 1.2133(1.2023) | Loss 3.468267(3.432858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2307 | Time 1.1944(1.2018) | Loss 3.342162(3.426509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2308 | Time 1.1936(1.2012) | Loss 3.379098(3.423191) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2309 | Time 1.1994(1.2011) | Loss 3.391527(3.420974) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2310 | Time 1.1911(1.2004) | Loss 3.403224(3.419732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2311 | Time 1.1981(1.2002) | Loss 3.406204(3.418785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2312 | Time 1.2699(1.2051) | Loss 3.440346(3.420294) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2313 | Time 1.2136(1.2057) | Loss 3.421271(3.420362) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2314 | Time 1.2088(1.2059) | Loss 3.336325(3.414480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2315 | Time 1.1932(1.2050) | Loss 3.417340(3.414680) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2316 | Time 1.2654(1.2092) | Loss 3.381414(3.412351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2317 | Time 1.2049(1.2089) | Loss 3.398661(3.411393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2318 | Time 1.1922(1.2078) | Loss 3.349535(3.407063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2319 | Time 1.2086(1.2078) | Loss 3.320046(3.400972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2320 | Time 1.2030(1.2075) | Loss 3.287735(3.393045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2321 | Time 1.2002(1.2070) | Loss 3.261350(3.383827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2322 | Time 1.2088(1.2071) | Loss 3.265839(3.375567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2323 | Time 1.2249(1.2083) | Loss 3.256816(3.367255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2324 | Time 1.2025(1.2079) | Loss 3.344481(3.365661) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2325 | Time 1.2140(1.2084) | Loss 3.306290(3.361505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2326 | Time 1.2005(1.2078) | Loss 3.312171(3.358051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2327 | Time 1.1957(1.2070) | Loss 3.347158(3.357289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2328 | Time 1.2300(1.2086) | Loss 3.356049(3.357202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2329 | Time 1.2305(1.2101) | Loss 3.334147(3.355588) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2330 | Time 1.2139(1.2104) | Loss 3.400093(3.358703) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2331 | Time 1.2096(1.2103) | Loss 3.375389(3.359871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2332 | Time 1.2312(1.2118) | Loss 3.404324(3.362983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2333 | Time 1.2043(1.2113) | Loss 3.490332(3.371898) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2334 | Time 1.2218(1.2120) | Loss 3.449566(3.377334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2335 | Time 1.2074(1.2117) | Loss 3.483805(3.384787) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2336 | Time 1.2121(1.2117) | Loss 3.428375(3.387838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2337 | Time 1.1942(1.2105) | Loss 3.466186(3.393323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2338 | Time 1.2180(1.2110) | Loss 3.409066(3.394425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2339 | Time 1.2155(1.2113) | Loss 3.475012(3.400066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2340 | Time 1.2186(1.2118) | Loss 3.468699(3.404870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2341 | Time 1.1936(1.2105) | Loss 3.390825(3.403887) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2342 | Time 1.1843(1.2087) | Loss 3.416970(3.404803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2343 | Time 1.1864(1.2071) | Loss 3.389711(3.403746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2344 | Time 1.1890(1.2059) | Loss 3.335495(3.398969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2345 | Time 1.2128(1.2064) | Loss 3.385738(3.398043) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2346 | Time 1.1842(1.2048) | Loss 3.374022(3.396361) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2347 | Time 1.2051(1.2048) | Loss 3.461957(3.400953) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2348 | Time 1.1836(1.2033) | Loss 3.367907(3.398640) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2349 | Time 1.1856(1.2021) | Loss 3.499524(3.405701) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2350 | Time 1.1826(1.2007) | Loss 3.524035(3.413985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2351 | Time 1.2006(1.2007) | Loss 3.514663(3.421032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2352 | Time 1.1954(1.2004) | Loss 3.555419(3.430439) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2353 | Time 1.2076(1.2009) | Loss 3.513395(3.436246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2354 | Time 1.1886(1.2000) | Loss 3.432133(3.435958) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2355 | Time 1.1872(1.1991) | Loss 3.500354(3.440466) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2356 | Time 1.1927(1.1987) | Loss 3.497144(3.444433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2357 | Time 1.1842(1.1976) | Loss 3.515806(3.449429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2358 | Time 1.1979(1.1977) | Loss 3.497102(3.452767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2359 | Time 1.2204(1.1993) | Loss 3.506659(3.456539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2360 | Time 1.2289(1.2013) | Loss 3.448056(3.455945) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2361 | Time 1.2021(1.2014) | Loss 3.455607(3.455921) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2362 | Time 1.2085(1.2019) | Loss 3.543133(3.462026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2363 | Time 1.2162(1.2029) | Loss 3.456788(3.461660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2364 | Time 1.2391(1.2054) | Loss 3.551989(3.467983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2365 | Time 1.2233(1.2067) | Loss 3.418757(3.464537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2366 | Time 1.2325(1.2085) | Loss 3.392717(3.459510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2367 | Time 1.2321(1.2101) | Loss 3.483412(3.461183) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2368 | Time 1.2360(1.2119) | Loss 3.354850(3.453739) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2369 | Time 1.2224(1.2127) | Loss 3.256379(3.439924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2370 | Time 1.2117(1.2126) | Loss 3.293345(3.429664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2371 | Time 1.2251(1.2135) | Loss 3.328546(3.422585) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2372 | Time 1.2131(1.2135) | Loss 3.237264(3.409613) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2373 | Time 1.1904(1.2118) | Loss 3.282262(3.400698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2374 | Time 1.1906(1.2104) | Loss 3.266856(3.391329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2375 | Time 1.2027(1.2098) | Loss 3.317215(3.386141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2376 | Time 1.2805(1.2148) | Loss 3.323729(3.381773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2377 | Time 1.2017(1.2139) | Loss 3.332111(3.378296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2378 | Time 1.1927(1.2124) | Loss 3.344177(3.375908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2379 | Time 1.1904(1.2108) | Loss 3.345142(3.373754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2380 | Time 1.2049(1.2104) | Loss 3.406657(3.376057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2381 | Time 1.1976(1.2095) | Loss 3.381686(3.376451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2382 | Time 1.1919(1.2083) | Loss 3.359303(3.375251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2383 | Time 1.2160(1.2088) | Loss 3.349684(3.373461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2384 | Time 1.2429(1.2112) | Loss 3.452037(3.378962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2385 | Time 1.1827(1.2092) | Loss 3.371711(3.378454) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2386 | Time 1.2224(1.2101) | Loss 3.370185(3.377875) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2387 | Time 1.2090(1.2101) | Loss 3.373774(3.377588) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2388 | Time 1.2030(1.2096) | Loss 3.325712(3.373957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2389 | Time 1.1924(1.2084) | Loss 3.397538(3.375608) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2390 | Time 1.2111(1.2086) | Loss 3.371612(3.375328) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2391 | Time 1.1935(1.2075) | Loss 3.447562(3.380384) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2392 | Time 1.2004(1.2070) | Loss 3.413978(3.382736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2393 | Time 1.2006(1.2066) | Loss 3.397905(3.383798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2394 | Time 1.2005(1.2061) | Loss 3.355282(3.381802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2395 | Time 1.1862(1.2047) | Loss 3.253483(3.372819) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2396 | Time 1.1888(1.2036) | Loss 3.324608(3.369444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2397 | Time 1.2074(1.2039) | Loss 3.337625(3.367217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2398 | Time 1.1919(1.2031) | Loss 3.386057(3.368536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2399 | Time 1.1761(1.2012) | Loss 3.399827(3.370726) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2400 | Time 1.1804(1.1997) | Loss 3.372735(3.370867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2400 | Test Loss 3.361668 | NFE 20
Skipping vis as data dimension is >2
Iter 2401 | Time 1.2574(1.2038) | Loss 3.443131(3.375925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2402 | Time 1.2011(1.2036) | Loss 3.352608(3.374293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2403 | Time 1.1926(1.2028) | Loss 3.362143(3.373443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2404 | Time 1.1832(1.2014) | Loss 3.373365(3.373437) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2405 | Time 1.1929(1.2008) | Loss 3.483414(3.381136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2406 | Time 1.1866(1.1998) | Loss 3.404957(3.382803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2407 | Time 1.1941(1.1994) | Loss 3.450401(3.387535) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2408 | Time 1.2000(1.1995) | Loss 3.404299(3.388708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2409 | Time 1.1806(1.1981) | Loss 3.463760(3.393962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2410 | Time 1.2131(1.1992) | Loss 3.391917(3.393819) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2411 | Time 1.1905(1.1986) | Loss 3.404643(3.394576) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2412 | Time 1.1779(1.1971) | Loss 3.471253(3.399944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2413 | Time 1.2145(1.1984) | Loss 3.407816(3.400495) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2414 | Time 1.2181(1.1997) | Loss 3.391535(3.399868) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2415 | Time 1.1853(1.1987) | Loss 3.418349(3.401161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2416 | Time 1.1953(1.1985) | Loss 3.376295(3.399421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2417 | Time 1.1859(1.1976) | Loss 3.432112(3.401709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2418 | Time 1.1874(1.1969) | Loss 3.430676(3.403737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2419 | Time 1.1904(1.1964) | Loss 3.397404(3.403294) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2420 | Time 1.1906(1.1960) | Loss 3.414548(3.404081) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2421 | Time 1.1893(1.1956) | Loss 3.332336(3.399059) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2422 | Time 1.1858(1.1949) | Loss 3.385077(3.398080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2423 | Time 1.1793(1.1938) | Loss 3.324921(3.392959) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2424 | Time 1.2497(1.1977) | Loss 3.448744(3.396864) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2425 | Time 1.1880(1.1970) | Loss 3.370964(3.395051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2426 | Time 1.2251(1.1990) | Loss 3.394203(3.394992) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2427 | Time 1.2449(1.2022) | Loss 3.441038(3.398215) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2428 | Time 1.1939(1.2016) | Loss 3.364304(3.395841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2429 | Time 1.1951(1.2012) | Loss 3.416706(3.397302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2430 | Time 1.1854(1.2001) | Loss 3.480432(3.403121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2431 | Time 1.1723(1.1981) | Loss 3.416818(3.404080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2432 | Time 1.2007(1.1983) | Loss 3.402467(3.403967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2433 | Time 1.2550(1.2023) | Loss 3.454735(3.407521) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2434 | Time 1.2020(1.2022) | Loss 3.500460(3.414026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2435 | Time 1.1858(1.2011) | Loss 3.466010(3.417665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2436 | Time 1.1780(1.1995) | Loss 3.465482(3.421012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2437 | Time 1.1900(1.1988) | Loss 3.414167(3.420533) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2438 | Time 1.1901(1.1982) | Loss 3.399242(3.419043) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2439 | Time 1.1920(1.1978) | Loss 3.410874(3.418471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2440 | Time 1.1585(1.1950) | Loss 3.376272(3.415517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2441 | Time 1.1965(1.1951) | Loss 3.377250(3.412838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2442 | Time 1.1859(1.1945) | Loss 3.316835(3.406118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2443 | Time 1.1822(1.1936) | Loss 3.383463(3.404532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2444 | Time 1.1985(1.1940) | Loss 3.365694(3.401814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2445 | Time 1.1834(1.1932) | Loss 3.444939(3.404832) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2446 | Time 1.1908(1.1930) | Loss 3.435543(3.406982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2447 | Time 1.1713(1.1915) | Loss 3.402894(3.406696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2448 | Time 1.2199(1.1935) | Loss 3.457697(3.410266) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2449 | Time 1.2195(1.1953) | Loss 3.467761(3.414291) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2450 | Time 1.1645(1.1932) | Loss 3.414908(3.414334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2451 | Time 1.1799(1.1922) | Loss 3.559073(3.424466) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2452 | Time 1.1817(1.1915) | Loss 3.380152(3.421364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2453 | Time 1.1768(1.1905) | Loss 3.508302(3.427449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2454 | Time 1.1830(1.1900) | Loss 3.549531(3.435995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2455 | Time 1.1859(1.1897) | Loss 3.557215(3.444481) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2456 | Time 1.1783(1.1889) | Loss 3.485004(3.447317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2457 | Time 1.1691(1.1875) | Loss 3.555731(3.454906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2458 | Time 1.1800(1.1870) | Loss 3.435238(3.453529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2459 | Time 1.1672(1.1856) | Loss 3.452877(3.453484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2460 | Time 1.1626(1.1840) | Loss 3.521383(3.458237) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2461 | Time 1.1697(1.1830) | Loss 3.431884(3.456392) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2462 | Time 1.1628(1.1816) | Loss 3.454313(3.456246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2463 | Time 1.1850(1.1818) | Loss 3.434150(3.454700) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2464 | Time 1.1695(1.1809) | Loss 3.466046(3.455494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2465 | Time 1.1600(1.1795) | Loss 3.448366(3.454995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2466 | Time 1.1588(1.1780) | Loss 3.470954(3.456112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2467 | Time 1.1736(1.1777) | Loss 3.503075(3.459399) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2468 | Time 1.1670(1.1770) | Loss 3.500036(3.462244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2469 | Time 1.2083(1.1792) | Loss 3.421588(3.459398) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2470 | Time 1.1862(1.1796) | Loss 3.500706(3.462290) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2471 | Time 1.1731(1.1792) | Loss 3.469334(3.462783) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2472 | Time 1.2436(1.1837) | Loss 3.464913(3.462932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2473 | Time 1.1701(1.1828) | Loss 3.456488(3.462481) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2474 | Time 1.1764(1.1823) | Loss 3.495127(3.464766) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2475 | Time 1.1812(1.1822) | Loss 3.531091(3.469409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2476 | Time 1.1805(1.1821) | Loss 3.569853(3.476440) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2477 | Time 1.1833(1.1822) | Loss 3.468690(3.475897) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2478 | Time 1.1931(1.1830) | Loss 3.553347(3.481319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2479 | Time 1.1624(1.1815) | Loss 3.574028(3.487808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2480 | Time 1.1929(1.1823) | Loss 3.504913(3.489006) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2481 | Time 1.2037(1.1838) | Loss 3.499940(3.489771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2482 | Time 1.1715(1.1830) | Loss 3.587830(3.496635) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2483 | Time 1.1802(1.1828) | Loss 3.543565(3.499920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2484 | Time 1.1559(1.1809) | Loss 3.508882(3.500548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2485 | Time 1.1768(1.1806) | Loss 3.547016(3.503800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2486 | Time 1.1711(1.1799) | Loss 3.546607(3.506797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2487 | Time 1.1831(1.1802) | Loss 3.482905(3.505124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2488 | Time 1.1615(1.1789) | Loss 3.498632(3.504670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2489 | Time 1.1696(1.1782) | Loss 3.551921(3.507978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2490 | Time 1.1581(1.1768) | Loss 3.519917(3.508813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2491 | Time 1.1585(1.1755) | Loss 3.466810(3.505873) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2492 | Time 1.1985(1.1771) | Loss 3.524773(3.507196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2493 | Time 1.2241(1.1804) | Loss 3.513365(3.507628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2494 | Time 1.1719(1.1798) | Loss 3.543669(3.510151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2495 | Time 1.1751(1.1795) | Loss 3.436753(3.505013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2496 | Time 1.1673(1.1786) | Loss 3.451603(3.501274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2497 | Time 1.2128(1.1810) | Loss 3.472425(3.499255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2498 | Time 1.1778(1.1808) | Loss 3.496614(3.499070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2499 | Time 1.1700(1.1800) | Loss 3.483770(3.497999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2500 | Time 1.1701(1.1794) | Loss 3.515043(3.499192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2500 | Test Loss 3.548180 | NFE 20
Skipping vis as data dimension is >2
Iter 2501 | Time 1.1722(1.1789) | Loss 3.475267(3.497517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2502 | Time 1.1766(1.1787) | Loss 3.480788(3.496346) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2503 | Time 1.1707(1.1781) | Loss 3.534469(3.499015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2504 | Time 1.1801(1.1783) | Loss 3.475853(3.497394) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2505 | Time 1.1843(1.1787) | Loss 3.443467(3.493619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2506 | Time 1.2337(1.1825) | Loss 3.439067(3.489800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2507 | Time 1.1886(1.1830) | Loss 3.474331(3.488717) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2508 | Time 1.1893(1.1834) | Loss 3.489296(3.488758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2509 | Time 1.1754(1.1829) | Loss 3.468534(3.487342) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2510 | Time 1.1879(1.1832) | Loss 3.496028(3.487950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2511 | Time 1.1807(1.1830) | Loss 3.495139(3.488453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2512 | Time 1.1814(1.1829) | Loss 3.536441(3.491812) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2513 | Time 1.1993(1.1841) | Loss 3.591124(3.498764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2514 | Time 1.1896(1.1844) | Loss 3.667003(3.510541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2515 | Time 1.1746(1.1838) | Loss 3.540630(3.512647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2516 | Time 1.1683(1.1827) | Loss 3.544703(3.514891) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2517 | Time 1.1859(1.1829) | Loss 3.655375(3.524725) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2518 | Time 1.1797(1.1827) | Loss 3.601230(3.530080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2519 | Time 1.1721(1.1819) | Loss 3.582810(3.533771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2520 | Time 1.1647(1.1807) | Loss 3.675878(3.543719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2521 | Time 1.1852(1.1810) | Loss 3.667451(3.552380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2522 | Time 1.1963(1.1821) | Loss 3.622220(3.557269) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2523 | Time 1.1828(1.1822) | Loss 3.602038(3.560403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2524 | Time 1.2284(1.1854) | Loss 3.686213(3.569209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2525 | Time 1.1991(1.1864) | Loss 3.657771(3.575409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2526 | Time 1.2069(1.1878) | Loss 3.707338(3.584644) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2527 | Time 1.2007(1.1887) | Loss 3.691356(3.592114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2528 | Time 1.1938(1.1891) | Loss 3.644483(3.595780) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2529 | Time 1.1805(1.1885) | Loss 3.634264(3.598473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2530 | Time 1.1885(1.1885) | Loss 3.565904(3.596194) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2531 | Time 1.1726(1.1873) | Loss 3.591153(3.595841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2532 | Time 1.1763(1.1866) | Loss 3.651007(3.599702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2533 | Time 1.2247(1.1892) | Loss 3.668422(3.604513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2534 | Time 1.1779(1.1884) | Loss 3.729317(3.613249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2535 | Time 1.1890(1.1885) | Loss 3.767163(3.624023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2536 | Time 1.1799(1.1879) | Loss 3.722214(3.630896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2537 | Time 1.1677(1.1865) | Loss 3.743726(3.638794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2538 | Time 1.1741(1.1856) | Loss 3.722726(3.644670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2539 | Time 1.1721(1.1847) | Loss 3.739608(3.651315) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2540 | Time 1.1776(1.1842) | Loss 3.690759(3.654076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2541 | Time 1.1505(1.1818) | Loss 3.680230(3.655907) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2542 | Time 1.1721(1.1811) | Loss 3.572513(3.650070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2543 | Time 1.1735(1.1806) | Loss 3.627331(3.648478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2544 | Time 1.1803(1.1806) | Loss 3.616757(3.646257) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2545 | Time 1.1710(1.1799) | Loss 3.609144(3.643660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2546 | Time 1.1643(1.1788) | Loss 3.548836(3.637022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2547 | Time 1.1656(1.1779) | Loss 3.519392(3.628788) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2548 | Time 1.1666(1.1771) | Loss 3.562464(3.624145) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2549 | Time 1.1668(1.1764) | Loss 3.533496(3.617800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2550 | Time 1.1634(1.1755) | Loss 3.485342(3.608528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2551 | Time 1.1696(1.1751) | Loss 3.597024(3.607722) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2552 | Time 1.2215(1.1783) | Loss 3.567777(3.604926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2553 | Time 1.1778(1.1783) | Loss 3.580739(3.603233) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2554 | Time 1.2102(1.1805) | Loss 3.563437(3.600447) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2555 | Time 1.1846(1.1808) | Loss 3.497655(3.593252) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2556 | Time 1.1702(1.1801) | Loss 3.498860(3.586645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2557 | Time 1.1738(1.1796) | Loss 3.542182(3.583532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2558 | Time 1.1701(1.1789) | Loss 3.536650(3.580250) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2559 | Time 1.1720(1.1785) | Loss 3.564395(3.579141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2560 | Time 1.2167(1.1811) | Loss 3.592779(3.580095) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2561 | Time 1.1749(1.1807) | Loss 3.619637(3.582863) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2562 | Time 1.1567(1.1790) | Loss 3.577917(3.582517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2563 | Time 1.1741(1.1787) | Loss 3.659200(3.587885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2564 | Time 1.1724(1.1782) | Loss 3.534090(3.584119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2565 | Time 1.1818(1.1785) | Loss 3.547334(3.581544) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2566 | Time 1.1807(1.1786) | Loss 3.531611(3.578049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2567 | Time 1.1959(1.1799) | Loss 3.606818(3.580063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2568 | Time 1.1697(1.1791) | Loss 3.509676(3.575136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2569 | Time 1.1702(1.1785) | Loss 3.520327(3.571299) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2570 | Time 1.1825(1.1788) | Loss 3.555927(3.570223) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2571 | Time 1.1701(1.1782) | Loss 3.563836(3.569776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2572 | Time 1.1710(1.1777) | Loss 3.605880(3.572303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2573 | Time 1.1618(1.1766) | Loss 3.517254(3.568450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2574 | Time 1.1663(1.1758) | Loss 3.536786(3.566233) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2575 | Time 1.1531(1.1743) | Loss 3.562212(3.565952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2576 | Time 1.1738(1.1742) | Loss 3.541935(3.564271) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2577 | Time 1.1881(1.1752) | Loss 3.622578(3.568352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2578 | Time 1.1779(1.1754) | Loss 3.530252(3.565685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2579 | Time 1.1707(1.1751) | Loss 3.641056(3.570961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2580 | Time 1.2052(1.1772) | Loss 3.537568(3.568623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2581 | Time 1.2014(1.1789) | Loss 3.479993(3.562419) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2582 | Time 1.1897(1.1796) | Loss 3.465516(3.555636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2583 | Time 1.1871(1.1801) | Loss 3.518860(3.553062) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2584 | Time 1.2163(1.1827) | Loss 3.499796(3.549333) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2585 | Time 1.2268(1.1858) | Loss 3.589364(3.552135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2586 | Time 1.1848(1.1857) | Loss 3.578579(3.553986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2587 | Time 1.1847(1.1856) | Loss 3.611362(3.558003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2588 | Time 1.1868(1.1857) | Loss 3.686710(3.567012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2589 | Time 1.1760(1.1850) | Loss 3.647229(3.572627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2590 | Time 1.1783(1.1846) | Loss 3.710594(3.582285) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2591 | Time 1.1884(1.1848) | Loss 3.630603(3.585667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2592 | Time 1.1640(1.1834) | Loss 3.759823(3.597858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2593 | Time 1.1747(1.1828) | Loss 3.684288(3.603908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2594 | Time 1.1714(1.1820) | Loss 3.760278(3.614854) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2595 | Time 1.1809(1.1819) | Loss 3.611985(3.614653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2596 | Time 1.1694(1.1810) | Loss 3.693326(3.620160) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2597 | Time 1.2041(1.1826) | Loss 3.671723(3.623770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2598 | Time 1.2266(1.1857) | Loss 3.652057(3.625750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2599 | Time 1.1679(1.1845) | Loss 3.562001(3.621287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2600 | Time 1.1584(1.1827) | Loss 3.586724(3.618868) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2600 | Test Loss 3.550379 | NFE 20
Skipping vis as data dimension is >2
Iter 2601 | Time 1.1665(1.1815) | Loss 3.597443(3.617368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2602 | Time 1.1784(1.1813) | Loss 3.606690(3.616621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2603 | Time 1.1847(1.1815) | Loss 3.581113(3.614135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2604 | Time 1.1782(1.1813) | Loss 3.554724(3.609976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2605 | Time 1.1660(1.1802) | Loss 3.569759(3.607161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2606 | Time 1.1795(1.1802) | Loss 3.571819(3.604687) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2607 | Time 1.2286(1.1836) | Loss 3.536795(3.599935) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2608 | Time 1.1961(1.1845) | Loss 3.670846(3.604899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2609 | Time 1.1941(1.1851) | Loss 3.602022(3.604697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2610 | Time 1.2005(1.1862) | Loss 3.497886(3.597220) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2611 | Time 1.1888(1.1864) | Loss 3.516146(3.591545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2612 | Time 1.1948(1.1870) | Loss 3.527179(3.587040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2613 | Time 1.1951(1.1875) | Loss 3.463564(3.578396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2614 | Time 1.2019(1.1886) | Loss 3.471262(3.570897) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2615 | Time 1.2021(1.1895) | Loss 3.391864(3.558365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2616 | Time 1.2252(1.1920) | Loss 3.410654(3.548025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2617 | Time 1.1972(1.1924) | Loss 3.468143(3.542433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2618 | Time 1.2043(1.1932) | Loss 3.444032(3.535545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2619 | Time 1.2220(1.1952) | Loss 3.513442(3.533998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2620 | Time 1.1858(1.1946) | Loss 3.492969(3.531126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2621 | Time 1.1993(1.1949) | Loss 3.580963(3.534615) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2622 | Time 1.2095(1.1959) | Loss 3.453898(3.528964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2623 | Time 1.2036(1.1965) | Loss 3.433184(3.522260) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2624 | Time 1.2029(1.1969) | Loss 3.580538(3.526339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2625 | Time 1.2081(1.1977) | Loss 3.548809(3.527912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2626 | Time 1.2080(1.1984) | Loss 3.505498(3.526343) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2627 | Time 1.2083(1.1991) | Loss 3.522936(3.526105) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2628 | Time 1.1968(1.1989) | Loss 3.485886(3.523289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2629 | Time 1.2043(1.1993) | Loss 3.581801(3.527385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2630 | Time 1.2173(1.2006) | Loss 3.500555(3.525507) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2631 | Time 1.2015(1.2006) | Loss 3.446136(3.519951) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2632 | Time 1.2012(1.2007) | Loss 3.511319(3.519347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2633 | Time 1.1954(1.2003) | Loss 3.625867(3.526803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2634 | Time 1.2216(1.2018) | Loss 3.625062(3.533681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2635 | Time 1.1902(1.2010) | Loss 3.623388(3.539961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2636 | Time 1.2038(1.2012) | Loss 3.586854(3.543243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2637 | Time 1.1939(1.2007) | Loss 3.592206(3.546671) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2638 | Time 1.1891(1.1999) | Loss 3.609237(3.551050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2639 | Time 1.1917(1.1993) | Loss 3.632349(3.556741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2640 | Time 1.2056(1.1997) | Loss 3.660082(3.563975) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2641 | Time 1.1977(1.1996) | Loss 3.658437(3.570587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2642 | Time 1.1875(1.1987) | Loss 3.633866(3.575017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2643 | Time 1.1978(1.1987) | Loss 3.633731(3.579127) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2644 | Time 1.2031(1.1990) | Loss 3.630639(3.582733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2645 | Time 1.1914(1.1985) | Loss 3.558587(3.581042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2646 | Time 1.2045(1.1989) | Loss 3.634223(3.584765) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2647 | Time 1.1901(1.1983) | Loss 3.498268(3.578710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2648 | Time 1.1874(1.1975) | Loss 3.617321(3.581413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2649 | Time 1.1782(1.1962) | Loss 3.615851(3.583824) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2650 | Time 1.1988(1.1963) | Loss 3.608382(3.585543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2651 | Time 1.1781(1.1951) | Loss 3.540232(3.582371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2652 | Time 1.1905(1.1947) | Loss 3.554893(3.580448) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2653 | Time 1.1866(1.1942) | Loss 3.522972(3.576424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2654 | Time 1.1831(1.1934) | Loss 3.581400(3.576773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2655 | Time 1.1797(1.1924) | Loss 3.511339(3.572192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2656 | Time 1.1925(1.1924) | Loss 3.588912(3.573363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2657 | Time 1.1913(1.1924) | Loss 3.577155(3.573628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2658 | Time 1.1951(1.1926) | Loss 3.523850(3.570144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2659 | Time 1.1945(1.1927) | Loss 3.472189(3.563287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2660 | Time 1.1895(1.1925) | Loss 3.525668(3.560654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2661 | Time 1.1977(1.1928) | Loss 3.498756(3.556321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2662 | Time 1.2270(1.1952) | Loss 3.530125(3.554487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2663 | Time 1.2119(1.1964) | Loss 3.578619(3.556176) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2664 | Time 1.2099(1.1973) | Loss 3.466505(3.549899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2665 | Time 1.2068(1.1980) | Loss 3.512772(3.547300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2666 | Time 1.1998(1.1981) | Loss 3.530034(3.546092) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2667 | Time 1.1978(1.1981) | Loss 3.500740(3.542917) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2668 | Time 1.2036(1.1985) | Loss 3.468635(3.537717) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2669 | Time 1.2033(1.1988) | Loss 3.457875(3.532128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2670 | Time 1.1821(1.1977) | Loss 3.530369(3.532005) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2671 | Time 1.1862(1.1969) | Loss 3.527605(3.531697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2672 | Time 1.1913(1.1965) | Loss 3.503746(3.529741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2673 | Time 1.1831(1.1955) | Loss 3.448936(3.524084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2674 | Time 1.1763(1.1942) | Loss 3.426935(3.517284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2675 | Time 1.1848(1.1935) | Loss 3.463639(3.513529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2676 | Time 1.1947(1.1936) | Loss 3.454737(3.509413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2677 | Time 1.1920(1.1935) | Loss 3.438310(3.504436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2678 | Time 1.1795(1.1925) | Loss 3.410978(3.497894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2679 | Time 1.1833(1.1919) | Loss 3.457331(3.495055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2680 | Time 1.1993(1.1924) | Loss 3.481616(3.494114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2681 | Time 1.1841(1.1918) | Loss 3.487232(3.493632) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2682 | Time 1.1940(1.1920) | Loss 3.485292(3.493048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2683 | Time 1.1990(1.1925) | Loss 3.456638(3.490500) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2684 | Time 1.1876(1.1921) | Loss 3.489087(3.490401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2685 | Time 1.1833(1.1915) | Loss 3.529702(3.493152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2686 | Time 1.1894(1.1914) | Loss 3.571177(3.498614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2687 | Time 1.1801(1.1906) | Loss 3.526201(3.500545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2688 | Time 1.1960(1.1909) | Loss 3.584346(3.506411) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2689 | Time 1.1903(1.1909) | Loss 3.551637(3.509577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2690 | Time 1.2255(1.1933) | Loss 3.526495(3.510761) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2691 | Time 1.2165(1.1949) | Loss 3.643820(3.520075) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2692 | Time 1.1927(1.1948) | Loss 3.520550(3.520108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2693 | Time 1.1914(1.1945) | Loss 3.667303(3.530412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2694 | Time 1.2109(1.1957) | Loss 3.603411(3.535522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2695 | Time 1.2048(1.1963) | Loss 3.598912(3.539959) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2696 | Time 1.1984(1.1965) | Loss 3.624227(3.545858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2697 | Time 1.2277(1.1987) | Loss 3.651917(3.553282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2698 | Time 1.1878(1.1979) | Loss 3.688010(3.562713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2699 | Time 1.1846(1.1970) | Loss 3.653164(3.569045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2700 | Time 1.1963(1.1969) | Loss 3.624635(3.572936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2700 | Test Loss 3.654386 | NFE 20
Skipping vis as data dimension is >2
Iter 2701 | Time 1.2036(1.1974) | Loss 3.640611(3.577673) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2702 | Time 1.1963(1.1973) | Loss 3.652688(3.582924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2703 | Time 1.2000(1.1975) | Loss 3.654016(3.587901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2704 | Time 1.2323(1.1999) | Loss 3.636293(3.591288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2705 | Time 1.2058(1.2003) | Loss 3.595218(3.591563) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2706 | Time 1.2007(1.2004) | Loss 3.531405(3.587352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2707 | Time 1.1904(1.1997) | Loss 3.534262(3.583636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2708 | Time 1.1955(1.1994) | Loss 3.597641(3.584616) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2709 | Time 1.2030(1.1996) | Loss 3.557996(3.582753) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2710 | Time 1.1935(1.1992) | Loss 3.544301(3.580061) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2711 | Time 1.1853(1.1982) | Loss 3.550695(3.578005) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2712 | Time 1.1905(1.1977) | Loss 3.551449(3.576146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2713 | Time 1.1890(1.1971) | Loss 3.601411(3.577915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2714 | Time 1.1757(1.1956) | Loss 3.635948(3.581977) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2715 | Time 1.2258(1.1977) | Loss 3.566554(3.580898) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2716 | Time 1.2733(1.2030) | Loss 3.526611(3.577098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2717 | Time 1.2600(1.2070) | Loss 3.500433(3.571731) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2718 | Time 1.2689(1.2113) | Loss 3.499284(3.566660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2719 | Time 1.2470(1.2138) | Loss 3.572300(3.567055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2720 | Time 1.2456(1.2160) | Loss 3.581524(3.568067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2721 | Time 1.2093(1.2156) | Loss 3.553314(3.567035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2722 | Time 1.2421(1.2174) | Loss 3.557596(3.566374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2723 | Time 1.2181(1.2175) | Loss 3.621539(3.570236) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2724 | Time 1.1955(1.2159) | Loss 3.539836(3.568108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2725 | Time 1.2100(1.2155) | Loss 3.557372(3.567356) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2726 | Time 1.2107(1.2152) | Loss 3.579002(3.568171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2727 | Time 1.2240(1.2158) | Loss 3.666767(3.575073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2728 | Time 1.2039(1.2150) | Loss 3.590032(3.576120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2729 | Time 1.1733(1.2121) | Loss 3.716369(3.585938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2730 | Time 1.1874(1.2103) | Loss 3.618378(3.588208) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2731 | Time 1.1854(1.2086) | Loss 3.661747(3.593356) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2732 | Time 1.1942(1.2076) | Loss 3.687977(3.599980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2733 | Time 1.1835(1.2059) | Loss 3.652040(3.603624) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2734 | Time 1.1841(1.2044) | Loss 3.734334(3.612774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2735 | Time 1.2079(1.2046) | Loss 3.710507(3.619615) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2736 | Time 1.1850(1.2032) | Loss 3.675946(3.623558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2737 | Time 1.1808(1.2017) | Loss 3.652853(3.625609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2738 | Time 1.1875(1.2007) | Loss 3.637975(3.626474) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2739 | Time 1.1824(1.1994) | Loss 3.644503(3.627736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2740 | Time 1.1985(1.1993) | Loss 3.631795(3.628020) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2741 | Time 1.1801(1.1980) | Loss 3.547478(3.622382) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2742 | Time 1.1921(1.1976) | Loss 3.579293(3.619366) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2743 | Time 1.2567(1.2017) | Loss 3.549984(3.614509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2744 | Time 1.3008(1.2086) | Loss 3.568974(3.611322) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2745 | Time 1.2209(1.2095) | Loss 3.489143(3.602769) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2746 | Time 1.2100(1.2095) | Loss 3.551749(3.599198) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2747 | Time 1.2127(1.2098) | Loss 3.426708(3.587124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2748 | Time 1.2058(1.2095) | Loss 3.470659(3.578971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2749 | Time 1.2011(1.2089) | Loss 3.459448(3.570604) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2750 | Time 1.2341(1.2107) | Loss 3.440778(3.561517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2751 | Time 1.1988(1.2098) | Loss 3.487163(3.556312) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2752 | Time 1.1892(1.2084) | Loss 3.461835(3.549698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2753 | Time 1.1852(1.2068) | Loss 3.459450(3.543381) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2754 | Time 1.1918(1.2057) | Loss 3.449973(3.536842) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2755 | Time 1.1973(1.2051) | Loss 3.473316(3.532396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2756 | Time 1.1974(1.2046) | Loss 3.536091(3.532654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2757 | Time 1.1927(1.2038) | Loss 3.495224(3.530034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2758 | Time 1.1811(1.2022) | Loss 3.598197(3.534806) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2759 | Time 1.1823(1.2008) | Loss 3.566250(3.537007) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2760 | Time 1.1937(1.2003) | Loss 3.568544(3.539214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2761 | Time 1.1865(1.1993) | Loss 3.513369(3.537405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2762 | Time 1.1804(1.1980) | Loss 3.576765(3.540160) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2763 | Time 1.1884(1.1973) | Loss 3.558036(3.541412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2764 | Time 1.1954(1.1972) | Loss 3.565924(3.543128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2765 | Time 1.1954(1.1971) | Loss 3.704299(3.554410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2766 | Time 1.2016(1.1974) | Loss 3.628811(3.559618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2767 | Time 1.1849(1.1965) | Loss 3.593118(3.561963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2768 | Time 1.1852(1.1957) | Loss 3.641290(3.567516) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2769 | Time 1.1788(1.1945) | Loss 3.578809(3.568306) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2770 | Time 1.1901(1.1942) | Loss 3.608312(3.571107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2771 | Time 1.2152(1.1957) | Loss 3.635637(3.575624) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2772 | Time 1.2035(1.1962) | Loss 3.658417(3.581419) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2773 | Time 1.1730(1.1946) | Loss 3.548354(3.579105) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2774 | Time 1.1712(1.1930) | Loss 3.534056(3.575951) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2775 | Time 1.1866(1.1925) | Loss 3.624095(3.579321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2776 | Time 1.1971(1.1928) | Loss 3.680983(3.586438) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2777 | Time 1.1792(1.1919) | Loss 3.516849(3.581566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2778 | Time 1.1591(1.1896) | Loss 3.591801(3.582283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2779 | Time 1.1655(1.1879) | Loss 3.595001(3.583173) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2780 | Time 1.1821(1.1875) | Loss 3.484690(3.576279) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2781 | Time 1.1684(1.1862) | Loss 3.483185(3.569763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2782 | Time 1.1661(1.1847) | Loss 3.487176(3.563982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2783 | Time 1.2089(1.1864) | Loss 3.396214(3.552238) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2784 | Time 1.1764(1.1857) | Loss 3.430723(3.543732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2785 | Time 1.2248(1.1885) | Loss 3.400055(3.533674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2786 | Time 1.2368(1.1919) | Loss 3.332429(3.519587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2787 | Time 1.2306(1.1946) | Loss 3.391673(3.510633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2788 | Time 1.2047(1.1953) | Loss 3.356823(3.499867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2789 | Time 1.1826(1.1944) | Loss 3.368881(3.490698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2790 | Time 1.2254(1.1966) | Loss 3.292048(3.476792) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2791 | Time 1.1773(1.1952) | Loss 3.432403(3.473685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2792 | Time 1.1908(1.1949) | Loss 3.495222(3.475192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2793 | Time 1.1928(1.1948) | Loss 3.432800(3.472225) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2794 | Time 1.1927(1.1946) | Loss 3.417109(3.468367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2795 | Time 1.1939(1.1946) | Loss 3.485054(3.469535) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2796 | Time 1.1979(1.1948) | Loss 3.431482(3.466871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2797 | Time 1.1995(1.1951) | Loss 3.444916(3.465334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2798 | Time 1.1880(1.1946) | Loss 3.474481(3.465975) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2799 | Time 1.2299(1.1971) | Loss 3.558643(3.472461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2800 | Time 1.1967(1.1971) | Loss 3.596662(3.481155) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2800 | Test Loss 3.582720 | NFE 20
Skipping vis as data dimension is >2
Iter 2801 | Time 1.1830(1.1961) | Loss 3.476467(3.480827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2802 | Time 1.1788(1.1949) | Loss 3.541220(3.485055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2803 | Time 1.1857(1.1942) | Loss 3.554159(3.489892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2804 | Time 1.1802(1.1933) | Loss 3.513993(3.491579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2805 | Time 1.2003(1.1937) | Loss 3.563729(3.496630) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2806 | Time 1.1873(1.1933) | Loss 3.607059(3.504360) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2807 | Time 1.1630(1.1912) | Loss 3.530395(3.506182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2808 | Time 1.1526(1.1885) | Loss 3.624829(3.514487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2809 | Time 1.1668(1.1869) | Loss 3.618817(3.521790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2810 | Time 1.1667(1.1855) | Loss 3.540166(3.523077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2811 | Time 1.1697(1.1844) | Loss 3.633503(3.530807) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2812 | Time 1.1688(1.1833) | Loss 3.646507(3.538906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2813 | Time 1.1658(1.1821) | Loss 3.648825(3.546600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2814 | Time 1.1574(1.1804) | Loss 3.671594(3.555350) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2815 | Time 1.1576(1.1788) | Loss 3.718251(3.566753) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2816 | Time 1.1705(1.1782) | Loss 3.670877(3.574041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2817 | Time 1.1794(1.1783) | Loss 3.638528(3.578555) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2818 | Time 1.1764(1.1781) | Loss 3.595638(3.579751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2819 | Time 1.1699(1.1776) | Loss 3.582999(3.579978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2820 | Time 1.1817(1.1779) | Loss 3.611603(3.582192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2821 | Time 1.1955(1.1791) | Loss 3.564811(3.580976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2822 | Time 1.1836(1.1794) | Loss 3.541392(3.578205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2823 | Time 1.2034(1.1811) | Loss 3.561912(3.577064) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2824 | Time 1.2106(1.1832) | Loss 3.511831(3.572498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2825 | Time 1.1938(1.1839) | Loss 3.563618(3.571876) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2826 | Time 1.2143(1.1860) | Loss 3.645588(3.577036) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2827 | Time 1.2018(1.1871) | Loss 3.560054(3.575847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2828 | Time 1.2118(1.1889) | Loss 3.624599(3.579260) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2829 | Time 1.2143(1.1906) | Loss 3.592078(3.580157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2830 | Time 1.1983(1.1912) | Loss 3.520191(3.575960) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2831 | Time 1.2116(1.1926) | Loss 3.570437(3.575573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2832 | Time 1.2143(1.1941) | Loss 3.617607(3.578515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2833 | Time 1.2741(1.1997) | Loss 3.568700(3.577828) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2834 | Time 1.2070(1.2002) | Loss 3.561350(3.576675) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2835 | Time 1.1976(1.2000) | Loss 3.658879(3.582429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2836 | Time 1.1883(1.1992) | Loss 3.579720(3.582240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2837 | Time 1.1829(1.1981) | Loss 3.623635(3.585137) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2838 | Time 1.1868(1.1973) | Loss 3.640461(3.589010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2839 | Time 1.1809(1.1961) | Loss 3.570125(3.587688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2840 | Time 1.1956(1.1961) | Loss 3.624282(3.590250) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2841 | Time 1.1780(1.1948) | Loss 3.679690(3.596510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2842 | Time 1.1748(1.1934) | Loss 3.622333(3.598318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2843 | Time 1.1912(1.1933) | Loss 3.655209(3.602300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2844 | Time 1.1774(1.1922) | Loss 3.665801(3.606745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2845 | Time 1.1730(1.1908) | Loss 3.682550(3.612052) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2846 | Time 1.1751(1.1897) | Loss 3.641699(3.614127) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2847 | Time 1.2246(1.1922) | Loss 3.658828(3.617256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2848 | Time 1.1894(1.1920) | Loss 3.675395(3.621326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2849 | Time 1.1836(1.1914) | Loss 3.665369(3.624409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2850 | Time 1.1827(1.1908) | Loss 3.638731(3.625411) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2851 | Time 1.1807(1.1901) | Loss 3.727490(3.632557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2852 | Time 1.1904(1.1901) | Loss 3.653144(3.633998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2853 | Time 1.1797(1.1894) | Loss 3.574905(3.629862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2854 | Time 1.2157(1.1912) | Loss 3.620309(3.629193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2855 | Time 1.1904(1.1911) | Loss 3.616248(3.628287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2856 | Time 1.1993(1.1917) | Loss 3.597122(3.626105) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2857 | Time 1.1962(1.1920) | Loss 3.607373(3.624794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2858 | Time 1.1977(1.1924) | Loss 3.575971(3.621376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2859 | Time 1.2030(1.1932) | Loss 3.565326(3.617453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2860 | Time 1.2006(1.1937) | Loss 3.602696(3.616420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2861 | Time 1.1776(1.1926) | Loss 3.626860(3.617151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2862 | Time 1.1764(1.1914) | Loss 3.603996(3.616230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2863 | Time 1.2304(1.1942) | Loss 3.510768(3.608847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2864 | Time 1.1913(1.1940) | Loss 3.618613(3.609531) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2865 | Time 1.1779(1.1928) | Loss 3.556332(3.605807) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2866 | Time 1.1793(1.1919) | Loss 3.589500(3.604666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2867 | Time 1.1765(1.1908) | Loss 3.543840(3.600408) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2868 | Time 1.1691(1.1893) | Loss 3.582197(3.599133) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2869 | Time 1.1778(1.1885) | Loss 3.501042(3.592267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2870 | Time 1.2044(1.1896) | Loss 3.524061(3.587492) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2871 | Time 1.1888(1.1895) | Loss 3.630675(3.590515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2872 | Time 1.1909(1.1896) | Loss 3.671085(3.596155) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2873 | Time 1.1935(1.1899) | Loss 3.552657(3.593110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2874 | Time 1.1763(1.1890) | Loss 3.584961(3.592540) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2875 | Time 1.1727(1.1878) | Loss 3.541975(3.589000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2876 | Time 1.1664(1.1863) | Loss 3.606582(3.590231) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2877 | Time 1.1874(1.1864) | Loss 3.551977(3.587553) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2878 | Time 1.1686(1.1851) | Loss 3.631608(3.590637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2879 | Time 1.1750(1.1844) | Loss 3.594156(3.590883) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2880 | Time 1.2053(1.1859) | Loss 3.601764(3.591645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2881 | Time 1.2129(1.1878) | Loss 3.558516(3.589326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2882 | Time 1.2027(1.1888) | Loss 3.636353(3.592618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2883 | Time 1.2093(1.1903) | Loss 3.515154(3.587195) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2884 | Time 1.2018(1.1911) | Loss 3.518825(3.582409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2885 | Time 1.2120(1.1925) | Loss 3.586220(3.582676) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2886 | Time 1.2072(1.1936) | Loss 3.557445(3.580910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2887 | Time 1.1908(1.1934) | Loss 3.524039(3.576929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2888 | Time 1.2149(1.1949) | Loss 3.607006(3.579034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2889 | Time 1.1915(1.1946) | Loss 3.579107(3.579039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2890 | Time 1.2049(1.1954) | Loss 3.590300(3.579828) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2891 | Time 1.1724(1.1937) | Loss 3.571140(3.579220) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2892 | Time 1.1865(1.1932) | Loss 3.594483(3.580288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2893 | Time 1.1748(1.1919) | Loss 3.609186(3.582311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2894 | Time 1.1577(1.1896) | Loss 3.621416(3.585048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2895 | Time 1.1821(1.1890) | Loss 3.612337(3.586958) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2896 | Time 1.1583(1.1869) | Loss 3.605191(3.588235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2897 | Time 1.1804(1.1864) | Loss 3.574804(3.587295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2898 | Time 1.1736(1.1855) | Loss 3.619511(3.589550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2899 | Time 1.1662(1.1842) | Loss 3.564175(3.587773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2900 | Time 1.1713(1.1833) | Loss 3.544891(3.584772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 2900 | Test Loss 3.551906 | NFE 20
Skipping vis as data dimension is >2
Iter 2901 | Time 1.1678(1.1822) | Loss 3.593556(3.585387) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2902 | Time 1.1717(1.1815) | Loss 3.553680(3.583167) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2903 | Time 1.1678(1.1805) | Loss 3.603194(3.584569) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2904 | Time 1.1742(1.1801) | Loss 3.591331(3.585042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2905 | Time 1.1779(1.1799) | Loss 3.577896(3.584542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2906 | Time 1.1827(1.1801) | Loss 3.577860(3.584074) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2907 | Time 1.1822(1.1802) | Loss 3.552869(3.581890) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2908 | Time 1.1954(1.1813) | Loss 3.652668(3.586844) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2909 | Time 1.2345(1.1850) | Loss 3.633491(3.590110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2910 | Time 1.1932(1.1856) | Loss 3.660664(3.595049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2911 | Time 1.1898(1.1859) | Loss 3.684354(3.601300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2912 | Time 1.2360(1.1894) | Loss 3.690971(3.607577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2913 | Time 1.2581(1.1942) | Loss 3.691256(3.613434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2914 | Time 1.2316(1.1968) | Loss 3.741298(3.622385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2915 | Time 1.1814(1.1957) | Loss 3.715424(3.628898) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2916 | Time 1.2024(1.1962) | Loss 3.753063(3.637589) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2917 | Time 1.1860(1.1955) | Loss 3.621458(3.636460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2918 | Time 1.1875(1.1949) | Loss 3.764306(3.645409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2919 | Time 1.1857(1.1943) | Loss 3.728291(3.651211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2920 | Time 1.1785(1.1932) | Loss 3.685430(3.653606) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2921 | Time 1.1897(1.1929) | Loss 3.685586(3.655845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2922 | Time 1.1756(1.1917) | Loss 3.549146(3.648376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2923 | Time 1.1813(1.1910) | Loss 3.650735(3.648541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2924 | Time 1.1744(1.1898) | Loss 3.624960(3.646890) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2925 | Time 1.1811(1.1892) | Loss 3.630857(3.645768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2926 | Time 1.2224(1.1915) | Loss 3.619721(3.643945) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2927 | Time 1.1842(1.1910) | Loss 3.699573(3.647839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2928 | Time 1.1977(1.1915) | Loss 3.603924(3.644765) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2929 | Time 1.2155(1.1932) | Loss 3.597605(3.641463) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2930 | Time 1.1839(1.1925) | Loss 3.667646(3.643296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2931 | Time 1.2080(1.1936) | Loss 3.625869(3.642076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2932 | Time 1.1901(1.1934) | Loss 3.586438(3.638182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2933 | Time 1.1785(1.1923) | Loss 3.689349(3.641763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2934 | Time 1.1691(1.1907) | Loss 3.672203(3.643894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2935 | Time 1.1652(1.1889) | Loss 3.609584(3.641492) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2936 | Time 1.1944(1.1893) | Loss 3.646067(3.641813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2937 | Time 1.1827(1.1888) | Loss 3.540387(3.634713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2938 | Time 1.1874(1.1887) | Loss 3.601380(3.632380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2939 | Time 1.1791(1.1881) | Loss 3.523645(3.624768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2940 | Time 1.2010(1.1890) | Loss 3.541935(3.618970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2941 | Time 1.1800(1.1883) | Loss 3.553582(3.614393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2942 | Time 1.2618(1.1935) | Loss 3.601609(3.613498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2943 | Time 1.2280(1.1959) | Loss 3.525374(3.607329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2944 | Time 1.1751(1.1944) | Loss 3.633507(3.609162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2945 | Time 1.1727(1.1929) | Loss 3.613151(3.609441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2946 | Time 1.1735(1.1916) | Loss 3.599935(3.608775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2947 | Time 1.2092(1.1928) | Loss 3.535156(3.603622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2948 | Time 1.1739(1.1915) | Loss 3.625219(3.605134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2949 | Time 1.1793(1.1906) | Loss 3.536598(3.600336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2950 | Time 1.1790(1.1898) | Loss 3.661189(3.604596) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2951 | Time 1.1788(1.1890) | Loss 3.660172(3.608486) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2952 | Time 1.1756(1.1881) | Loss 3.682995(3.613702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2953 | Time 1.1697(1.1868) | Loss 3.651783(3.616368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2954 | Time 1.2073(1.1882) | Loss 3.717422(3.623441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2955 | Time 1.2149(1.1901) | Loss 3.743537(3.631848) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2956 | Time 1.1680(1.1886) | Loss 3.802960(3.643826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2957 | Time 1.1646(1.1869) | Loss 3.731931(3.649993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2958 | Time 1.1606(1.1850) | Loss 3.708694(3.654102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2959 | Time 1.1737(1.1842) | Loss 3.557844(3.647364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2960 | Time 1.1687(1.1832) | Loss 3.739097(3.653786) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2961 | Time 1.1585(1.1814) | Loss 3.713754(3.657983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2962 | Time 1.1589(1.1799) | Loss 3.584305(3.652826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2963 | Time 1.1602(1.1785) | Loss 3.712019(3.656969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2964 | Time 1.2775(1.1854) | Loss 3.619410(3.654340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2965 | Time 1.2000(1.1864) | Loss 3.686561(3.656596) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2966 | Time 1.1698(1.1853) | Loss 3.609331(3.653287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2967 | Time 1.1780(1.1848) | Loss 3.598060(3.649421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2968 | Time 1.1646(1.1833) | Loss 3.560441(3.643193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2969 | Time 1.1716(1.1825) | Loss 3.498321(3.633052) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2970 | Time 1.1696(1.1816) | Loss 3.588928(3.629963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2971 | Time 1.1946(1.1825) | Loss 3.600852(3.627925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2972 | Time 1.2450(1.1869) | Loss 3.551363(3.622566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2973 | Time 1.1692(1.1857) | Loss 3.520905(3.615450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2974 | Time 1.1676(1.1844) | Loss 3.598440(3.614259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2975 | Time 1.1683(1.1833) | Loss 3.574144(3.611451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2976 | Time 1.1705(1.1824) | Loss 3.619647(3.612025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2977 | Time 1.1752(1.1819) | Loss 3.597235(3.610989) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2978 | Time 1.1661(1.1808) | Loss 3.626868(3.612101) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2979 | Time 1.1837(1.1810) | Loss 3.651745(3.614876) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2980 | Time 1.1786(1.1808) | Loss 3.737074(3.623430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2981 | Time 1.1608(1.1794) | Loss 3.667611(3.626523) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2982 | Time 1.2159(1.1820) | Loss 3.759467(3.635829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2983 | Time 1.1874(1.1823) | Loss 3.655276(3.637190) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2984 | Time 1.1685(1.1814) | Loss 3.632142(3.636837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2985 | Time 1.1828(1.1815) | Loss 3.673699(3.639417) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2986 | Time 1.1662(1.1804) | Loss 3.642979(3.639666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2987 | Time 1.1688(1.1796) | Loss 3.541839(3.632818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2988 | Time 1.1768(1.1794) | Loss 3.639144(3.633261) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2989 | Time 1.1462(1.1771) | Loss 3.678084(3.636399) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2990 | Time 1.2293(1.1807) | Loss 3.754148(3.644641) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2991 | Time 1.2156(1.1832) | Loss 3.694335(3.648120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2992 | Time 1.1920(1.1838) | Loss 3.719871(3.653142) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2993 | Time 1.1944(1.1845) | Loss 3.736392(3.658970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2994 | Time 1.2429(1.1886) | Loss 3.675334(3.660115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2995 | Time 1.2072(1.1899) | Loss 3.684690(3.661836) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2996 | Time 1.2329(1.1929) | Loss 3.713025(3.665419) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2997 | Time 1.1951(1.1931) | Loss 3.728058(3.669804) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2998 | Time 1.1914(1.1930) | Loss 3.699447(3.671879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 2999 | Time 1.1949(1.1931) | Loss 3.695574(3.673537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3000 | Time 1.1983(1.1935) | Loss 3.669360(3.673245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3000 | Test Loss 3.793850 | NFE 20
Skipping vis as data dimension is >2
Iter 3001 | Time 1.1943(1.1935) | Loss 3.668692(3.672926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3002 | Time 1.1950(1.1936) | Loss 3.781678(3.680539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3003 | Time 1.1919(1.1935) | Loss 3.777491(3.687326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3004 | Time 1.1830(1.1928) | Loss 3.689461(3.687475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3005 | Time 1.1918(1.1927) | Loss 3.668581(3.686152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3006 | Time 1.1897(1.1925) | Loss 3.607304(3.680633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3007 | Time 1.1768(1.1914) | Loss 3.694967(3.681636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3008 | Time 1.1864(1.1910) | Loss 3.701425(3.683022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3009 | Time 1.1830(1.1905) | Loss 3.656400(3.681158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3010 | Time 1.1928(1.1906) | Loss 3.688161(3.681648) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3011 | Time 1.2292(1.1933) | Loss 3.644420(3.679042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3012 | Time 1.2063(1.1942) | Loss 3.538467(3.669202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3013 | Time 1.1748(1.1929) | Loss 3.655131(3.668217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3014 | Time 1.1905(1.1927) | Loss 3.609025(3.664074) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3015 | Time 1.1692(1.1911) | Loss 3.554236(3.656385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3016 | Time 1.1744(1.1899) | Loss 3.555419(3.649317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3017 | Time 1.1707(1.1886) | Loss 3.580365(3.644491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3018 | Time 1.1883(1.1885) | Loss 3.670991(3.646346) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3019 | Time 1.2442(1.1924) | Loss 3.591939(3.642537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3020 | Time 1.1973(1.1928) | Loss 3.616939(3.640745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3021 | Time 1.2056(1.1937) | Loss 3.542553(3.633872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3022 | Time 1.1988(1.1940) | Loss 3.666475(3.636154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3023 | Time 1.2245(1.1962) | Loss 3.597163(3.633425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3024 | Time 1.1999(1.1964) | Loss 3.641836(3.634014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3025 | Time 1.2009(1.1967) | Loss 3.640516(3.634469) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3026 | Time 1.2000(1.1970) | Loss 3.648878(3.635477) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3027 | Time 1.1992(1.1971) | Loss 3.598825(3.632912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3028 | Time 1.1972(1.1971) | Loss 3.589217(3.629853) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3029 | Time 1.1845(1.1963) | Loss 3.576681(3.626131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3030 | Time 1.1814(1.1952) | Loss 3.649635(3.627776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3031 | Time 1.1630(1.1930) | Loss 3.664530(3.630349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3032 | Time 1.2050(1.1938) | Loss 3.576554(3.626583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3033 | Time 1.1789(1.1928) | Loss 3.513013(3.618633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3034 | Time 1.1834(1.1921) | Loss 3.587599(3.616461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3035 | Time 1.1828(1.1915) | Loss 3.689486(3.621573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3036 | Time 1.1752(1.1903) | Loss 3.619417(3.621422) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3037 | Time 1.1901(1.1903) | Loss 3.550126(3.616431) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3038 | Time 1.1807(1.1896) | Loss 3.548644(3.611686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3039 | Time 1.2373(1.1930) | Loss 3.559382(3.608025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3040 | Time 1.1915(1.1929) | Loss 3.676739(3.612835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3041 | Time 1.1777(1.1918) | Loss 3.561751(3.609259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3042 | Time 1.1819(1.1911) | Loss 3.533684(3.603969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3043 | Time 1.1801(1.1903) | Loss 3.545149(3.599851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3044 | Time 1.1770(1.1894) | Loss 3.487151(3.591962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3045 | Time 1.1770(1.1885) | Loss 3.618846(3.593844) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3046 | Time 1.2018(1.1895) | Loss 3.572983(3.592384) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3047 | Time 1.1926(1.1897) | Loss 3.691431(3.599317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3048 | Time 1.1955(1.1901) | Loss 3.644034(3.602447) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3049 | Time 1.1828(1.1896) | Loss 3.656223(3.606212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3050 | Time 1.1769(1.1887) | Loss 3.678178(3.611249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3051 | Time 1.2012(1.1896) | Loss 3.643583(3.613513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3052 | Time 1.1680(1.1881) | Loss 3.708896(3.620189) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3053 | Time 1.1881(1.1881) | Loss 3.772139(3.630826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3054 | Time 1.1677(1.1866) | Loss 3.696455(3.635420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3055 | Time 1.1724(1.1856) | Loss 3.698872(3.639862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3056 | Time 1.1725(1.1847) | Loss 3.712723(3.644962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3057 | Time 1.1814(1.1845) | Loss 3.722798(3.650410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3058 | Time 1.2034(1.1858) | Loss 3.678558(3.652381) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3059 | Time 1.1721(1.1848) | Loss 3.597539(3.648542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3060 | Time 1.1772(1.1843) | Loss 3.669674(3.650021) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3061 | Time 1.1753(1.1837) | Loss 3.684937(3.652465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3062 | Time 1.1807(1.1835) | Loss 3.593010(3.648303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3063 | Time 1.1704(1.1826) | Loss 3.653414(3.648661) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3064 | Time 1.1747(1.1820) | Loss 3.650651(3.648800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3065 | Time 1.1715(1.1813) | Loss 3.606292(3.645825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3066 | Time 1.1792(1.1811) | Loss 3.625062(3.644371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3067 | Time 1.1915(1.1819) | Loss 3.535494(3.636750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3068 | Time 1.1871(1.1822) | Loss 3.618847(3.635497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3069 | Time 1.1816(1.1822) | Loss 3.553181(3.629735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3070 | Time 1.1791(1.1820) | Loss 3.503663(3.620910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3071 | Time 1.1827(1.1820) | Loss 3.529287(3.614496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3072 | Time 1.1898(1.1826) | Loss 3.517781(3.607726) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3073 | Time 1.1798(1.1824) | Loss 3.557880(3.604237) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3074 | Time 1.2083(1.1842) | Loss 3.454008(3.593721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3075 | Time 1.2185(1.1866) | Loss 3.578106(3.592628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3076 | Time 1.1880(1.1867) | Loss 3.525543(3.587932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3077 | Time 1.1912(1.1870) | Loss 3.503730(3.582038) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3078 | Time 1.1750(1.1862) | Loss 3.446306(3.572536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3079 | Time 1.1722(1.1852) | Loss 3.487123(3.566557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3080 | Time 1.1746(1.1845) | Loss 3.523963(3.563576) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3081 | Time 1.2187(1.1868) | Loss 3.506818(3.559603) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3082 | Time 1.2048(1.1881) | Loss 3.585130(3.561390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3083 | Time 1.1843(1.1878) | Loss 3.578811(3.562609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3084 | Time 1.1588(1.1858) | Loss 3.571828(3.563255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3085 | Time 1.1885(1.1860) | Loss 3.542038(3.561769) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3086 | Time 1.1665(1.1846) | Loss 3.633412(3.566784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3087 | Time 1.1738(1.1839) | Loss 3.643865(3.572180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3088 | Time 1.1807(1.1837) | Loss 3.632729(3.576418) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3089 | Time 1.1746(1.1830) | Loss 3.601091(3.578145) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3090 | Time 1.1692(1.1821) | Loss 3.604327(3.579978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3091 | Time 1.2020(1.1834) | Loss 3.584474(3.580293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3092 | Time 1.1878(1.1838) | Loss 3.607420(3.582192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3093 | Time 1.1730(1.1830) | Loss 3.685174(3.589401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3094 | Time 1.1777(1.1826) | Loss 3.582644(3.588928) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3095 | Time 1.1711(1.1818) | Loss 3.637522(3.592329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3096 | Time 1.1759(1.1814) | Loss 3.593706(3.592426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3097 | Time 1.2046(1.1830) | Loss 3.636974(3.595544) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3098 | Time 1.1748(1.1825) | Loss 3.682334(3.601619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3099 | Time 1.1873(1.1828) | Loss 3.654778(3.605340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3100 | Time 1.1674(1.1817) | Loss 3.544382(3.601073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3100 | Test Loss 3.577140 | NFE 20
Skipping vis as data dimension is >2
Iter 3101 | Time 1.2523(1.1867) | Loss 3.517199(3.595202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3102 | Time 1.2523(1.1913) | Loss 3.550552(3.592077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3103 | Time 1.1927(1.1914) | Loss 3.504011(3.585912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3104 | Time 1.2082(1.1925) | Loss 3.516074(3.581023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3105 | Time 1.1774(1.1915) | Loss 3.478513(3.573848) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3106 | Time 1.1797(1.1907) | Loss 3.453965(3.565456) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3107 | Time 1.1756(1.1896) | Loss 3.523125(3.562493) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3108 | Time 1.1795(1.1889) | Loss 3.474464(3.556331) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3109 | Time 1.1662(1.1873) | Loss 3.499343(3.552342) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3110 | Time 1.1570(1.1852) | Loss 3.681998(3.561418) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3111 | Time 1.1510(1.1828) | Loss 3.589481(3.563382) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3112 | Time 1.1518(1.1806) | Loss 3.598810(3.565862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3113 | Time 1.1923(1.1814) | Loss 3.620229(3.569668) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3114 | Time 1.1701(1.1806) | Loss 3.500347(3.564815) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3115 | Time 1.1829(1.1808) | Loss 3.611345(3.568072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3116 | Time 1.1742(1.1803) | Loss 3.568828(3.568125) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3117 | Time 1.1774(1.1801) | Loss 3.548338(3.566740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3118 | Time 1.1895(1.1808) | Loss 3.566017(3.566689) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3119 | Time 1.1804(1.1808) | Loss 3.597125(3.568820) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3120 | Time 1.1724(1.1802) | Loss 3.599947(3.570999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3121 | Time 1.1708(1.1795) | Loss 3.617715(3.574269) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3122 | Time 1.1789(1.1795) | Loss 3.610744(3.576822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3123 | Time 1.1731(1.1790) | Loss 3.590016(3.577746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3124 | Time 1.1621(1.1778) | Loss 3.652198(3.582957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3125 | Time 1.1904(1.1787) | Loss 3.643970(3.587228) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3126 | Time 1.1991(1.1802) | Loss 3.587590(3.587254) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3127 | Time 1.1802(1.1802) | Loss 3.553275(3.584875) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3128 | Time 1.1681(1.1793) | Loss 3.673347(3.591068) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3129 | Time 1.1934(1.1803) | Loss 3.594177(3.591286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3130 | Time 1.2015(1.1818) | Loss 3.629563(3.593965) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3131 | Time 1.1917(1.1825) | Loss 3.610372(3.595114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3132 | Time 1.1975(1.1835) | Loss 3.635815(3.597963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3133 | Time 1.2072(1.1852) | Loss 3.565559(3.595694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3134 | Time 1.1930(1.1857) | Loss 3.577242(3.594403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3135 | Time 1.1864(1.1858) | Loss 3.575519(3.593081) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3136 | Time 1.1810(1.1854) | Loss 3.527775(3.588510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3137 | Time 1.1696(1.1843) | Loss 3.602956(3.589521) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3138 | Time 1.1858(1.1844) | Loss 3.504536(3.583572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3139 | Time 1.1921(1.1850) | Loss 3.608855(3.585342) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3140 | Time 1.2014(1.1861) | Loss 3.568465(3.584160) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3141 | Time 1.2077(1.1876) | Loss 3.521414(3.579768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3142 | Time 1.1950(1.1881) | Loss 3.547756(3.577527) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3143 | Time 1.1881(1.1881) | Loss 3.644388(3.582207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3144 | Time 1.1938(1.1885) | Loss 3.554241(3.580250) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3145 | Time 1.1864(1.1884) | Loss 3.662661(3.586018) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3146 | Time 1.2022(1.1894) | Loss 3.619611(3.588370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3147 | Time 1.2091(1.1907) | Loss 3.648658(3.592590) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3148 | Time 1.1895(1.1907) | Loss 3.644211(3.596204) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3149 | Time 1.1793(1.1899) | Loss 3.687588(3.602600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3150 | Time 1.2031(1.1908) | Loss 3.639143(3.605158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3151 | Time 1.2049(1.1918) | Loss 3.644637(3.607922) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3152 | Time 1.1826(1.1911) | Loss 3.686002(3.613387) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3153 | Time 1.1594(1.1889) | Loss 3.743358(3.622485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3154 | Time 1.1580(1.1867) | Loss 3.681599(3.626623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3155 | Time 1.1720(1.1857) | Loss 3.719275(3.633109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3156 | Time 1.1633(1.1841) | Loss 3.742548(3.640770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3157 | Time 1.2077(1.1858) | Loss 3.679272(3.643465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3158 | Time 1.1924(1.1863) | Loss 3.633725(3.642783) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3159 | Time 1.1770(1.1856) | Loss 3.622110(3.641336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3160 | Time 1.1803(1.1852) | Loss 3.629341(3.640496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3161 | Time 1.1669(1.1840) | Loss 3.670844(3.642621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3162 | Time 1.1558(1.1820) | Loss 3.609976(3.640336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3163 | Time 1.1775(1.1817) | Loss 3.667888(3.642264) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3164 | Time 1.1841(1.1818) | Loss 3.628690(3.641314) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3165 | Time 1.1892(1.1824) | Loss 3.642823(3.641420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3166 | Time 1.1650(1.1811) | Loss 3.641091(3.641397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3167 | Time 1.1780(1.1809) | Loss 3.748924(3.648923) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3168 | Time 1.1772(1.1807) | Loss 3.671614(3.650512) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3169 | Time 1.1793(1.1806) | Loss 3.679077(3.652511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3170 | Time 1.1694(1.1798) | Loss 3.592274(3.648295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3171 | Time 1.1774(1.1796) | Loss 3.706242(3.652351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3172 | Time 1.1935(1.1806) | Loss 3.586922(3.647771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3173 | Time 1.1911(1.1813) | Loss 3.706150(3.651858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3174 | Time 1.1823(1.1814) | Loss 3.674467(3.653440) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3175 | Time 1.1667(1.1804) | Loss 3.697520(3.656526) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3176 | Time 1.1765(1.1801) | Loss 3.727610(3.661502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3177 | Time 1.1672(1.1792) | Loss 3.696347(3.663941) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3178 | Time 1.1629(1.1781) | Loss 3.688170(3.665637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3179 | Time 1.1687(1.1774) | Loss 3.682893(3.666845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3180 | Time 1.1639(1.1764) | Loss 3.602331(3.662329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3181 | Time 1.1758(1.1764) | Loss 3.716609(3.666128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3182 | Time 1.1763(1.1764) | Loss 3.615198(3.662563) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3183 | Time 1.1766(1.1764) | Loss 3.674537(3.663402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3184 | Time 1.1788(1.1766) | Loss 3.706417(3.666413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3185 | Time 1.2113(1.1790) | Loss 3.701460(3.668866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3186 | Time 1.1925(1.1800) | Loss 3.678121(3.669514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3187 | Time 1.1993(1.1813) | Loss 3.732607(3.673930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3188 | Time 1.1906(1.1820) | Loss 3.572656(3.666841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3189 | Time 1.1864(1.1823) | Loss 3.640907(3.665026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3190 | Time 1.1861(1.1825) | Loss 3.678154(3.665945) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3191 | Time 1.1924(1.1832) | Loss 3.620591(3.662770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3192 | Time 1.1967(1.1842) | Loss 3.695620(3.665069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3193 | Time 1.1858(1.1843) | Loss 3.562512(3.657890) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3194 | Time 1.2059(1.1858) | Loss 3.549179(3.650281) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3195 | Time 1.1822(1.1855) | Loss 3.590738(3.646113) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3196 | Time 1.1892(1.1858) | Loss 3.606444(3.643336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3197 | Time 1.2194(1.1881) | Loss 3.605182(3.640665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3198 | Time 1.2442(1.1921) | Loss 3.564409(3.635327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3199 | Time 1.2328(1.1949) | Loss 3.644149(3.635945) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3200 | Time 1.2427(1.1983) | Loss 3.579427(3.631988) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3200 | Test Loss 3.707310 | NFE 20
Skipping vis as data dimension is >2
Iter 3201 | Time 1.1999(1.1984) | Loss 3.617927(3.631004) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3202 | Time 1.1740(1.1967) | Loss 3.631576(3.631044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3203 | Time 1.1640(1.1944) | Loss 3.691473(3.635274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3204 | Time 1.1726(1.1929) | Loss 3.707972(3.640363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3205 | Time 1.1688(1.1912) | Loss 3.734483(3.646951) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3206 | Time 1.1747(1.1900) | Loss 3.700649(3.650710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3207 | Time 1.1764(1.1891) | Loss 3.671314(3.652153) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3208 | Time 1.1695(1.1877) | Loss 3.707255(3.656010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3209 | Time 1.1633(1.1860) | Loss 3.712125(3.659938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3210 | Time 1.2312(1.1892) | Loss 3.617988(3.657001) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3211 | Time 1.1741(1.1881) | Loss 3.664641(3.657536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3212 | Time 1.2336(1.1913) | Loss 3.540123(3.649317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3213 | Time 1.1778(1.1903) | Loss 3.595049(3.645518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3214 | Time 1.1923(1.1905) | Loss 3.615455(3.643414) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3215 | Time 1.1923(1.1906) | Loss 3.607349(3.640889) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3216 | Time 1.1844(1.1902) | Loss 3.532678(3.633315) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3217 | Time 1.1872(1.1900) | Loss 3.547624(3.627316) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3218 | Time 1.1888(1.1899) | Loss 3.591016(3.624775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3219 | Time 1.1837(1.1895) | Loss 3.530723(3.618192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3220 | Time 1.1727(1.1883) | Loss 3.463711(3.607378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3221 | Time 1.2434(1.1921) | Loss 3.506313(3.600303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3222 | Time 1.3002(1.1997) | Loss 3.599088(3.600218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3223 | Time 1.2171(1.2009) | Loss 3.467704(3.590942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3224 | Time 1.1849(1.1998) | Loss 3.502745(3.584768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3225 | Time 1.1942(1.1994) | Loss 3.508468(3.579427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3226 | Time 1.2476(1.2028) | Loss 3.614766(3.581901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3227 | Time 1.2068(1.2031) | Loss 3.577216(3.581573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3228 | Time 1.1890(1.2021) | Loss 3.592964(3.582371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3229 | Time 1.1859(1.2009) | Loss 3.599037(3.583537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3230 | Time 1.1879(1.2000) | Loss 3.496722(3.577460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3231 | Time 1.2512(1.2036) | Loss 3.526679(3.573905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3232 | Time 1.1949(1.2030) | Loss 3.626647(3.577597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3233 | Time 1.1968(1.2026) | Loss 3.560021(3.576367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3234 | Time 1.1888(1.2016) | Loss 3.599833(3.578010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3235 | Time 1.1864(1.2005) | Loss 3.608404(3.580137) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3236 | Time 1.1935(1.2000) | Loss 3.611035(3.582300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3237 | Time 1.1777(1.1985) | Loss 3.606923(3.584024) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3238 | Time 1.1812(1.1973) | Loss 3.746052(3.595366) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3239 | Time 1.2140(1.1984) | Loss 3.718942(3.604016) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3240 | Time 1.2023(1.1987) | Loss 3.726114(3.612563) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3241 | Time 1.1960(1.1985) | Loss 3.764103(3.623171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3242 | Time 1.1929(1.1981) | Loss 3.711740(3.629370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3243 | Time 1.2130(1.1992) | Loss 3.793815(3.640882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3244 | Time 1.1823(1.1980) | Loss 3.666339(3.642664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3245 | Time 1.1859(1.1971) | Loss 3.645485(3.642861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3246 | Time 1.1910(1.1967) | Loss 3.711023(3.647633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3247 | Time 1.1887(1.1962) | Loss 3.767790(3.656044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3248 | Time 1.1831(1.1952) | Loss 3.719231(3.660467) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3249 | Time 1.1815(1.1943) | Loss 3.616064(3.657358) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3250 | Time 1.2566(1.1986) | Loss 3.639385(3.656100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3251 | Time 1.2817(1.2045) | Loss 3.803529(3.666420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3252 | Time 1.2420(1.2071) | Loss 3.577533(3.660198) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3253 | Time 1.1808(1.2052) | Loss 3.616899(3.657167) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3254 | Time 1.2049(1.2052) | Loss 3.617478(3.654389) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3255 | Time 1.1844(1.2038) | Loss 3.575706(3.648881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3256 | Time 1.1895(1.2028) | Loss 3.532721(3.640750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3257 | Time 1.1788(1.2011) | Loss 3.603293(3.638128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3258 | Time 1.1854(1.2000) | Loss 3.590670(3.634806) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3259 | Time 1.1783(1.1985) | Loss 3.647886(3.635721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3260 | Time 1.1880(1.1977) | Loss 3.637742(3.635863) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3261 | Time 1.1802(1.1965) | Loss 3.580909(3.632016) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3262 | Time 1.1905(1.1961) | Loss 3.535884(3.625287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3263 | Time 1.1849(1.1953) | Loss 3.624241(3.625214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3264 | Time 1.1884(1.1948) | Loss 3.545491(3.619633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3265 | Time 1.1916(1.1946) | Loss 3.554242(3.615056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3266 | Time 1.1893(1.1942) | Loss 3.586832(3.613080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3267 | Time 1.2026(1.1948) | Loss 3.576266(3.610503) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3268 | Time 1.1942(1.1948) | Loss 3.602364(3.609933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3269 | Time 1.1896(1.1944) | Loss 3.595800(3.608944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3270 | Time 1.2070(1.1953) | Loss 3.597329(3.608131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3271 | Time 1.2031(1.1958) | Loss 3.680119(3.613170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3272 | Time 1.1982(1.1960) | Loss 3.645495(3.615433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3273 | Time 1.1903(1.1956) | Loss 3.646571(3.617612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3274 | Time 1.1808(1.1946) | Loss 3.608191(3.616953) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3275 | Time 1.1905(1.1943) | Loss 3.651119(3.619345) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3276 | Time 1.2030(1.1949) | Loss 3.711811(3.625817) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3277 | Time 1.1906(1.1946) | Loss 3.770332(3.635933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3278 | Time 1.1920(1.1944) | Loss 3.712635(3.641302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3279 | Time 1.1737(1.1930) | Loss 3.705908(3.645825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3280 | Time 1.1796(1.1920) | Loss 3.680496(3.648252) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3281 | Time 1.1865(1.1916) | Loss 3.696262(3.651613) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3282 | Time 1.1766(1.1906) | Loss 3.767621(3.659733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3283 | Time 1.2032(1.1915) | Loss 3.837718(3.672192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3284 | Time 1.1824(1.1908) | Loss 3.685335(3.673112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3285 | Time 1.1935(1.1910) | Loss 3.689117(3.674232) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3286 | Time 1.1896(1.1909) | Loss 3.701688(3.676154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3287 | Time 1.1720(1.1896) | Loss 3.693958(3.677401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3288 | Time 1.1712(1.1883) | Loss 3.701663(3.679099) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3289 | Time 1.1874(1.1882) | Loss 3.724068(3.682247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3290 | Time 1.1849(1.1880) | Loss 3.761958(3.687826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3291 | Time 1.2006(1.1889) | Loss 3.806056(3.696103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3292 | Time 1.1849(1.1886) | Loss 3.758592(3.700477) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3293 | Time 1.1693(1.1873) | Loss 3.643269(3.696472) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3294 | Time 1.1775(1.1866) | Loss 3.711355(3.697514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3295 | Time 1.2025(1.1877) | Loss 3.582790(3.689483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3296 | Time 1.1870(1.1876) | Loss 3.579803(3.681806) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3297 | Time 1.1887(1.1877) | Loss 3.530795(3.671235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3298 | Time 1.1932(1.1881) | Loss 3.538450(3.661940) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3299 | Time 1.1878(1.1881) | Loss 3.540134(3.653414) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3300 | Time 1.1634(1.1863) | Loss 3.655478(3.653558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3300 | Test Loss 3.668718 | NFE 20
Skipping vis as data dimension is >2
Iter 3301 | Time 1.2399(1.1901) | Loss 3.668329(3.654592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3302 | Time 1.2046(1.1911) | Loss 3.681720(3.656491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3303 | Time 1.1813(1.1904) | Loss 3.707748(3.660079) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3304 | Time 1.1622(1.1884) | Loss 3.709729(3.663555) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3305 | Time 1.1847(1.1882) | Loss 3.690642(3.665451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3306 | Time 1.1914(1.1884) | Loss 3.678029(3.666331) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3307 | Time 1.1917(1.1886) | Loss 3.674166(3.666880) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3308 | Time 1.1829(1.1882) | Loss 3.672621(3.667281) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3309 | Time 1.1891(1.1883) | Loss 3.629697(3.664651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3310 | Time 1.1854(1.1881) | Loss 3.654288(3.663925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3311 | Time 1.1808(1.1876) | Loss 3.595119(3.659109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3312 | Time 1.1887(1.1877) | Loss 3.676431(3.660321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3313 | Time 1.1907(1.1879) | Loss 3.587760(3.655242) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3314 | Time 1.1934(1.1883) | Loss 3.558060(3.648439) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3315 | Time 1.1813(1.1878) | Loss 3.607226(3.645554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3316 | Time 1.1844(1.1875) | Loss 3.657027(3.646357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3317 | Time 1.1979(1.1883) | Loss 3.605820(3.643520) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3318 | Time 1.2041(1.1894) | Loss 3.590145(3.639784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3319 | Time 1.1828(1.1889) | Loss 3.565313(3.634571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3320 | Time 1.1907(1.1890) | Loss 3.654850(3.635990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3321 | Time 1.1919(1.1892) | Loss 3.500383(3.626498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3322 | Time 1.2275(1.1919) | Loss 3.606742(3.625115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3323 | Time 1.1995(1.1925) | Loss 3.635329(3.625830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3324 | Time 1.1905(1.1923) | Loss 3.614999(3.625072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3325 | Time 1.2084(1.1934) | Loss 3.580102(3.621924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3326 | Time 1.2141(1.1949) | Loss 3.646829(3.623667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3327 | Time 1.2173(1.1965) | Loss 3.598420(3.621900) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3328 | Time 1.2116(1.1975) | Loss 3.534335(3.615770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3329 | Time 1.2005(1.1977) | Loss 3.592633(3.614151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3330 | Time 1.2324(1.2002) | Loss 3.565209(3.610725) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3331 | Time 1.2009(1.2002) | Loss 3.573285(3.608104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3332 | Time 1.2166(1.2013) | Loss 3.649681(3.611014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3333 | Time 1.1997(1.2012) | Loss 3.543663(3.606300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3334 | Time 1.1963(1.2009) | Loss 3.614479(3.606872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3335 | Time 1.2142(1.2018) | Loss 3.615896(3.607504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3336 | Time 1.1785(1.2002) | Loss 3.605382(3.607355) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3337 | Time 1.1935(1.1997) | Loss 3.625068(3.608595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3338 | Time 1.1938(1.1993) | Loss 3.644213(3.611089) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3339 | Time 1.2038(1.1996) | Loss 3.623889(3.611985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3340 | Time 1.1908(1.1990) | Loss 3.574142(3.609336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3341 | Time 1.2043(1.1994) | Loss 3.641513(3.611588) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3342 | Time 1.1919(1.1988) | Loss 3.540982(3.606646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3343 | Time 1.2108(1.1997) | Loss 3.601461(3.606283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3344 | Time 1.2026(1.1999) | Loss 3.687428(3.611963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3345 | Time 1.1779(1.1983) | Loss 3.619161(3.612467) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3346 | Time 1.1834(1.1973) | Loss 3.620419(3.613023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3347 | Time 1.1944(1.1971) | Loss 3.658542(3.616210) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3348 | Time 1.1904(1.1966) | Loss 3.609694(3.615754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3349 | Time 1.2254(1.1986) | Loss 3.697773(3.621495) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3350 | Time 1.2069(1.1992) | Loss 3.651073(3.623565) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3351 | Time 1.2087(1.1999) | Loss 3.691768(3.628340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3352 | Time 1.1988(1.1998) | Loss 3.610014(3.627057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3353 | Time 1.2180(1.2011) | Loss 3.576516(3.623519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3354 | Time 1.2041(1.2013) | Loss 3.643626(3.624926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3355 | Time 1.2129(1.2021) | Loss 3.596298(3.622922) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3356 | Time 1.2159(1.2031) | Loss 3.617080(3.622513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3357 | Time 1.1868(1.2019) | Loss 3.637452(3.623559) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3358 | Time 1.2129(1.2027) | Loss 3.621298(3.623401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3359 | Time 1.1976(1.2024) | Loss 3.633960(3.624140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3360 | Time 1.1847(1.2011) | Loss 3.588776(3.621665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3361 | Time 1.1903(1.2004) | Loss 3.632011(3.622389) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3362 | Time 1.2067(1.2008) | Loss 3.590756(3.620174) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3363 | Time 1.2046(1.2011) | Loss 3.594188(3.618355) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3364 | Time 1.1931(1.2005) | Loss 3.554054(3.613854) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3365 | Time 1.2059(1.2009) | Loss 3.624534(3.614602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3366 | Time 1.1996(1.2008) | Loss 3.518163(3.607851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3367 | Time 1.2042(1.2010) | Loss 3.494656(3.599928) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3368 | Time 1.1872(1.2001) | Loss 3.551476(3.596536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3369 | Time 1.1791(1.1986) | Loss 3.453168(3.586500) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3370 | Time 1.1674(1.1964) | Loss 3.536890(3.583027) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3371 | Time 1.1859(1.1957) | Loss 3.481951(3.575952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3372 | Time 1.1992(1.1959) | Loss 3.538392(3.573323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3373 | Time 1.2074(1.1967) | Loss 3.454286(3.564990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3374 | Time 1.2067(1.1974) | Loss 3.599990(3.567440) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3375 | Time 1.2155(1.1987) | Loss 3.557833(3.566768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3376 | Time 1.2126(1.1997) | Loss 3.562332(3.566457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3377 | Time 1.2255(1.2015) | Loss 3.543480(3.564849) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3378 | Time 1.2037(1.2016) | Loss 3.513103(3.561227) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3379 | Time 1.2101(1.2022) | Loss 3.601483(3.564045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3380 | Time 1.2021(1.2022) | Loss 3.485103(3.558519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3381 | Time 1.2063(1.2025) | Loss 3.536059(3.556947) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3382 | Time 1.2011(1.2024) | Loss 3.575004(3.558211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3383 | Time 1.1994(1.2022) | Loss 3.485109(3.553093) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3384 | Time 1.2056(1.2024) | Loss 3.546597(3.552639) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3385 | Time 1.2164(1.2034) | Loss 3.606628(3.556418) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3386 | Time 1.2044(1.2035) | Loss 3.602882(3.559670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3387 | Time 1.1995(1.2032) | Loss 3.565060(3.560048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3388 | Time 1.1850(1.2019) | Loss 3.548601(3.559246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3389 | Time 1.1809(1.2005) | Loss 3.525697(3.556898) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3390 | Time 1.1818(1.1991) | Loss 3.572786(3.558010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3391 | Time 1.1927(1.1987) | Loss 3.557798(3.557995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3392 | Time 1.1771(1.1972) | Loss 3.527908(3.555889) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3393 | Time 1.1819(1.1961) | Loss 3.569457(3.556839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3394 | Time 1.1914(1.1958) | Loss 3.593427(3.559400) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3395 | Time 1.1937(1.1956) | Loss 3.543427(3.558282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3396 | Time 1.1994(1.1959) | Loss 3.528911(3.556226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3397 | Time 1.2077(1.1967) | Loss 3.506796(3.552766) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3398 | Time 1.2280(1.1989) | Loss 3.537903(3.551726) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3399 | Time 1.2356(1.2015) | Loss 3.633795(3.557470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3400 | Time 1.2855(1.2074) | Loss 3.549666(3.556924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3400 | Test Loss 3.610255 | NFE 20
Skipping vis as data dimension is >2
Iter 3401 | Time 1.2453(1.2100) | Loss 3.686586(3.566000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3402 | Time 1.2800(1.2149) | Loss 3.552852(3.565080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3403 | Time 1.2641(1.2184) | Loss 3.677445(3.572946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3404 | Time 1.2441(1.2202) | Loss 3.657009(3.578830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3405 | Time 1.2212(1.2202) | Loss 3.710928(3.588077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3406 | Time 1.2268(1.2207) | Loss 3.632575(3.591192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3407 | Time 1.2177(1.2205) | Loss 3.703548(3.599057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3408 | Time 1.2232(1.2207) | Loss 3.674789(3.604358) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3409 | Time 1.2023(1.2194) | Loss 3.662993(3.608463) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3410 | Time 1.2025(1.2182) | Loss 3.681489(3.613574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3411 | Time 1.1923(1.2164) | Loss 3.705808(3.620031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3412 | Time 1.1864(1.2143) | Loss 3.554288(3.615429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3413 | Time 1.2073(1.2138) | Loss 3.563185(3.611772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3414 | Time 1.1956(1.2125) | Loss 3.563139(3.608367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3415 | Time 1.1957(1.2114) | Loss 3.514261(3.601780) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3416 | Time 1.1924(1.2100) | Loss 3.629496(3.603720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3417 | Time 1.1966(1.2091) | Loss 3.596249(3.603197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3418 | Time 1.2001(1.2085) | Loss 3.570045(3.600876) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3419 | Time 1.1954(1.2075) | Loss 3.585138(3.599775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3420 | Time 1.1930(1.2065) | Loss 3.622097(3.601337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3421 | Time 1.2556(1.2100) | Loss 3.549332(3.597697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3422 | Time 1.2636(1.2137) | Loss 3.528694(3.592867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3423 | Time 1.2487(1.2162) | Loss 3.548314(3.589748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3424 | Time 1.1996(1.2150) | Loss 3.538561(3.586165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3425 | Time 1.2188(1.2153) | Loss 3.550447(3.583665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3426 | Time 1.2001(1.2142) | Loss 3.564583(3.582329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3427 | Time 1.1920(1.2127) | Loss 3.582225(3.582322) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3428 | Time 1.2116(1.2126) | Loss 3.581509(3.582265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3429 | Time 1.2129(1.2126) | Loss 3.564645(3.581031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3430 | Time 1.2143(1.2127) | Loss 3.574166(3.580551) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3431 | Time 1.2044(1.2121) | Loss 3.551318(3.578504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3432 | Time 1.2091(1.2119) | Loss 3.496477(3.572763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3433 | Time 1.2082(1.2117) | Loss 3.608055(3.575233) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3434 | Time 1.2213(1.2123) | Loss 3.557155(3.573968) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3435 | Time 1.2101(1.2122) | Loss 3.602239(3.575947) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3436 | Time 1.2037(1.2116) | Loss 3.622608(3.579213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3437 | Time 1.2190(1.2121) | Loss 3.627506(3.582593) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3438 | Time 1.2074(1.2118) | Loss 3.565543(3.581400) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3439 | Time 1.1826(1.2097) | Loss 3.639962(3.585499) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3440 | Time 1.2044(1.2094) | Loss 3.608649(3.587120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3441 | Time 1.2030(1.2089) | Loss 3.589936(3.587317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3442 | Time 1.1950(1.2079) | Loss 3.624362(3.589910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3443 | Time 1.2023(1.2075) | Loss 3.611613(3.591429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3444 | Time 1.1881(1.2062) | Loss 3.622000(3.593569) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3445 | Time 1.1916(1.2052) | Loss 3.571125(3.591998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3446 | Time 1.2018(1.2049) | Loss 3.651342(3.596152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3447 | Time 1.2131(1.2055) | Loss 3.559919(3.593616) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3448 | Time 1.2090(1.2057) | Loss 3.629785(3.596148) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3449 | Time 1.1993(1.2053) | Loss 3.551229(3.593003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3450 | Time 1.1953(1.2046) | Loss 3.618078(3.594759) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3451 | Time 1.2020(1.2044) | Loss 3.582870(3.593926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3452 | Time 1.1894(1.2034) | Loss 3.516932(3.588537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3453 | Time 1.1977(1.2030) | Loss 3.554650(3.586165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3454 | Time 1.1998(1.2027) | Loss 3.531309(3.582325) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3455 | Time 1.2040(1.2028) | Loss 3.559958(3.580759) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3456 | Time 1.1912(1.2020) | Loss 3.478901(3.573629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3457 | Time 1.1962(1.2016) | Loss 3.542923(3.571480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3458 | Time 1.2446(1.2046) | Loss 3.535333(3.568949) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3459 | Time 1.2140(1.2053) | Loss 3.512002(3.564963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3460 | Time 1.2293(1.2070) | Loss 3.497173(3.560218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3461 | Time 1.2181(1.2077) | Loss 3.555597(3.559894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3462 | Time 1.2294(1.2093) | Loss 3.628771(3.564716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3463 | Time 1.2559(1.2125) | Loss 3.557437(3.564206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3464 | Time 1.2523(1.2153) | Loss 3.525664(3.561508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3465 | Time 1.2687(1.2190) | Loss 3.616441(3.565354) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3466 | Time 1.2357(1.2202) | Loss 3.521416(3.562278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3467 | Time 1.2370(1.2214) | Loss 3.575478(3.563202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3468 | Time 1.2031(1.2201) | Loss 3.516418(3.559927) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3469 | Time 1.1780(1.2172) | Loss 3.476372(3.554078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3470 | Time 1.2059(1.2164) | Loss 3.576808(3.555669) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3471 | Time 1.1908(1.2146) | Loss 3.491117(3.551151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3472 | Time 1.1839(1.2124) | Loss 3.482834(3.546368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3473 | Time 1.1897(1.2108) | Loss 3.483859(3.541993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3474 | Time 1.1869(1.2092) | Loss 3.589225(3.545299) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3475 | Time 1.1909(1.2079) | Loss 3.500347(3.542152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3476 | Time 1.1935(1.2069) | Loss 3.499899(3.539195) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3477 | Time 1.1793(1.2049) | Loss 3.535709(3.538951) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3478 | Time 1.1945(1.2042) | Loss 3.535719(3.538724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3479 | Time 1.1936(1.2035) | Loss 3.511644(3.536829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3480 | Time 1.1982(1.2031) | Loss 3.524567(3.535970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3481 | Time 1.1968(1.2027) | Loss 3.557106(3.537450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3482 | Time 1.1912(1.2019) | Loss 3.562790(3.539224) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3483 | Time 1.2007(1.2018) | Loss 3.567001(3.541168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3484 | Time 1.1792(1.2002) | Loss 3.599096(3.545223) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3485 | Time 1.2031(1.2004) | Loss 3.623134(3.550677) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3486 | Time 1.1857(1.1994) | Loss 3.625771(3.555933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3487 | Time 1.1803(1.1980) | Loss 3.550426(3.555548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3488 | Time 1.1907(1.1975) | Loss 3.684455(3.564571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3489 | Time 1.1918(1.1971) | Loss 3.561311(3.564343) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3490 | Time 1.2002(1.1973) | Loss 3.669728(3.571720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3491 | Time 1.1919(1.1970) | Loss 3.597356(3.573515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3492 | Time 1.1824(1.1959) | Loss 3.555942(3.572285) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3493 | Time 1.1800(1.1948) | Loss 3.579840(3.572813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3494 | Time 1.1796(1.1938) | Loss 3.506432(3.568167) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3495 | Time 1.1994(1.1942) | Loss 3.547462(3.566717) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3496 | Time 1.1923(1.1940) | Loss 3.563079(3.566463) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3497 | Time 1.1932(1.1940) | Loss 3.599548(3.568779) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3498 | Time 1.1995(1.1944) | Loss 3.511439(3.564765) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3499 | Time 1.2041(1.1950) | Loss 3.511325(3.561024) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3500 | Time 1.1934(1.1949) | Loss 3.529411(3.558811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3500 | Test Loss 3.496275 | NFE 20
Skipping vis as data dimension is >2
Iter 3501 | Time 1.1836(1.1941) | Loss 3.529179(3.556737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3502 | Time 1.1776(1.1930) | Loss 3.562535(3.557143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3503 | Time 1.1981(1.1933) | Loss 3.620114(3.561551) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3504 | Time 1.1869(1.1929) | Loss 3.596121(3.563971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3505 | Time 1.2508(1.1969) | Loss 3.531213(3.561678) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3506 | Time 1.1953(1.1968) | Loss 3.632573(3.566640) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3507 | Time 1.1893(1.1963) | Loss 3.616535(3.570133) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3508 | Time 1.1936(1.1961) | Loss 3.596105(3.571951) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3509 | Time 1.1889(1.1956) | Loss 3.552667(3.570601) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3510 | Time 1.1886(1.1951) | Loss 3.570545(3.570597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3511 | Time 1.1866(1.1945) | Loss 3.578705(3.571165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3512 | Time 1.2280(1.1969) | Loss 3.580372(3.571809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3513 | Time 1.1989(1.1970) | Loss 3.423729(3.561444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3514 | Time 1.2090(1.1979) | Loss 3.516792(3.558318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3515 | Time 1.1916(1.1974) | Loss 3.485205(3.553200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3516 | Time 1.2019(1.1977) | Loss 3.519312(3.550828) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3517 | Time 1.1833(1.1967) | Loss 3.558565(3.551370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3518 | Time 1.1993(1.1969) | Loss 3.574294(3.552974) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3519 | Time 1.2192(1.1985) | Loss 3.586333(3.555309) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3520 | Time 1.1926(1.1981) | Loss 3.545089(3.554594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3521 | Time 1.2271(1.2001) | Loss 3.599559(3.557741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3522 | Time 1.2029(1.2003) | Loss 3.488544(3.552898) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3523 | Time 1.1854(1.1992) | Loss 3.566460(3.553847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3524 | Time 1.1875(1.1984) | Loss 3.630318(3.559200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3525 | Time 1.1904(1.1979) | Loss 3.611859(3.562886) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3526 | Time 1.1949(1.1977) | Loss 3.578695(3.563993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3527 | Time 1.2116(1.1986) | Loss 3.535118(3.561971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3528 | Time 1.1953(1.1984) | Loss 3.590423(3.563963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3529 | Time 1.2012(1.1986) | Loss 3.680611(3.572128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3530 | Time 1.2145(1.1997) | Loss 3.676141(3.579409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3531 | Time 1.1885(1.1989) | Loss 3.679326(3.586403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3532 | Time 1.1998(1.1990) | Loss 3.699919(3.594349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3533 | Time 1.1881(1.1982) | Loss 3.670058(3.599649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3534 | Time 1.1883(1.1975) | Loss 3.691594(3.606085) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3535 | Time 1.1841(1.1966) | Loss 3.724737(3.614391) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3536 | Time 1.1881(1.1960) | Loss 3.647792(3.616729) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3537 | Time 1.1809(1.1949) | Loss 3.693656(3.622114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3538 | Time 1.1905(1.1946) | Loss 3.668553(3.625365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3539 | Time 1.2010(1.1951) | Loss 3.703946(3.630865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3540 | Time 1.2296(1.1975) | Loss 3.750604(3.639247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3541 | Time 1.1944(1.1973) | Loss 3.715316(3.644572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3542 | Time 1.1906(1.1968) | Loss 3.635626(3.643946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3543 | Time 1.1919(1.1965) | Loss 3.654706(3.644699) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3544 | Time 1.1984(1.1966) | Loss 3.626343(3.643414) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3545 | Time 1.2024(1.1970) | Loss 3.618947(3.641701) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3546 | Time 1.2063(1.1977) | Loss 3.596179(3.638515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3547 | Time 1.2081(1.1984) | Loss 3.591822(3.635246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3548 | Time 1.2086(1.1991) | Loss 3.729561(3.641848) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3549 | Time 1.2052(1.1995) | Loss 3.699433(3.645879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3550 | Time 1.1888(1.1988) | Loss 3.650745(3.646220) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3551 | Time 1.1998(1.1989) | Loss 3.658042(3.647047) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3552 | Time 1.1971(1.1987) | Loss 3.582321(3.642516) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3553 | Time 1.1876(1.1980) | Loss 3.640745(3.642392) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3554 | Time 1.1845(1.1970) | Loss 3.722628(3.648009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3555 | Time 1.1852(1.1962) | Loss 3.687530(3.650775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3556 | Time 1.2302(1.1986) | Loss 3.701811(3.654348) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3557 | Time 1.1900(1.1980) | Loss 3.622671(3.652130) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3558 | Time 1.1865(1.1972) | Loss 3.633703(3.650841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3559 | Time 1.1850(1.1963) | Loss 3.640867(3.650142) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3560 | Time 1.1744(1.1948) | Loss 3.702819(3.653830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3561 | Time 1.1865(1.1942) | Loss 3.652601(3.653744) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3562 | Time 1.1701(1.1925) | Loss 3.615355(3.651057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3563 | Time 1.1711(1.1910) | Loss 3.666257(3.652121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3564 | Time 1.1886(1.1908) | Loss 3.706655(3.655938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3565 | Time 1.1687(1.1893) | Loss 3.741033(3.661895) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3566 | Time 1.1967(1.1898) | Loss 3.680424(3.663192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3567 | Time 1.2053(1.1909) | Loss 3.786093(3.671795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3568 | Time 1.2197(1.1929) | Loss 3.735924(3.676284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3569 | Time 1.2140(1.1944) | Loss 3.627777(3.672888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3570 | Time 1.2101(1.1955) | Loss 3.720022(3.676188) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3571 | Time 1.2058(1.1962) | Loss 3.663494(3.675299) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3572 | Time 1.2056(1.1969) | Loss 3.619431(3.671388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3573 | Time 1.2057(1.1975) | Loss 3.641347(3.669286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3574 | Time 1.2562(1.2016) | Loss 3.601819(3.664563) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3575 | Time 1.2133(1.2024) | Loss 3.567125(3.657742) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3576 | Time 1.2221(1.2038) | Loss 3.624510(3.655416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3577 | Time 1.2047(1.2038) | Loss 3.538127(3.647206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3578 | Time 1.1966(1.2033) | Loss 3.572289(3.641962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3579 | Time 1.2020(1.2032) | Loss 3.546096(3.635251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3580 | Time 1.2184(1.2043) | Loss 3.586174(3.631816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3581 | Time 1.2155(1.2051) | Loss 3.630550(3.631727) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3582 | Time 1.2011(1.2048) | Loss 3.493747(3.622068) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3583 | Time 1.2052(1.2048) | Loss 3.587003(3.619614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3584 | Time 1.2129(1.2054) | Loss 3.574815(3.616478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3585 | Time 1.2094(1.2057) | Loss 3.598836(3.615243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3586 | Time 1.2088(1.2059) | Loss 3.600549(3.614214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3587 | Time 1.2160(1.2066) | Loss 3.610306(3.613941) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3588 | Time 1.2081(1.2067) | Loss 3.579379(3.611522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3589 | Time 1.2199(1.2076) | Loss 3.643966(3.613793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3590 | Time 1.2399(1.2099) | Loss 3.636088(3.615353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3591 | Time 1.2144(1.2102) | Loss 3.566547(3.611937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3592 | Time 1.2159(1.2106) | Loss 3.568724(3.608912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3593 | Time 1.2129(1.2108) | Loss 3.572075(3.606333) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3594 | Time 1.2211(1.2115) | Loss 3.625361(3.607665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3595 | Time 1.2328(1.2130) | Loss 3.631234(3.609315) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3596 | Time 1.2208(1.2135) | Loss 3.621489(3.610167) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3597 | Time 1.2156(1.2137) | Loss 3.644615(3.612579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3598 | Time 1.2194(1.2141) | Loss 3.706688(3.619166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3599 | Time 1.2177(1.2143) | Loss 3.604719(3.618155) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3600 | Time 1.2156(1.2144) | Loss 3.614809(3.617921) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3600 | Test Loss 3.770816 | NFE 20
Skipping vis as data dimension is >2
Iter 3601 | Time 1.2141(1.2144) | Loss 3.659312(3.620818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3602 | Time 1.2055(1.2138) | Loss 3.662187(3.623714) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3603 | Time 1.1985(1.2127) | Loss 3.711955(3.629891) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3604 | Time 1.2039(1.2121) | Loss 3.659333(3.631952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3605 | Time 1.2020(1.2114) | Loss 3.677914(3.635169) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3606 | Time 1.2161(1.2117) | Loss 3.628296(3.634688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3607 | Time 1.2068(1.2114) | Loss 3.648157(3.635631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3608 | Time 1.2030(1.2108) | Loss 3.650371(3.636663) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3609 | Time 1.1990(1.2100) | Loss 3.617752(3.635339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3610 | Time 1.2089(1.2099) | Loss 3.612600(3.633747) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3611 | Time 1.2168(1.2104) | Loss 3.703781(3.638649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3612 | Time 1.2023(1.2098) | Loss 3.712396(3.643812) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3613 | Time 1.2693(1.2140) | Loss 3.567860(3.638495) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3614 | Time 1.2202(1.2144) | Loss 3.679631(3.641375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3615 | Time 1.2011(1.2135) | Loss 3.661214(3.642763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3616 | Time 1.2037(1.2128) | Loss 3.687358(3.645885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3617 | Time 1.1994(1.2119) | Loss 3.652080(3.646319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3618 | Time 1.1992(1.2110) | Loss 3.748009(3.653437) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3619 | Time 1.1984(1.2101) | Loss 3.720752(3.658149) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3620 | Time 1.1879(1.2085) | Loss 3.639554(3.656847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3621 | Time 1.2375(1.2106) | Loss 3.725015(3.661619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3622 | Time 1.1972(1.2096) | Loss 3.655412(3.661185) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3623 | Time 1.2020(1.2091) | Loss 3.600669(3.656949) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3624 | Time 1.2065(1.2089) | Loss 3.642570(3.655942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3625 | Time 1.2022(1.2084) | Loss 3.726126(3.660855) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3626 | Time 1.2279(1.2098) | Loss 3.676361(3.661940) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3627 | Time 1.2135(1.2101) | Loss 3.585026(3.656556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3628 | Time 1.2252(1.2111) | Loss 3.645392(3.655775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3629 | Time 1.2636(1.2148) | Loss 3.615121(3.652929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3630 | Time 1.2009(1.2138) | Loss 3.622929(3.650829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3631 | Time 1.2002(1.2129) | Loss 3.591976(3.646709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3632 | Time 1.1973(1.2118) | Loss 3.582678(3.642227) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3633 | Time 1.1920(1.2104) | Loss 3.580016(3.637872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3634 | Time 1.1950(1.2093) | Loss 3.521345(3.629715) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3635 | Time 1.2784(1.2142) | Loss 3.541786(3.623560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3636 | Time 1.2040(1.2135) | Loss 3.681276(3.627601) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3637 | Time 1.2385(1.2152) | Loss 3.584014(3.624549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3638 | Time 1.2376(1.2168) | Loss 3.605044(3.623184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3639 | Time 1.2222(1.2172) | Loss 3.588070(3.620726) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3640 | Time 1.2119(1.2168) | Loss 3.672677(3.624363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3641 | Time 1.2121(1.2165) | Loss 3.611520(3.623464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3642 | Time 1.2210(1.2168) | Loss 3.575389(3.620098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3643 | Time 1.2375(1.2182) | Loss 3.603367(3.618927) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3644 | Time 1.2603(1.2212) | Loss 3.613385(3.618539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3645 | Time 1.2443(1.2228) | Loss 3.653379(3.620978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3646 | Time 1.2118(1.2220) | Loss 3.633556(3.621859) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3647 | Time 1.1985(1.2204) | Loss 3.660540(3.624566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3648 | Time 1.2165(1.2201) | Loss 3.668298(3.627628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3649 | Time 1.2139(1.2197) | Loss 3.659247(3.629841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3650 | Time 1.1986(1.2182) | Loss 3.669659(3.632628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3651 | Time 1.2025(1.2171) | Loss 3.715054(3.638398) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3652 | Time 1.2375(1.2185) | Loss 3.687437(3.641831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3653 | Time 1.2250(1.2190) | Loss 3.661288(3.643193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3654 | Time 1.2151(1.2187) | Loss 3.691303(3.646560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3655 | Time 1.2316(1.2196) | Loss 3.669757(3.648184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3656 | Time 1.2298(1.2203) | Loss 3.623613(3.646464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3657 | Time 1.2198(1.2203) | Loss 3.686840(3.649290) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3658 | Time 1.2251(1.2206) | Loss 3.620506(3.647276) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3659 | Time 1.2261(1.2210) | Loss 3.673320(3.649099) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3660 | Time 1.2517(1.2232) | Loss 3.592820(3.645159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3661 | Time 1.2345(1.2239) | Loss 3.594238(3.641595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3662 | Time 1.2164(1.2234) | Loss 3.534217(3.634078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3663 | Time 1.2206(1.2232) | Loss 3.532350(3.626957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3664 | Time 1.2304(1.2237) | Loss 3.466986(3.615759) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3665 | Time 1.2068(1.2225) | Loss 3.539383(3.610413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3666 | Time 1.2017(1.2211) | Loss 3.566615(3.607347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3667 | Time 1.2084(1.2202) | Loss 3.577479(3.605256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3668 | Time 1.2170(1.2200) | Loss 3.472304(3.595950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3669 | Time 1.1949(1.2182) | Loss 3.536699(3.591802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3670 | Time 1.2020(1.2171) | Loss 3.529128(3.587415) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3671 | Time 1.2176(1.2171) | Loss 3.596346(3.588040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3672 | Time 1.2086(1.2165) | Loss 3.454643(3.578702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3673 | Time 1.2038(1.2156) | Loss 3.566669(3.577860) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3674 | Time 1.2123(1.2154) | Loss 3.499566(3.572379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3675 | Time 1.2352(1.2168) | Loss 3.531859(3.569543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3676 | Time 1.2116(1.2164) | Loss 3.573576(3.569825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3677 | Time 1.2127(1.2162) | Loss 3.465390(3.562515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3678 | Time 1.2256(1.2168) | Loss 3.476858(3.556519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3679 | Time 1.2133(1.2166) | Loss 3.570028(3.557464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3680 | Time 1.2254(1.2172) | Loss 3.562558(3.557821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3681 | Time 1.1941(1.2156) | Loss 3.517190(3.554977) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3682 | Time 1.1911(1.2139) | Loss 3.604968(3.558476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3683 | Time 1.2051(1.2132) | Loss 3.633521(3.563729) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3684 | Time 1.1941(1.2119) | Loss 3.640370(3.569094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3685 | Time 1.2382(1.2137) | Loss 3.698721(3.578168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3686 | Time 1.2007(1.2128) | Loss 3.663052(3.584110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3687 | Time 1.2136(1.2129) | Loss 3.722767(3.593816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3688 | Time 1.2092(1.2126) | Loss 3.663005(3.598659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3689 | Time 1.2109(1.2125) | Loss 3.798143(3.612623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3690 | Time 1.2181(1.2129) | Loss 3.850618(3.629283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3691 | Time 1.2097(1.2127) | Loss 3.685025(3.633185) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3692 | Time 1.2091(1.2124) | Loss 3.739422(3.640621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3693 | Time 1.1926(1.2110) | Loss 3.729438(3.646838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3694 | Time 1.2057(1.2107) | Loss 3.669358(3.648415) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3695 | Time 1.2025(1.2101) | Loss 3.625788(3.646831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3696 | Time 1.2268(1.2113) | Loss 3.573529(3.641700) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3697 | Time 1.2051(1.2108) | Loss 3.687844(3.644930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3698 | Time 1.2588(1.2142) | Loss 3.658705(3.645894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3699 | Time 1.2447(1.2163) | Loss 3.528186(3.637655) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3700 | Time 1.2073(1.2157) | Loss 3.637380(3.637635) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3700 | Test Loss 3.593883 | NFE 20
Skipping vis as data dimension is >2
Iter 3701 | Time 1.2079(1.2151) | Loss 3.645621(3.638194) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3702 | Time 1.2401(1.2169) | Loss 3.550801(3.632077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3703 | Time 1.2050(1.2161) | Loss 3.578081(3.628297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3704 | Time 1.2050(1.2153) | Loss 3.563363(3.623752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3705 | Time 1.1905(1.2135) | Loss 3.589015(3.621320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3706 | Time 1.2131(1.2135) | Loss 3.614258(3.620826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3707 | Time 1.1951(1.2122) | Loss 3.666874(3.624049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3708 | Time 1.2017(1.2115) | Loss 3.631033(3.624538) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3709 | Time 1.2055(1.2111) | Loss 3.555884(3.619732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3710 | Time 1.1993(1.2102) | Loss 3.623061(3.619965) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3711 | Time 1.1905(1.2089) | Loss 3.582886(3.617370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3712 | Time 1.1925(1.2077) | Loss 3.686316(3.622196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3713 | Time 1.2081(1.2077) | Loss 3.650219(3.624158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3714 | Time 1.2283(1.2092) | Loss 3.655499(3.626351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3715 | Time 1.2045(1.2089) | Loss 3.624547(3.626225) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3716 | Time 1.2426(1.2112) | Loss 3.663142(3.628809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3717 | Time 1.1953(1.2101) | Loss 3.685926(3.632808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3718 | Time 1.2495(1.2129) | Loss 3.615261(3.631579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3719 | Time 1.2099(1.2127) | Loss 3.639289(3.632119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3720 | Time 1.2050(1.2121) | Loss 3.593323(3.629403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3721 | Time 1.1981(1.2111) | Loss 3.579432(3.625905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3722 | Time 1.2011(1.2104) | Loss 3.587926(3.623247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3723 | Time 1.2029(1.2099) | Loss 3.607911(3.622173) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3724 | Time 1.1956(1.2089) | Loss 3.741997(3.630561) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3725 | Time 1.1819(1.2070) | Loss 3.614048(3.629405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3726 | Time 1.2737(1.2117) | Loss 3.674524(3.632563) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3727 | Time 1.2741(1.2160) | Loss 3.657555(3.634313) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3728 | Time 1.2279(1.2169) | Loss 3.641115(3.634789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3729 | Time 1.2344(1.2181) | Loss 3.690932(3.638719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3730 | Time 1.2245(1.2185) | Loss 3.614344(3.637013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3731 | Time 1.1904(1.2166) | Loss 3.607737(3.634963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3732 | Time 1.1987(1.2153) | Loss 3.714237(3.640513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3733 | Time 1.2099(1.2149) | Loss 3.716351(3.645821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3734 | Time 1.2020(1.2140) | Loss 3.608013(3.643175) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3735 | Time 1.2073(1.2136) | Loss 3.632252(3.642410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3736 | Time 1.2165(1.2138) | Loss 3.642360(3.642407) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3737 | Time 1.2241(1.2145) | Loss 3.632218(3.641693) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3738 | Time 1.2126(1.2144) | Loss 3.548075(3.635140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3739 | Time 1.2136(1.2143) | Loss 3.652616(3.636363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3740 | Time 1.2086(1.2139) | Loss 3.606414(3.634267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3741 | Time 1.2100(1.2136) | Loss 3.597207(3.631673) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3742 | Time 1.1987(1.2126) | Loss 3.606194(3.629889) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3743 | Time 1.1951(1.2114) | Loss 3.596809(3.627574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3744 | Time 1.2046(1.2109) | Loss 3.694281(3.632243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3745 | Time 1.2083(1.2107) | Loss 3.604110(3.630274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3746 | Time 1.2083(1.2105) | Loss 3.589858(3.627445) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3747 | Time 1.1806(1.2084) | Loss 3.576571(3.623884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3748 | Time 1.1914(1.2073) | Loss 3.537939(3.617867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3749 | Time 1.1996(1.2067) | Loss 3.658621(3.620720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3750 | Time 1.2047(1.2066) | Loss 3.576694(3.617638) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3751 | Time 1.1974(1.2059) | Loss 3.588126(3.615572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3752 | Time 1.1943(1.2051) | Loss 3.647086(3.617778) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3753 | Time 1.1841(1.2036) | Loss 3.577109(3.614932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3754 | Time 1.1916(1.2028) | Loss 3.460141(3.604096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3755 | Time 1.1821(1.2014) | Loss 3.589526(3.603076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3756 | Time 1.2118(1.2021) | Loss 3.624109(3.604549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3757 | Time 1.2120(1.2028) | Loss 3.645600(3.607422) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3758 | Time 1.2042(1.2029) | Loss 3.610710(3.607652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3759 | Time 1.2091(1.2033) | Loss 3.606012(3.607537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3760 | Time 1.2030(1.2033) | Loss 3.525297(3.601781) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3761 | Time 1.2214(1.2046) | Loss 3.575758(3.599959) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3762 | Time 1.2162(1.2054) | Loss 3.454980(3.589811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3763 | Time 1.1969(1.2048) | Loss 3.568206(3.588298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3764 | Time 1.2124(1.2053) | Loss 3.538017(3.584779) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3765 | Time 1.2021(1.2051) | Loss 3.517365(3.580060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3766 | Time 1.2084(1.2053) | Loss 3.521420(3.575955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3767 | Time 1.2163(1.2061) | Loss 3.495610(3.570331) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3768 | Time 1.1857(1.2047) | Loss 3.482466(3.564180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3769 | Time 1.1827(1.2031) | Loss 3.544470(3.562801) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3770 | Time 1.1931(1.2024) | Loss 3.604119(3.565693) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3771 | Time 1.2170(1.2034) | Loss 3.596819(3.567872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3772 | Time 1.2609(1.2075) | Loss 3.463394(3.560558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3773 | Time 1.2125(1.2078) | Loss 3.610633(3.564063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3774 | Time 1.2126(1.2081) | Loss 3.520318(3.561001) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3775 | Time 1.1945(1.2072) | Loss 3.700848(3.570791) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3776 | Time 1.2048(1.2070) | Loss 3.598746(3.572747) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3777 | Time 1.1852(1.2055) | Loss 3.590684(3.574003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3778 | Time 1.1936(1.2047) | Loss 3.628272(3.577802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3779 | Time 1.2057(1.2047) | Loss 3.609374(3.580012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3780 | Time 1.1925(1.2039) | Loss 3.631033(3.583583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3781 | Time 1.2553(1.2075) | Loss 3.710792(3.592488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3782 | Time 1.2551(1.2108) | Loss 3.621030(3.594486) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3783 | Time 1.2806(1.2157) | Loss 3.659330(3.599025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3784 | Time 1.2845(1.2205) | Loss 3.642126(3.602042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3785 | Time 1.2510(1.2226) | Loss 3.685006(3.607849) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3786 | Time 1.2193(1.2224) | Loss 3.607701(3.607839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3787 | Time 1.2173(1.2221) | Loss 3.652042(3.610933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3788 | Time 1.2176(1.2217) | Loss 3.624418(3.611877) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3789 | Time 1.2050(1.2206) | Loss 3.637015(3.613637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3790 | Time 1.1976(1.2190) | Loss 3.631197(3.614866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3791 | Time 1.2116(1.2184) | Loss 3.685127(3.619784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3792 | Time 1.2190(1.2185) | Loss 3.614930(3.619445) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3793 | Time 1.2092(1.2178) | Loss 3.595421(3.617763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3794 | Time 1.2088(1.2172) | Loss 3.604858(3.616860) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3795 | Time 1.1995(1.2160) | Loss 3.565709(3.613279) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3796 | Time 1.1999(1.2148) | Loss 3.618886(3.613671) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3797 | Time 1.2120(1.2146) | Loss 3.654929(3.616560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3798 | Time 1.2137(1.2146) | Loss 3.632540(3.617678) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3799 | Time 1.2096(1.2142) | Loss 3.635022(3.618892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3800 | Time 1.2165(1.2144) | Loss 3.657336(3.621583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3800 | Test Loss 3.598814 | NFE 20
Skipping vis as data dimension is >2
Iter 3801 | Time 1.1958(1.2131) | Loss 3.665942(3.624688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3802 | Time 1.2063(1.2126) | Loss 3.629864(3.625051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3803 | Time 1.1999(1.2117) | Loss 3.562118(3.620645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3804 | Time 1.1953(1.2106) | Loss 3.574286(3.617400) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3805 | Time 1.1980(1.2097) | Loss 3.591023(3.615554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3806 | Time 1.1878(1.2082) | Loss 3.542042(3.610408) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3807 | Time 1.2048(1.2079) | Loss 3.498993(3.602609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3808 | Time 1.2110(1.2081) | Loss 3.537708(3.598066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3809 | Time 1.2130(1.2085) | Loss 3.569001(3.596031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3810 | Time 1.2384(1.2106) | Loss 3.561549(3.593618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3811 | Time 1.1992(1.2098) | Loss 3.521325(3.588557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3812 | Time 1.2086(1.2097) | Loss 3.517330(3.583571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3813 | Time 1.2014(1.2091) | Loss 3.559682(3.581899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3814 | Time 1.1979(1.2083) | Loss 3.520922(3.577630) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3815 | Time 1.2132(1.2087) | Loss 3.641784(3.582121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3816 | Time 1.1976(1.2079) | Loss 3.585171(3.582335) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3817 | Time 1.1789(1.2059) | Loss 3.576744(3.581943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3818 | Time 1.1929(1.2050) | Loss 3.591131(3.582586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3819 | Time 1.2003(1.2046) | Loss 3.533544(3.579154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3820 | Time 1.1999(1.2043) | Loss 3.596516(3.580369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3821 | Time 1.2011(1.2041) | Loss 3.542692(3.577732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3822 | Time 1.1979(1.2036) | Loss 3.554650(3.576116) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3823 | Time 1.2070(1.2039) | Loss 3.669262(3.582636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3824 | Time 1.2034(1.2038) | Loss 3.575743(3.582154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3825 | Time 1.2099(1.2043) | Loss 3.660154(3.587614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3826 | Time 1.2331(1.2063) | Loss 3.534778(3.583915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3827 | Time 1.1868(1.2049) | Loss 3.600042(3.585044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3828 | Time 1.1876(1.2037) | Loss 3.551505(3.582696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3829 | Time 1.1943(1.2031) | Loss 3.610490(3.584642) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3830 | Time 1.2019(1.2030) | Loss 3.570600(3.583659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3831 | Time 1.2062(1.2032) | Loss 3.550200(3.581317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3832 | Time 1.1940(1.2026) | Loss 3.474239(3.573821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3833 | Time 1.1809(1.2010) | Loss 3.527911(3.570608) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3834 | Time 1.2015(1.2011) | Loss 3.571718(3.570685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3835 | Time 1.2011(1.2011) | Loss 3.628064(3.574702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3836 | Time 1.2005(1.2010) | Loss 3.592427(3.575943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3837 | Time 1.1990(1.2009) | Loss 3.527039(3.572519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3838 | Time 1.2138(1.2018) | Loss 3.652064(3.578088) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3839 | Time 1.2061(1.2021) | Loss 3.720607(3.588064) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3840 | Time 1.2228(1.2035) | Loss 3.648391(3.592287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3841 | Time 1.1855(1.2023) | Loss 3.654011(3.596608) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3842 | Time 1.1940(1.2017) | Loss 3.689397(3.603103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3843 | Time 1.2050(1.2019) | Loss 3.637711(3.605525) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3844 | Time 1.2045(1.2021) | Loss 3.595030(3.604791) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3845 | Time 1.1964(1.2017) | Loss 3.615958(3.605572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3846 | Time 1.1847(1.2005) | Loss 3.622187(3.606735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3847 | Time 1.1978(1.2003) | Loss 3.503336(3.599497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3848 | Time 1.2042(1.2006) | Loss 3.588878(3.598754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3849 | Time 1.2070(1.2011) | Loss 3.555090(3.595698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3850 | Time 1.2023(1.2011) | Loss 3.501692(3.589117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3851 | Time 1.1943(1.2007) | Loss 3.505702(3.583278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3852 | Time 1.2023(1.2008) | Loss 3.485852(3.576458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3853 | Time 1.1897(1.2000) | Loss 3.528248(3.573084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3854 | Time 1.1939(1.1996) | Loss 3.518878(3.569289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3855 | Time 1.1954(1.1993) | Loss 3.526000(3.566259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3856 | Time 1.2017(1.1995) | Loss 3.486872(3.560702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3857 | Time 1.2011(1.1996) | Loss 3.511063(3.557227) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3858 | Time 1.1988(1.1995) | Loss 3.570043(3.558124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3859 | Time 1.1986(1.1994) | Loss 3.517006(3.555246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3860 | Time 1.2014(1.1996) | Loss 3.612376(3.559245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3861 | Time 1.1921(1.1991) | Loss 3.606264(3.562536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3862 | Time 1.1931(1.1986) | Loss 3.565601(3.562751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3863 | Time 1.1979(1.1986) | Loss 3.586381(3.564405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3864 | Time 1.1967(1.1985) | Loss 3.509588(3.560568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3865 | Time 1.2423(1.2015) | Loss 3.629765(3.565412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3866 | Time 1.2131(1.2023) | Loss 3.589105(3.567070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3867 | Time 1.2059(1.2026) | Loss 3.601956(3.569512) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3868 | Time 1.2061(1.2028) | Loss 3.622383(3.573213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3869 | Time 1.2259(1.2044) | Loss 3.560510(3.572324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3870 | Time 1.2014(1.2042) | Loss 3.602388(3.574428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3871 | Time 1.2056(1.2043) | Loss 3.589923(3.575513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3872 | Time 1.2011(1.2041) | Loss 3.566763(3.574901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3873 | Time 1.2042(1.2041) | Loss 3.636892(3.579240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3874 | Time 1.1980(1.2037) | Loss 3.553564(3.577443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3875 | Time 1.2036(1.2037) | Loss 3.469045(3.569855) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3876 | Time 1.1893(1.2027) | Loss 3.479203(3.563509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3877 | Time 1.1819(1.2012) | Loss 3.534066(3.561448) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3878 | Time 1.1889(1.2004) | Loss 3.565541(3.561735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3879 | Time 1.1905(1.1997) | Loss 3.488600(3.556615) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3880 | Time 1.2016(1.1998) | Loss 3.438006(3.548313) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3881 | Time 1.2164(1.2010) | Loss 3.467244(3.542638) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3882 | Time 1.1875(1.2000) | Loss 3.457294(3.536664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3883 | Time 1.1759(1.1983) | Loss 3.433278(3.529427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3884 | Time 1.1784(1.1969) | Loss 3.489898(3.526660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3885 | Time 1.2163(1.1983) | Loss 3.483663(3.523650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3886 | Time 1.1983(1.1983) | Loss 3.428613(3.516997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3887 | Time 1.1847(1.1973) | Loss 3.474490(3.514022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3888 | Time 1.1817(1.1962) | Loss 3.435352(3.508515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3889 | Time 1.1990(1.1964) | Loss 3.403798(3.501185) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3890 | Time 1.2002(1.1967) | Loss 3.430169(3.496214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3891 | Time 1.2421(1.1999) | Loss 3.431876(3.491710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3892 | Time 1.2746(1.2051) | Loss 3.479823(3.490878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3893 | Time 1.2835(1.2106) | Loss 3.523790(3.493182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3894 | Time 1.1973(1.2097) | Loss 3.512860(3.494559) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3895 | Time 1.1999(1.2090) | Loss 3.437912(3.490594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3896 | Time 1.2018(1.2085) | Loss 3.524912(3.492996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3897 | Time 1.1984(1.2078) | Loss 3.511732(3.494308) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3898 | Time 1.1944(1.2068) | Loss 3.459923(3.491901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3899 | Time 1.2102(1.2071) | Loss 3.500649(3.492513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3900 | Time 1.2215(1.2081) | Loss 3.587767(3.499181) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 3900 | Test Loss 3.482651 | NFE 20
Skipping vis as data dimension is >2
Iter 3901 | Time 1.1891(1.2068) | Loss 3.527900(3.501191) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3902 | Time 1.1876(1.2054) | Loss 3.511123(3.501886) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3903 | Time 1.2414(1.2079) | Loss 3.446415(3.498003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3904 | Time 1.1983(1.2073) | Loss 3.552887(3.501845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3905 | Time 1.1966(1.2065) | Loss 3.498746(3.501628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3906 | Time 1.2669(1.2107) | Loss 3.400567(3.494554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3907 | Time 1.2714(1.2150) | Loss 3.506210(3.495370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3908 | Time 1.2880(1.2201) | Loss 3.511869(3.496525) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3909 | Time 1.2127(1.2196) | Loss 3.463576(3.494218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3910 | Time 1.2169(1.2194) | Loss 3.521894(3.496156) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3911 | Time 1.2157(1.2191) | Loss 3.615322(3.504497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3912 | Time 1.2124(1.2187) | Loss 3.487150(3.503283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3913 | Time 1.2112(1.2181) | Loss 3.446459(3.499305) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3914 | Time 1.2672(1.2216) | Loss 3.428645(3.494359) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3915 | Time 1.2733(1.2252) | Loss 3.536869(3.497335) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3916 | Time 1.2801(1.2290) | Loss 3.370475(3.488455) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3917 | Time 1.2728(1.2321) | Loss 3.409309(3.482915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3918 | Time 1.2937(1.2364) | Loss 3.482316(3.482873) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3919 | Time 1.2665(1.2385) | Loss 3.438601(3.479774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3920 | Time 1.2580(1.2399) | Loss 3.457911(3.478243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3921 | Time 1.2613(1.2414) | Loss 3.489239(3.479013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3922 | Time 1.2404(1.2413) | Loss 3.537739(3.483124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3923 | Time 1.2099(1.2391) | Loss 3.437139(3.479905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3924 | Time 1.2151(1.2374) | Loss 3.466624(3.478975) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3925 | Time 1.1928(1.2343) | Loss 3.536290(3.482987) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3926 | Time 1.2014(1.2320) | Loss 3.446575(3.480438) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3927 | Time 1.1963(1.2295) | Loss 3.551153(3.485388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3928 | Time 1.1932(1.2270) | Loss 3.506930(3.486896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3929 | Time 1.2115(1.2259) | Loss 3.590754(3.494166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3930 | Time 1.1834(1.2229) | Loss 3.626701(3.503444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3931 | Time 1.1921(1.2207) | Loss 3.508373(3.503789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3932 | Time 1.1991(1.2192) | Loss 3.637814(3.513171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3933 | Time 1.2182(1.2192) | Loss 3.588415(3.518438) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3934 | Time 1.2255(1.2196) | Loss 3.664483(3.528661) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3935 | Time 1.2044(1.2185) | Loss 3.633294(3.535985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3936 | Time 1.1966(1.2170) | Loss 3.648576(3.543867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3937 | Time 1.1868(1.2149) | Loss 3.659752(3.551978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3938 | Time 1.1941(1.2134) | Loss 3.569796(3.553226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3939 | Time 1.1824(1.2113) | Loss 3.614882(3.557542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3940 | Time 1.1804(1.2091) | Loss 3.579931(3.559109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3941 | Time 1.2557(1.2124) | Loss 3.530513(3.557107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3942 | Time 1.2691(1.2163) | Loss 3.588441(3.559301) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3943 | Time 1.2694(1.2200) | Loss 3.505546(3.555538) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3944 | Time 1.2634(1.2231) | Loss 3.569139(3.556490) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3945 | Time 1.2191(1.2228) | Loss 3.626858(3.561416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3946 | Time 1.2555(1.2251) | Loss 3.553934(3.560892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3947 | Time 1.2178(1.2246) | Loss 3.508796(3.557245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3948 | Time 1.2107(1.2236) | Loss 3.509550(3.553906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3949 | Time 1.2118(1.2228) | Loss 3.544506(3.553248) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3950 | Time 1.2176(1.2224) | Loss 3.583505(3.555366) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3951 | Time 1.2081(1.2214) | Loss 3.479224(3.550036) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3952 | Time 1.1978(1.2198) | Loss 3.506405(3.546982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3953 | Time 1.2061(1.2188) | Loss 3.582266(3.549452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3954 | Time 1.2137(1.2185) | Loss 3.637813(3.555637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3955 | Time 1.1931(1.2167) | Loss 3.507727(3.552284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3956 | Time 1.1874(1.2146) | Loss 3.543966(3.551701) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3957 | Time 1.2009(1.2137) | Loss 3.509017(3.548714) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3958 | Time 1.1920(1.2122) | Loss 3.490448(3.544635) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3959 | Time 1.1916(1.2107) | Loss 3.441256(3.537398) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3960 | Time 1.1940(1.2095) | Loss 3.480392(3.533408) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3961 | Time 1.1836(1.2077) | Loss 3.476107(3.529397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3962 | Time 1.2250(1.2089) | Loss 3.469192(3.525183) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3963 | Time 1.2446(1.2114) | Loss 3.547332(3.526733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3964 | Time 1.2049(1.2110) | Loss 3.460205(3.522076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3965 | Time 1.1874(1.2093) | Loss 3.422466(3.515103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3966 | Time 1.1965(1.2084) | Loss 3.553634(3.517801) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3967 | Time 1.2301(1.2099) | Loss 3.518566(3.517854) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3968 | Time 1.2248(1.2110) | Loss 3.441629(3.512518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3969 | Time 1.2110(1.2110) | Loss 3.529958(3.513739) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3970 | Time 1.2017(1.2103) | Loss 3.478399(3.511265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3971 | Time 1.1940(1.2092) | Loss 3.568738(3.515288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3972 | Time 1.1981(1.2084) | Loss 3.457879(3.511270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3973 | Time 1.2361(1.2103) | Loss 3.528708(3.512490) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3974 | Time 1.2077(1.2102) | Loss 3.510817(3.512373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3975 | Time 1.1956(1.2091) | Loss 3.449307(3.507959) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3976 | Time 1.1881(1.2077) | Loss 3.555883(3.511313) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3977 | Time 1.2020(1.2073) | Loss 3.553951(3.514298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3978 | Time 1.2005(1.2068) | Loss 3.529839(3.515386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3979 | Time 1.2158(1.2074) | Loss 3.519704(3.515688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3980 | Time 1.1963(1.2066) | Loss 3.510718(3.515340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3981 | Time 1.1766(1.2045) | Loss 3.556963(3.518254) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3982 | Time 1.1820(1.2030) | Loss 3.573297(3.522107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3983 | Time 1.2114(1.2036) | Loss 3.563945(3.525035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3984 | Time 1.1949(1.2030) | Loss 3.522934(3.524888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3985 | Time 1.1895(1.2020) | Loss 3.556833(3.527124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3986 | Time 1.2234(1.2035) | Loss 3.578432(3.530716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3987 | Time 1.2660(1.2079) | Loss 3.444559(3.524685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3988 | Time 1.1987(1.2072) | Loss 3.524556(3.524676) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3989 | Time 1.1928(1.2062) | Loss 3.478761(3.521462) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3990 | Time 1.1918(1.2052) | Loss 3.554505(3.523775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3991 | Time 1.1974(1.2047) | Loss 3.512837(3.523009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3992 | Time 1.1914(1.2037) | Loss 3.520063(3.522803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3993 | Time 1.1952(1.2032) | Loss 3.476026(3.519529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3994 | Time 1.2087(1.2035) | Loss 3.464413(3.515671) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3995 | Time 1.2020(1.2034) | Loss 3.603216(3.521799) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3996 | Time 1.1988(1.2031) | Loss 3.539445(3.523034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3997 | Time 1.2087(1.2035) | Loss 3.578633(3.526926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3998 | Time 1.2091(1.2039) | Loss 3.612110(3.532889) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 3999 | Time 1.2220(1.2052) | Loss 3.624564(3.539306) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4000 | Time 1.2100(1.2055) | Loss 3.562742(3.540947) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4000 | Test Loss 3.547791 | NFE 20
Skipping vis as data dimension is >2
Iter 4001 | Time 1.2137(1.2061) | Loss 3.553991(3.541860) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4002 | Time 1.2177(1.2069) | Loss 3.581933(3.544665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4003 | Time 1.2121(1.2073) | Loss 3.560841(3.545797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4004 | Time 1.2024(1.2069) | Loss 3.514203(3.543586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4005 | Time 1.2055(1.2068) | Loss 3.543247(3.543562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4006 | Time 1.2042(1.2066) | Loss 3.557111(3.544510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4007 | Time 1.2180(1.2074) | Loss 3.428406(3.536383) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4008 | Time 1.1987(1.2068) | Loss 3.498937(3.533762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4009 | Time 1.1823(1.2051) | Loss 3.494654(3.531024) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4010 | Time 1.1817(1.2035) | Loss 3.387182(3.520955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4011 | Time 1.1937(1.2028) | Loss 3.413494(3.513433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4012 | Time 1.2099(1.2033) | Loss 3.413034(3.506405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4013 | Time 1.2126(1.2039) | Loss 3.432115(3.501205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4014 | Time 1.1925(1.2031) | Loss 3.417173(3.495323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4015 | Time 1.2018(1.2030) | Loss 3.449123(3.492089) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4016 | Time 1.2055(1.2032) | Loss 3.431020(3.487814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4017 | Time 1.2089(1.2036) | Loss 3.363736(3.479128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4018 | Time 1.2007(1.2034) | Loss 3.427893(3.475542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4019 | Time 1.2029(1.2034) | Loss 3.440340(3.473078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4020 | Time 1.1851(1.2021) | Loss 3.443058(3.470976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4021 | Time 1.2374(1.2046) | Loss 3.486303(3.472049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4022 | Time 1.2264(1.2061) | Loss 3.494513(3.473622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4023 | Time 1.2034(1.2059) | Loss 3.490152(3.474779) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4024 | Time 1.1961(1.2052) | Loss 3.543353(3.479579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4025 | Time 1.1956(1.2045) | Loss 3.522198(3.482562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4026 | Time 1.1949(1.2039) | Loss 3.573252(3.488911) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4027 | Time 1.2138(1.2046) | Loss 3.596976(3.496475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4028 | Time 1.1916(1.2037) | Loss 3.589642(3.502997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4029 | Time 1.2008(1.2035) | Loss 3.577219(3.508192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4030 | Time 1.1779(1.2017) | Loss 3.591231(3.514005) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4031 | Time 1.1892(1.2008) | Loss 3.530555(3.515164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4032 | Time 1.1997(1.2007) | Loss 3.515994(3.515222) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4033 | Time 1.1981(1.2005) | Loss 3.502771(3.514350) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4034 | Time 1.2020(1.2006) | Loss 3.586954(3.519432) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4035 | Time 1.1901(1.1999) | Loss 3.528963(3.520100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4036 | Time 1.2495(1.2034) | Loss 3.597049(3.525486) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4037 | Time 1.2079(1.2037) | Loss 3.529313(3.525754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4038 | Time 1.1989(1.2033) | Loss 3.536900(3.526534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4039 | Time 1.1953(1.2028) | Loss 3.658935(3.535802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4040 | Time 1.1961(1.2023) | Loss 3.519825(3.534684) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4041 | Time 1.1911(1.2015) | Loss 3.542177(3.535208) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4042 | Time 1.1870(1.2005) | Loss 3.658452(3.543835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4043 | Time 1.1834(1.1993) | Loss 3.541467(3.543670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4044 | Time 1.1952(1.1990) | Loss 3.617456(3.548835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4045 | Time 1.1974(1.1989) | Loss 3.539899(3.548209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4046 | Time 1.1924(1.1985) | Loss 3.602614(3.552017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4047 | Time 1.1892(1.1978) | Loss 3.466416(3.546025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4048 | Time 1.2032(1.1982) | Loss 3.551086(3.546380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4049 | Time 1.1998(1.1983) | Loss 3.614649(3.551158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4050 | Time 1.1974(1.1982) | Loss 3.545553(3.550766) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4051 | Time 1.2020(1.1985) | Loss 3.560460(3.551445) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4052 | Time 1.1968(1.1984) | Loss 3.558019(3.551905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4053 | Time 1.2054(1.1989) | Loss 3.474468(3.546484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4054 | Time 1.2829(1.2048) | Loss 3.497149(3.543031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4055 | Time 1.1913(1.2038) | Loss 3.412771(3.533913) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4056 | Time 1.2024(1.2037) | Loss 3.480183(3.530152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4057 | Time 1.2381(1.2061) | Loss 3.414649(3.522066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4058 | Time 1.2151(1.2067) | Loss 3.403264(3.513750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4059 | Time 1.2063(1.2067) | Loss 3.457761(3.509831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4060 | Time 1.2039(1.2065) | Loss 3.451544(3.505751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4061 | Time 1.2182(1.2073) | Loss 3.414883(3.499390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4062 | Time 1.2106(1.2076) | Loss 3.422399(3.494001) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4063 | Time 1.1963(1.2068) | Loss 3.370723(3.485371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4064 | Time 1.2088(1.2069) | Loss 3.479554(3.484964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4065 | Time 1.2137(1.2074) | Loss 3.420741(3.480468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4066 | Time 1.2102(1.2076) | Loss 3.449619(3.478309) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4067 | Time 1.2005(1.2071) | Loss 3.544783(3.482962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4068 | Time 1.1989(1.2065) | Loss 3.426405(3.479003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4069 | Time 1.2126(1.2070) | Loss 3.515018(3.481524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4070 | Time 1.1960(1.2062) | Loss 3.444256(3.478915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4071 | Time 1.2033(1.2060) | Loss 3.522836(3.481990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4072 | Time 1.2178(1.2068) | Loss 3.535892(3.485763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4073 | Time 1.1958(1.2060) | Loss 3.502913(3.486964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4074 | Time 1.1973(1.2054) | Loss 3.606129(3.495305) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4075 | Time 1.2087(1.2057) | Loss 3.572261(3.500692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4076 | Time 1.2335(1.2076) | Loss 3.592287(3.507104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4077 | Time 1.2040(1.2073) | Loss 3.501103(3.506684) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4078 | Time 1.2094(1.2075) | Loss 3.570388(3.511143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4079 | Time 1.1975(1.2068) | Loss 3.566280(3.515003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4080 | Time 1.2003(1.2063) | Loss 3.571603(3.518965) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4081 | Time 1.1987(1.2058) | Loss 3.510611(3.518380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4082 | Time 1.2508(1.2090) | Loss 3.503370(3.517329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4083 | Time 1.2117(1.2091) | Loss 3.496801(3.515892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4084 | Time 1.2105(1.2092) | Loss 3.584083(3.520665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4085 | Time 1.2066(1.2091) | Loss 3.499295(3.519170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4086 | Time 1.2254(1.2102) | Loss 3.479511(3.516394) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4087 | Time 1.2192(1.2108) | Loss 3.549552(3.518715) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4088 | Time 1.2232(1.2117) | Loss 3.472929(3.515510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4089 | Time 1.2104(1.2116) | Loss 3.538549(3.517122) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4090 | Time 1.2198(1.2122) | Loss 3.523489(3.517568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4091 | Time 1.2173(1.2125) | Loss 3.472568(3.514418) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4092 | Time 1.1901(1.2110) | Loss 3.544261(3.516507) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4093 | Time 1.1938(1.2098) | Loss 3.452921(3.512056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4094 | Time 1.1992(1.2090) | Loss 3.510926(3.511977) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4095 | Time 1.2045(1.2087) | Loss 3.447335(3.507452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4096 | Time 1.2010(1.2082) | Loss 3.470391(3.504858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4097 | Time 1.1976(1.2074) | Loss 3.450229(3.501034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4098 | Time 1.1999(1.2069) | Loss 3.437984(3.496620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4099 | Time 1.2060(1.2068) | Loss 3.438004(3.492517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4100 | Time 1.2116(1.2072) | Loss 3.513690(3.493999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4100 | Test Loss 3.521932 | NFE 20
Skipping vis as data dimension is >2
Iter 4101 | Time 1.1865(1.2057) | Loss 3.595928(3.501134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4102 | Time 1.1850(1.2043) | Loss 3.626714(3.509925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4103 | Time 1.2083(1.2046) | Loss 3.606503(3.516685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4104 | Time 1.1908(1.2036) | Loss 3.548523(3.518914) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4105 | Time 1.2088(1.2040) | Loss 3.486794(3.516666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4106 | Time 1.2073(1.2042) | Loss 3.517378(3.516715) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4107 | Time 1.2035(1.2041) | Loss 3.523566(3.517195) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4108 | Time 1.2190(1.2052) | Loss 3.519702(3.517370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4109 | Time 1.2136(1.2058) | Loss 3.578499(3.521649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4110 | Time 1.2064(1.2058) | Loss 3.583158(3.525955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4111 | Time 1.2044(1.2057) | Loss 3.510179(3.524851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4112 | Time 1.1808(1.2040) | Loss 3.537760(3.525754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4113 | Time 1.1895(1.2030) | Loss 3.528686(3.525960) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4114 | Time 1.1970(1.2025) | Loss 3.605908(3.531556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4115 | Time 1.2039(1.2026) | Loss 3.554162(3.533138) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4116 | Time 1.1986(1.2024) | Loss 3.588832(3.537037) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4117 | Time 1.2065(1.2026) | Loss 3.553729(3.538205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4118 | Time 1.2038(1.2027) | Loss 3.407482(3.529055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4119 | Time 1.1856(1.2015) | Loss 3.479304(3.525572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4120 | Time 1.1891(1.2007) | Loss 3.455977(3.520701) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4121 | Time 1.2552(1.2045) | Loss 3.458774(3.516366) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4122 | Time 1.2066(1.2046) | Loss 3.463921(3.512695) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4123 | Time 1.1873(1.2034) | Loss 3.493479(3.511349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4124 | Time 1.1835(1.2020) | Loss 3.477265(3.508964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4125 | Time 1.1782(1.2004) | Loss 3.466344(3.505980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4126 | Time 1.1662(1.1980) | Loss 3.437208(3.501166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4127 | Time 1.1831(1.1969) | Loss 3.499407(3.501043) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4128 | Time 1.1794(1.1957) | Loss 3.555836(3.504879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4129 | Time 1.1726(1.1941) | Loss 3.448503(3.500932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4130 | Time 1.1776(1.1929) | Loss 3.484289(3.499767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4131 | Time 1.1988(1.1933) | Loss 3.430800(3.494939) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4132 | Time 1.1824(1.1926) | Loss 3.374959(3.486541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4133 | Time 1.2014(1.1932) | Loss 3.446059(3.483707) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4134 | Time 1.2001(1.1937) | Loss 3.461617(3.482161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4135 | Time 1.1679(1.1919) | Loss 3.448226(3.479785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4136 | Time 1.2218(1.1940) | Loss 3.514229(3.482196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4137 | Time 1.1944(1.1940) | Loss 3.553014(3.487154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4138 | Time 1.2026(1.1946) | Loss 3.514586(3.489074) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4139 | Time 1.1941(1.1946) | Loss 3.631582(3.499049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4140 | Time 1.1861(1.1940) | Loss 3.580839(3.504775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4141 | Time 1.1692(1.1922) | Loss 3.535354(3.506915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4142 | Time 1.1881(1.1919) | Loss 3.616189(3.514564) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4143 | Time 1.1920(1.1919) | Loss 3.558546(3.517643) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4144 | Time 1.1835(1.1914) | Loss 3.604650(3.523734) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4145 | Time 1.1879(1.1911) | Loss 3.537505(3.524698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4146 | Time 1.1949(1.1914) | Loss 3.596301(3.529710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4147 | Time 1.1873(1.1911) | Loss 3.584456(3.533542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4148 | Time 1.1700(1.1896) | Loss 3.514559(3.532213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4149 | Time 1.1924(1.1898) | Loss 3.520534(3.531396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4150 | Time 1.1816(1.1892) | Loss 3.585593(3.535190) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4151 | Time 1.1866(1.1891) | Loss 3.611443(3.540527) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4152 | Time 1.1956(1.1895) | Loss 3.661159(3.548972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4153 | Time 1.2120(1.1911) | Loss 3.582506(3.551319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4154 | Time 1.1795(1.1903) | Loss 3.610092(3.555433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4155 | Time 1.1790(1.1895) | Loss 3.551240(3.555140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4156 | Time 1.2160(1.1913) | Loss 3.671957(3.563317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4157 | Time 1.1746(1.1902) | Loss 3.559903(3.563078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4158 | Time 1.1658(1.1885) | Loss 3.621454(3.567164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4159 | Time 1.1727(1.1874) | Loss 3.561097(3.566739) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4160 | Time 1.1824(1.1870) | Loss 3.596798(3.568843) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4161 | Time 1.1872(1.1870) | Loss 3.520847(3.565484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4162 | Time 1.2236(1.1896) | Loss 3.463675(3.558357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4163 | Time 1.2040(1.1906) | Loss 3.434938(3.549718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4164 | Time 1.2578(1.1953) | Loss 3.434589(3.541659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4165 | Time 1.2202(1.1970) | Loss 3.508262(3.539321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4166 | Time 1.2036(1.1975) | Loss 3.502003(3.536709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4167 | Time 1.2142(1.1987) | Loss 3.434303(3.529540) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4168 | Time 1.2206(1.2002) | Loss 3.519053(3.528806) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4169 | Time 1.2118(1.2010) | Loss 3.446616(3.523053) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4170 | Time 1.2474(1.2043) | Loss 3.450131(3.517948) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4171 | Time 1.2244(1.2057) | Loss 3.520272(3.518111) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4172 | Time 1.2107(1.2060) | Loss 3.538307(3.519525) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4173 | Time 1.2110(1.2064) | Loss 3.565340(3.522732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4174 | Time 1.1960(1.2057) | Loss 3.542729(3.524132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4175 | Time 1.1773(1.2037) | Loss 3.529413(3.524501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4176 | Time 1.1786(1.2019) | Loss 3.578830(3.528304) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4177 | Time 1.1762(1.2001) | Loss 3.482240(3.525080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4178 | Time 1.1880(1.1993) | Loss 3.499047(3.523257) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4179 | Time 1.1855(1.1983) | Loss 3.561864(3.525960) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4180 | Time 1.1637(1.1959) | Loss 3.549148(3.527583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4181 | Time 1.1988(1.1961) | Loss 3.453862(3.522423) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4182 | Time 1.1867(1.1954) | Loss 3.491941(3.520289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4183 | Time 1.1944(1.1954) | Loss 3.502297(3.519029) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4184 | Time 1.1866(1.1947) | Loss 3.494421(3.517307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4185 | Time 1.1872(1.1942) | Loss 3.525459(3.517877) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4186 | Time 1.1808(1.1933) | Loss 3.452472(3.513299) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4187 | Time 1.1780(1.1922) | Loss 3.534336(3.514772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4188 | Time 1.1777(1.1912) | Loss 3.522942(3.515344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4189 | Time 1.1928(1.1913) | Loss 3.535511(3.516755) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4190 | Time 1.2012(1.1920) | Loss 3.506815(3.516059) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4191 | Time 1.2052(1.1929) | Loss 3.491386(3.514332) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4192 | Time 1.2177(1.1947) | Loss 3.485126(3.512288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4193 | Time 1.1955(1.1947) | Loss 3.446977(3.507716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4194 | Time 1.2082(1.1957) | Loss 3.491526(3.506583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4195 | Time 1.2019(1.1961) | Loss 3.467399(3.503840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4196 | Time 1.2043(1.1967) | Loss 3.650651(3.514117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4197 | Time 1.2107(1.1977) | Loss 3.481480(3.511832) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4198 | Time 1.2262(1.1997) | Loss 3.466419(3.508653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4199 | Time 1.2806(1.2053) | Loss 3.560598(3.512289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4200 | Time 1.2835(1.2108) | Loss 3.428105(3.506396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4200 | Test Loss 3.414358 | NFE 20
Skipping vis as data dimension is >2
Iter 4201 | Time 1.2629(1.2144) | Loss 3.527374(3.507865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4202 | Time 1.2456(1.2166) | Loss 3.402592(3.500496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4203 | Time 1.2161(1.2166) | Loss 3.462427(3.497831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4204 | Time 1.1794(1.2140) | Loss 3.461585(3.495294) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4205 | Time 1.1890(1.2122) | Loss 3.494597(3.495245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4206 | Time 1.2450(1.2145) | Loss 3.523730(3.497239) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4207 | Time 1.1973(1.2133) | Loss 3.476944(3.495818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4208 | Time 1.1929(1.2119) | Loss 3.472489(3.494185) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4209 | Time 1.2559(1.2150) | Loss 3.492969(3.494100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4210 | Time 1.1959(1.2136) | Loss 3.509663(3.495190) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4211 | Time 1.2002(1.2127) | Loss 3.481006(3.494197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4212 | Time 1.1917(1.2112) | Loss 3.502866(3.494804) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4213 | Time 1.1919(1.2099) | Loss 3.551044(3.498740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4214 | Time 1.1923(1.2086) | Loss 3.522196(3.500382) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4215 | Time 1.1833(1.2069) | Loss 3.514884(3.501397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4216 | Time 1.1867(1.2055) | Loss 3.471162(3.499281) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4217 | Time 1.1954(1.2048) | Loss 3.509764(3.500015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4218 | Time 1.2179(1.2057) | Loss 3.601070(3.507089) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4219 | Time 1.2076(1.2058) | Loss 3.583942(3.512468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4220 | Time 1.2190(1.2067) | Loss 3.540194(3.514409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4221 | Time 1.2078(1.2068) | Loss 3.429751(3.508483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4222 | Time 1.2000(1.2063) | Loss 3.455345(3.504764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4223 | Time 1.2024(1.2061) | Loss 3.544758(3.507563) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4224 | Time 1.2165(1.2068) | Loss 3.483889(3.505906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4225 | Time 1.1951(1.2060) | Loss 3.475191(3.503756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4226 | Time 1.1936(1.2051) | Loss 3.579382(3.509050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4227 | Time 1.1852(1.2037) | Loss 3.563395(3.512854) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4228 | Time 1.1700(1.2014) | Loss 3.511841(3.512783) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4229 | Time 1.1755(1.1995) | Loss 3.503227(3.512114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4230 | Time 1.2065(1.2000) | Loss 3.498888(3.511188) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4231 | Time 1.2037(1.2003) | Loss 3.526761(3.512278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4232 | Time 1.1840(1.1991) | Loss 3.521067(3.512893) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4233 | Time 1.2013(1.1993) | Loss 3.556570(3.515951) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4234 | Time 1.1943(1.1990) | Loss 3.564490(3.519349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4235 | Time 1.2008(1.1991) | Loss 3.558913(3.522118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4236 | Time 1.2217(1.2007) | Loss 3.553572(3.524320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4237 | Time 1.2588(1.2047) | Loss 3.576751(3.527990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4238 | Time 1.1996(1.2044) | Loss 3.630383(3.535158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4239 | Time 1.1914(1.2035) | Loss 3.546637(3.535961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4240 | Time 1.1905(1.2026) | Loss 3.608232(3.541020) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4241 | Time 1.1951(1.2020) | Loss 3.647979(3.548507) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4242 | Time 1.1993(1.2018) | Loss 3.582236(3.550868) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4243 | Time 1.1856(1.2007) | Loss 3.671998(3.559347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4244 | Time 1.1871(1.1998) | Loss 3.532253(3.557451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4245 | Time 1.2268(1.2017) | Loss 3.517259(3.554637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4246 | Time 1.1999(1.2015) | Loss 3.499559(3.550782) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4247 | Time 1.1965(1.2012) | Loss 3.448874(3.543648) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4248 | Time 1.1993(1.2010) | Loss 3.562115(3.544941) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4249 | Time 1.1976(1.2008) | Loss 3.466388(3.539442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4250 | Time 1.1962(1.2005) | Loss 3.420043(3.531084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4251 | Time 1.1976(1.2003) | Loss 3.513940(3.529884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4252 | Time 1.2080(1.2008) | Loss 3.448941(3.524218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4253 | Time 1.2092(1.2014) | Loss 3.401886(3.515655) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4254 | Time 1.2017(1.2014) | Loss 3.487397(3.513677) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4255 | Time 1.2065(1.2018) | Loss 3.543010(3.515730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4256 | Time 1.1923(1.2011) | Loss 3.450687(3.511177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4257 | Time 1.1992(1.2010) | Loss 3.528187(3.512368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4258 | Time 1.2012(1.2010) | Loss 3.462878(3.508904) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4259 | Time 1.2032(1.2012) | Loss 3.426609(3.503143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4260 | Time 1.1928(1.2006) | Loss 3.451557(3.499532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4261 | Time 1.1996(1.2005) | Loss 3.421956(3.494102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4262 | Time 1.1879(1.1996) | Loss 3.488297(3.493695) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4263 | Time 1.2081(1.2002) | Loss 3.428925(3.489161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4264 | Time 1.2089(1.2008) | Loss 3.488766(3.489134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4265 | Time 1.2223(1.2023) | Loss 3.408499(3.483489) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4266 | Time 1.2027(1.2024) | Loss 3.435311(3.480117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4267 | Time 1.1962(1.2019) | Loss 3.303372(3.467745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4268 | Time 1.1967(1.2016) | Loss 3.384710(3.461932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4269 | Time 1.2098(1.2021) | Loss 3.421699(3.459116) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4270 | Time 1.2258(1.2038) | Loss 3.367610(3.452710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4271 | Time 1.1938(1.2031) | Loss 3.345600(3.445213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4272 | Time 1.2098(1.2036) | Loss 3.467185(3.446751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4273 | Time 1.2351(1.2058) | Loss 3.437216(3.446083) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4274 | Time 1.2149(1.2064) | Loss 3.328628(3.437861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4275 | Time 1.2261(1.2078) | Loss 3.431038(3.437384) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4276 | Time 1.2125(1.2081) | Loss 3.373358(3.432902) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4277 | Time 1.1942(1.2071) | Loss 3.416496(3.431754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4278 | Time 1.1860(1.2057) | Loss 3.462013(3.433872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4279 | Time 1.1890(1.2045) | Loss 3.428441(3.433492) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4280 | Time 1.2072(1.2047) | Loss 3.483335(3.436981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4281 | Time 1.2388(1.2071) | Loss 3.429661(3.436468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4282 | Time 1.2581(1.2106) | Loss 3.459031(3.438048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4283 | Time 1.2137(1.2109) | Loss 3.484765(3.441318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4284 | Time 1.2031(1.2103) | Loss 3.466643(3.443091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4285 | Time 1.1987(1.2095) | Loss 3.471619(3.445088) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4286 | Time 1.1988(1.2088) | Loss 3.413369(3.442867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4287 | Time 1.1959(1.2078) | Loss 3.472777(3.444961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4288 | Time 1.1988(1.2072) | Loss 3.469492(3.446678) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4289 | Time 1.2173(1.2079) | Loss 3.517797(3.451656) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4290 | Time 1.1998(1.2074) | Loss 3.604778(3.462375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4291 | Time 1.1874(1.2060) | Loss 3.460764(3.462262) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4292 | Time 1.2045(1.2059) | Loss 3.478908(3.463427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4293 | Time 1.1939(1.2050) | Loss 3.518560(3.467287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4294 | Time 1.2004(1.2047) | Loss 3.544937(3.472722) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4295 | Time 1.1991(1.2043) | Loss 3.455680(3.471529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4296 | Time 1.1851(1.2030) | Loss 3.483121(3.472341) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4297 | Time 1.2031(1.2030) | Loss 3.479191(3.472820) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4298 | Time 1.1827(1.2016) | Loss 3.541588(3.477634) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4299 | Time 1.1978(1.2013) | Loss 3.416796(3.473375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4300 | Time 1.2145(1.2022) | Loss 3.461570(3.472549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4300 | Test Loss 3.454342 | NFE 20
Skipping vis as data dimension is >2
Iter 4301 | Time 1.2120(1.2029) | Loss 3.437659(3.470107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4302 | Time 1.2055(1.2031) | Loss 3.388493(3.464394) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4303 | Time 1.2063(1.2033) | Loss 3.452581(3.463567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4304 | Time 1.1956(1.2028) | Loss 3.448808(3.462534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4305 | Time 1.1973(1.2024) | Loss 3.348257(3.454534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4306 | Time 1.1962(1.2019) | Loss 3.449131(3.454156) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4307 | Time 1.2051(1.2022) | Loss 3.441553(3.453274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4308 | Time 1.1886(1.2012) | Loss 3.419950(3.450941) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4309 | Time 1.1857(1.2001) | Loss 3.506134(3.454805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4310 | Time 1.1977(1.2000) | Loss 3.547476(3.461292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4311 | Time 1.1950(1.1996) | Loss 3.465203(3.461565) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4312 | Time 1.1954(1.1993) | Loss 3.526596(3.466118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4313 | Time 1.1827(1.1982) | Loss 3.437535(3.464117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4314 | Time 1.1904(1.1976) | Loss 3.472293(3.464689) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4315 | Time 1.2035(1.1980) | Loss 3.370762(3.458114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4316 | Time 1.2595(1.2023) | Loss 3.433346(3.456380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4317 | Time 1.2738(1.2073) | Loss 3.524427(3.461144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4318 | Time 1.2111(1.2076) | Loss 3.363518(3.454310) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4319 | Time 1.2134(1.2080) | Loss 3.436790(3.453084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4320 | Time 1.1885(1.2066) | Loss 3.449866(3.452858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4321 | Time 1.1975(1.2060) | Loss 3.446657(3.452424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4322 | Time 1.1978(1.2054) | Loss 3.397346(3.448569) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4323 | Time 1.2142(1.2060) | Loss 3.413066(3.446084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4324 | Time 1.1811(1.2043) | Loss 3.501025(3.449929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4325 | Time 1.1914(1.2034) | Loss 3.313221(3.440360) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4326 | Time 1.1943(1.2028) | Loss 3.467593(3.442266) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4327 | Time 1.2419(1.2055) | Loss 3.369362(3.437163) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4328 | Time 1.2156(1.2062) | Loss 3.385165(3.433523) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4329 | Time 1.2095(1.2064) | Loss 3.493245(3.437704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4330 | Time 1.1992(1.2059) | Loss 3.516891(3.443247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4331 | Time 1.2018(1.2056) | Loss 3.484906(3.446163) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4332 | Time 1.1853(1.2042) | Loss 3.378662(3.441438) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4333 | Time 1.2132(1.2048) | Loss 3.530259(3.447655) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4334 | Time 1.1987(1.2044) | Loss 3.470021(3.449221) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4335 | Time 1.1840(1.2030) | Loss 3.466988(3.450465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4336 | Time 1.1805(1.2014) | Loss 3.504907(3.454276) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4337 | Time 1.1838(1.2002) | Loss 3.436510(3.453032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4338 | Time 1.2048(1.2005) | Loss 3.478147(3.454790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4339 | Time 1.1856(1.1995) | Loss 3.485217(3.456920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4340 | Time 1.1979(1.1993) | Loss 3.409589(3.453607) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4341 | Time 1.2083(1.2000) | Loss 3.446874(3.453135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4342 | Time 1.1963(1.1997) | Loss 3.508077(3.456981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4343 | Time 1.1884(1.1989) | Loss 3.424592(3.454714) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4344 | Time 1.1838(1.1979) | Loss 3.440289(3.453704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4345 | Time 1.2477(1.2013) | Loss 3.452962(3.453652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4346 | Time 1.2427(1.2042) | Loss 3.441228(3.452783) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4347 | Time 1.1843(1.2028) | Loss 3.453547(3.452836) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4348 | Time 1.1845(1.2016) | Loss 3.483108(3.454955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4349 | Time 1.1864(1.2005) | Loss 3.439180(3.453851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4350 | Time 1.1815(1.1992) | Loss 3.498150(3.456952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4351 | Time 1.1988(1.1991) | Loss 3.461774(3.457289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4352 | Time 1.2062(1.1996) | Loss 3.369336(3.451133) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4353 | Time 1.1861(1.1987) | Loss 3.427347(3.449468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4354 | Time 1.2076(1.1993) | Loss 3.450753(3.449558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4355 | Time 1.2076(1.1999) | Loss 3.488910(3.452312) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4356 | Time 1.2042(1.2002) | Loss 3.462012(3.452991) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4357 | Time 1.2098(1.2009) | Loss 3.508411(3.456871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4358 | Time 1.2052(1.2012) | Loss 3.522048(3.461433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4359 | Time 1.1933(1.2006) | Loss 3.461754(3.461455) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4360 | Time 1.1954(1.2003) | Loss 3.550420(3.467683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4361 | Time 1.1902(1.1996) | Loss 3.529885(3.472037) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4362 | Time 1.1870(1.1987) | Loss 3.570004(3.478895) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4363 | Time 1.1889(1.1980) | Loss 3.541982(3.483311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4364 | Time 1.1969(1.1979) | Loss 3.549134(3.487919) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4365 | Time 1.1854(1.1970) | Loss 3.599903(3.495758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4366 | Time 1.1937(1.1968) | Loss 3.641155(3.505935) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4367 | Time 1.1981(1.1969) | Loss 3.553886(3.509292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4368 | Time 1.1906(1.1965) | Loss 3.554569(3.512461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4369 | Time 1.2062(1.1971) | Loss 3.551535(3.515196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4370 | Time 1.1965(1.1971) | Loss 3.565279(3.518702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4371 | Time 1.1886(1.1965) | Loss 3.538062(3.520057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4372 | Time 1.1715(1.1947) | Loss 3.536623(3.521217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4373 | Time 1.1778(1.1936) | Loss 3.474169(3.517924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4374 | Time 1.1602(1.1912) | Loss 3.495907(3.516383) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4375 | Time 1.1728(1.1899) | Loss 3.426254(3.510074) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4376 | Time 1.1763(1.1890) | Loss 3.541590(3.512280) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4377 | Time 1.1498(1.1862) | Loss 3.466928(3.509105) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4378 | Time 1.1614(1.1845) | Loss 3.550248(3.511985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4379 | Time 1.1704(1.1835) | Loss 3.504412(3.511455) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4380 | Time 1.1593(1.1818) | Loss 3.490158(3.509964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4381 | Time 1.1688(1.1809) | Loss 3.469448(3.507128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4382 | Time 1.1964(1.1820) | Loss 3.464164(3.504121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4383 | Time 1.1677(1.1810) | Loss 3.530772(3.505986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4384 | Time 1.1740(1.1805) | Loss 3.408777(3.499182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4385 | Time 1.1688(1.1797) | Loss 3.533441(3.501580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4386 | Time 1.1697(1.1790) | Loss 3.449886(3.497961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4387 | Time 1.1799(1.1790) | Loss 3.418479(3.492397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4388 | Time 1.1669(1.1782) | Loss 3.499700(3.492909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4389 | Time 1.1717(1.1777) | Loss 3.467722(3.491145) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4390 | Time 1.1725(1.1774) | Loss 3.506248(3.492203) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4391 | Time 1.1556(1.1758) | Loss 3.507805(3.493295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4392 | Time 1.1671(1.1752) | Loss 3.472270(3.491823) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4393 | Time 1.1578(1.1740) | Loss 3.528221(3.494371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4394 | Time 1.1638(1.1733) | Loss 3.533816(3.497132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4395 | Time 1.1675(1.1729) | Loss 3.611222(3.505118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4396 | Time 1.1711(1.1728) | Loss 3.569657(3.509636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4397 | Time 1.1655(1.1723) | Loss 3.502065(3.509106) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4398 | Time 1.1672(1.1719) | Loss 3.529164(3.510510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4399 | Time 1.1950(1.1735) | Loss 3.546814(3.513051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4400 | Time 1.1634(1.1728) | Loss 3.497886(3.511990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4400 | Test Loss 3.530235 | NFE 20
Skipping vis as data dimension is >2
Iter 4401 | Time 1.1727(1.1728) | Loss 3.424655(3.505876) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4402 | Time 1.1714(1.1727) | Loss 3.424502(3.500180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4403 | Time 1.1699(1.1725) | Loss 3.476654(3.498533) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4404 | Time 1.1657(1.1720) | Loss 3.517345(3.499850) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4405 | Time 1.1863(1.1730) | Loss 3.432761(3.495154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4406 | Time 1.1813(1.1736) | Loss 3.448441(3.491884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4407 | Time 1.1802(1.1741) | Loss 3.421672(3.486969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4408 | Time 1.1719(1.1739) | Loss 3.476299(3.486222) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4409 | Time 1.1997(1.1757) | Loss 3.433564(3.482536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4410 | Time 1.1942(1.1770) | Loss 3.413301(3.477690) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4411 | Time 1.1948(1.1783) | Loss 3.470459(3.477184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4412 | Time 1.1866(1.1789) | Loss 3.449358(3.475236) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4413 | Time 1.1947(1.1800) | Loss 3.429724(3.472050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4414 | Time 1.1810(1.1800) | Loss 3.429849(3.469096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4415 | Time 1.1913(1.1808) | Loss 3.496102(3.470986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4416 | Time 1.1983(1.1820) | Loss 3.565137(3.477577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4417 | Time 1.1749(1.1815) | Loss 3.468751(3.476959) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4418 | Time 1.1952(1.1825) | Loss 3.422571(3.473152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4419 | Time 1.2940(1.1903) | Loss 3.466422(3.472681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4420 | Time 1.2270(1.1929) | Loss 3.521643(3.476108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4421 | Time 1.1984(1.1933) | Loss 3.372531(3.468858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4422 | Time 1.2562(1.1977) | Loss 3.483589(3.469889) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4423 | Time 1.2661(1.2025) | Loss 3.504848(3.472336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4424 | Time 1.2075(1.2028) | Loss 3.536531(3.476830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4425 | Time 1.1966(1.2024) | Loss 3.409609(3.472124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4426 | Time 1.2166(1.2034) | Loss 3.446606(3.470338) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4427 | Time 1.1754(1.2014) | Loss 3.387946(3.464571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4428 | Time 1.1724(1.1994) | Loss 3.443255(3.463079) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4429 | Time 1.2630(1.2038) | Loss 3.403594(3.458915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4430 | Time 1.2574(1.2076) | Loss 3.474811(3.460027) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4431 | Time 1.2570(1.2111) | Loss 3.351004(3.452396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4432 | Time 1.2417(1.2132) | Loss 3.461652(3.453044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4433 | Time 1.2630(1.2167) | Loss 3.431996(3.451570) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4434 | Time 1.2322(1.2178) | Loss 3.488096(3.454127) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4435 | Time 1.2518(1.2202) | Loss 3.483625(3.456192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4436 | Time 1.2691(1.2236) | Loss 3.450759(3.455812) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4437 | Time 1.3352(1.2314) | Loss 3.402184(3.452058) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4438 | Time 1.1997(1.2292) | Loss 3.433316(3.450746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4439 | Time 1.1921(1.2266) | Loss 3.452773(3.450888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4440 | Time 1.1854(1.2237) | Loss 3.451100(3.450903) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4441 | Time 1.1864(1.2211) | Loss 3.411147(3.448120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4442 | Time 1.1745(1.2178) | Loss 3.487644(3.450886) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4443 | Time 1.1706(1.2145) | Loss 3.377815(3.445771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4444 | Time 1.1944(1.2131) | Loss 3.459172(3.446709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4445 | Time 1.1705(1.2101) | Loss 3.404057(3.443724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4446 | Time 1.1976(1.2093) | Loss 3.419869(3.442054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4447 | Time 1.1658(1.2062) | Loss 3.376288(3.437450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4448 | Time 1.1715(1.2038) | Loss 3.464257(3.439327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4449 | Time 1.1581(1.2006) | Loss 3.343992(3.432653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4450 | Time 1.1650(1.1981) | Loss 3.401253(3.430455) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4451 | Time 1.1861(1.1973) | Loss 3.328912(3.423347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4452 | Time 1.1624(1.1948) | Loss 3.388985(3.420942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4453 | Time 1.1780(1.1936) | Loss 3.432199(3.421730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4454 | Time 1.1675(1.1918) | Loss 3.398316(3.420091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4455 | Time 1.1754(1.1907) | Loss 3.403891(3.418957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4456 | Time 1.1768(1.1897) | Loss 3.401432(3.417730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4457 | Time 1.1760(1.1887) | Loss 3.515542(3.424577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4458 | Time 1.2364(1.1921) | Loss 3.365457(3.420439) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4459 | Time 1.1788(1.1911) | Loss 3.350542(3.415546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4460 | Time 1.1717(1.1898) | Loss 3.403613(3.414711) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4461 | Time 1.1856(1.1895) | Loss 3.384369(3.412587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4462 | Time 1.1849(1.1892) | Loss 3.365934(3.409321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4463 | Time 1.1879(1.1891) | Loss 3.400200(3.408683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4464 | Time 1.2081(1.1904) | Loss 3.436540(3.410633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4465 | Time 1.1865(1.1901) | Loss 3.406083(3.410314) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4466 | Time 1.1915(1.1902) | Loss 3.398895(3.409515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4467 | Time 1.1794(1.1895) | Loss 3.353445(3.405590) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4468 | Time 1.1999(1.1902) | Loss 3.337499(3.400824) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4469 | Time 1.1992(1.1908) | Loss 3.455992(3.404685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4470 | Time 1.1938(1.1910) | Loss 3.375183(3.402620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4471 | Time 1.1978(1.1915) | Loss 3.431899(3.404670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4472 | Time 1.1791(1.1906) | Loss 3.414200(3.405337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4473 | Time 1.1752(1.1896) | Loss 3.415012(3.406014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4474 | Time 1.2404(1.1931) | Loss 3.416387(3.406740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4475 | Time 1.2233(1.1952) | Loss 3.371283(3.404258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4476 | Time 1.2320(1.1978) | Loss 3.462185(3.408313) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4477 | Time 1.2291(1.2000) | Loss 3.432968(3.410039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4478 | Time 1.2290(1.2020) | Loss 3.444714(3.412466) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4479 | Time 1.2506(1.2054) | Loss 3.365826(3.409201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4480 | Time 1.2307(1.2072) | Loss 3.453307(3.412289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4481 | Time 1.2491(1.2101) | Loss 3.459980(3.415627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4482 | Time 1.2551(1.2133) | Loss 3.376954(3.412920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4483 | Time 1.2249(1.2141) | Loss 3.402701(3.412205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4484 | Time 1.1913(1.2125) | Loss 3.472274(3.416410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4485 | Time 1.1892(1.2109) | Loss 3.446620(3.418524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4486 | Time 1.1868(1.2092) | Loss 3.480059(3.422832) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4487 | Time 1.1768(1.2069) | Loss 3.405392(3.421611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4488 | Time 1.1757(1.2047) | Loss 3.404532(3.420415) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4489 | Time 1.1745(1.2026) | Loss 3.411010(3.419757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4490 | Time 1.2078(1.2030) | Loss 3.511263(3.426162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4491 | Time 1.1903(1.2021) | Loss 3.387064(3.423426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4492 | Time 1.2300(1.2040) | Loss 3.510007(3.429486) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4493 | Time 1.1823(1.2025) | Loss 3.463295(3.431853) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4494 | Time 1.1917(1.2018) | Loss 3.509669(3.437300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4495 | Time 1.1975(1.2015) | Loss 3.475835(3.439997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4496 | Time 1.1823(1.2001) | Loss 3.470882(3.442159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4497 | Time 1.1828(1.1989) | Loss 3.556978(3.450197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4498 | Time 1.1880(1.1981) | Loss 3.475533(3.451970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4499 | Time 1.1941(1.1979) | Loss 3.465895(3.452945) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4500 | Time 1.1987(1.1979) | Loss 3.502425(3.456409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4500 | Test Loss 3.464036 | NFE 20
Skipping vis as data dimension is >2
Iter 4501 | Time 1.1879(1.1972) | Loss 3.378981(3.450989) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4502 | Time 1.1959(1.1971) | Loss 3.371541(3.445427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4503 | Time 1.1728(1.1954) | Loss 3.407688(3.442786) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4504 | Time 1.1749(1.1940) | Loss 3.409297(3.440441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4505 | Time 1.1737(1.1926) | Loss 3.311460(3.431413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4506 | Time 1.1818(1.1918) | Loss 3.394010(3.428794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4507 | Time 1.1842(1.1913) | Loss 3.362231(3.424135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4508 | Time 1.1864(1.1909) | Loss 3.467998(3.427205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4509 | Time 1.1811(1.1903) | Loss 3.457942(3.429357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4510 | Time 1.1855(1.1899) | Loss 3.413532(3.428249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4511 | Time 1.1875(1.1898) | Loss 3.433514(3.428618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4512 | Time 1.1950(1.1901) | Loss 3.400234(3.426631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4513 | Time 1.1692(1.1887) | Loss 3.456671(3.428734) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4514 | Time 1.1943(1.1891) | Loss 3.392161(3.426174) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4515 | Time 1.1888(1.1890) | Loss 3.424881(3.426083) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4516 | Time 1.1787(1.1883) | Loss 3.450590(3.427799) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4517 | Time 1.1835(1.1880) | Loss 3.451535(3.429460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4518 | Time 1.2100(1.1895) | Loss 3.528169(3.436370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4519 | Time 1.2417(1.1932) | Loss 3.485666(3.439821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4520 | Time 1.1875(1.1928) | Loss 3.537833(3.446681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4521 | Time 1.2106(1.1940) | Loss 3.484703(3.449343) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4522 | Time 1.1893(1.1937) | Loss 3.575693(3.458187) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4523 | Time 1.1785(1.1926) | Loss 3.542990(3.464124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4524 | Time 1.2018(1.1933) | Loss 3.509407(3.467293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4525 | Time 1.1946(1.1934) | Loss 3.431850(3.464812) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4526 | Time 1.1996(1.1938) | Loss 3.351926(3.456910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4527 | Time 1.1956(1.1939) | Loss 3.422266(3.454485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4528 | Time 1.1862(1.1934) | Loss 3.334868(3.446112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4529 | Time 1.2117(1.1947) | Loss 3.353067(3.439599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4530 | Time 1.1879(1.1942) | Loss 3.431905(3.439060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4531 | Time 1.1853(1.1936) | Loss 3.358141(3.433396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4532 | Time 1.1801(1.1926) | Loss 3.396891(3.430841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4533 | Time 1.1723(1.1912) | Loss 3.291059(3.421056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4534 | Time 1.1937(1.1914) | Loss 3.351252(3.416170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4535 | Time 1.2006(1.1920) | Loss 3.355434(3.411918) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4536 | Time 1.1967(1.1924) | Loss 3.307184(3.404587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4537 | Time 1.1871(1.1920) | Loss 3.350324(3.400788) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4538 | Time 1.1837(1.1914) | Loss 3.313046(3.394646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4539 | Time 1.2061(1.1924) | Loss 3.290125(3.387330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4540 | Time 1.1851(1.1919) | Loss 3.261930(3.378552) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4541 | Time 1.1900(1.1918) | Loss 3.267549(3.370782) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4542 | Time 1.1884(1.1915) | Loss 3.314801(3.366863) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4543 | Time 1.1945(1.1918) | Loss 3.296804(3.361959) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4544 | Time 1.1901(1.1916) | Loss 3.309426(3.358282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4545 | Time 1.2166(1.1934) | Loss 3.300388(3.354229) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4546 | Time 1.2225(1.1954) | Loss 3.365622(3.355027) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4547 | Time 1.2242(1.1974) | Loss 3.404194(3.358468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4548 | Time 1.2167(1.1988) | Loss 3.435312(3.363847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4549 | Time 1.2147(1.1999) | Loss 3.460507(3.370614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4550 | Time 1.2104(1.2006) | Loss 3.466761(3.377344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4551 | Time 1.2526(1.2043) | Loss 3.473321(3.384062) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4552 | Time 1.2137(1.2049) | Loss 3.381775(3.383902) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4553 | Time 1.2122(1.2054) | Loss 3.464054(3.389513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4554 | Time 1.2037(1.2053) | Loss 3.449724(3.393728) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4555 | Time 1.1995(1.2049) | Loss 3.511668(3.401983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4556 | Time 1.2013(1.2047) | Loss 3.463233(3.406271) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4557 | Time 1.2125(1.2052) | Loss 3.539928(3.415627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4558 | Time 1.2051(1.2052) | Loss 3.406418(3.414982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4559 | Time 1.2085(1.2054) | Loss 3.452064(3.417578) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4560 | Time 1.2097(1.2057) | Loss 3.487504(3.422473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4561 | Time 1.2161(1.2065) | Loss 3.456378(3.424846) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4562 | Time 1.2000(1.2060) | Loss 3.451342(3.426701) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4563 | Time 1.1949(1.2052) | Loss 3.440889(3.427694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4564 | Time 1.1913(1.2043) | Loss 3.571529(3.437762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4565 | Time 1.1925(1.2034) | Loss 3.509962(3.442816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4566 | Time 1.1970(1.2030) | Loss 3.417253(3.441027) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4567 | Time 1.2022(1.2029) | Loss 3.447431(3.441475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4568 | Time 1.2140(1.2037) | Loss 3.418493(3.439867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4569 | Time 1.1916(1.2029) | Loss 3.480859(3.442736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4570 | Time 1.1950(1.2023) | Loss 3.506743(3.447217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4571 | Time 1.1834(1.2010) | Loss 3.439727(3.446692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4572 | Time 1.1850(1.1999) | Loss 3.446752(3.446696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4573 | Time 1.1915(1.1993) | Loss 3.425725(3.445228) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4574 | Time 1.2364(1.2019) | Loss 3.517935(3.450318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4575 | Time 1.1908(1.2011) | Loss 3.408590(3.447397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4576 | Time 1.1891(1.2003) | Loss 3.516727(3.452250) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4577 | Time 1.1981(1.2001) | Loss 3.416893(3.449775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4578 | Time 1.1969(1.1999) | Loss 3.507149(3.453791) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4579 | Time 1.2034(1.2001) | Loss 3.404951(3.450372) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4580 | Time 1.1888(1.1993) | Loss 3.451860(3.450477) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4581 | Time 1.1922(1.1988) | Loss 3.438391(3.449631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4582 | Time 1.2086(1.1995) | Loss 3.491152(3.452537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4583 | Time 1.1818(1.1983) | Loss 3.506912(3.456343) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4584 | Time 1.1878(1.1976) | Loss 3.444865(3.455540) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4585 | Time 1.1830(1.1965) | Loss 3.417735(3.452894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4586 | Time 1.2153(1.1979) | Loss 3.398341(3.449075) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4587 | Time 1.2092(1.1986) | Loss 3.423476(3.447283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4588 | Time 1.1933(1.1983) | Loss 3.458662(3.448080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4589 | Time 1.2602(1.2026) | Loss 3.447131(3.448013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4590 | Time 1.2473(1.2057) | Loss 3.390131(3.443961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4591 | Time 1.2018(1.2055) | Loss 3.436095(3.443411) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4592 | Time 1.1936(1.2046) | Loss 3.390472(3.439705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4593 | Time 1.1916(1.2037) | Loss 3.395303(3.436597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4594 | Time 1.2025(1.2036) | Loss 3.469723(3.438916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4595 | Time 1.2040(1.2037) | Loss 3.516121(3.444320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4596 | Time 1.1995(1.2034) | Loss 3.478310(3.446699) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4597 | Time 1.1915(1.2025) | Loss 3.367574(3.441161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4598 | Time 1.1841(1.2013) | Loss 3.497543(3.445107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4599 | Time 1.1963(1.2009) | Loss 3.391700(3.441369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4600 | Time 1.1938(1.2004) | Loss 3.473418(3.443612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4600 | Test Loss 3.407571 | NFE 20
Skipping vis as data dimension is >2
Iter 4601 | Time 1.1966(1.2001) | Loss 3.459334(3.444713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4602 | Time 1.1906(1.1995) | Loss 3.426741(3.443455) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4603 | Time 1.1883(1.1987) | Loss 3.366826(3.438091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4604 | Time 1.2009(1.1988) | Loss 3.446379(3.438671) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4605 | Time 1.2092(1.1996) | Loss 3.419462(3.437326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4606 | Time 1.2761(1.2049) | Loss 3.431044(3.436887) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4607 | Time 1.2302(1.2067) | Loss 3.485521(3.440291) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4608 | Time 1.1909(1.2056) | Loss 3.430698(3.439619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4609 | Time 1.1816(1.2039) | Loss 3.459223(3.440992) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4610 | Time 1.2268(1.2055) | Loss 3.395664(3.437819) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4611 | Time 1.1905(1.2045) | Loss 3.432082(3.437417) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4612 | Time 1.2166(1.2053) | Loss 3.556654(3.445764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4613 | Time 1.2587(1.2090) | Loss 3.387959(3.441717) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4614 | Time 1.2604(1.2126) | Loss 3.437897(3.441450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4615 | Time 1.2532(1.2155) | Loss 3.401727(3.438669) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4616 | Time 1.1838(1.2133) | Loss 3.406014(3.436384) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4617 | Time 1.2014(1.2124) | Loss 3.499768(3.440820) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4618 | Time 1.1895(1.2108) | Loss 3.450346(3.441487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4619 | Time 1.2016(1.2102) | Loss 3.511364(3.446379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4620 | Time 1.1862(1.2085) | Loss 3.494642(3.449757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4621 | Time 1.1946(1.2075) | Loss 3.400465(3.446307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4622 | Time 1.1878(1.2062) | Loss 3.434726(3.445496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4623 | Time 1.1801(1.2043) | Loss 3.445195(3.445475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4624 | Time 1.1754(1.2023) | Loss 3.495520(3.448978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4625 | Time 1.1819(1.2009) | Loss 3.477644(3.450985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4626 | Time 1.1815(1.1995) | Loss 3.360449(3.444647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4627 | Time 1.1829(1.1984) | Loss 3.489183(3.447765) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4628 | Time 1.2298(1.2006) | Loss 3.382859(3.443221) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4629 | Time 1.2035(1.2008) | Loss 3.423942(3.441872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4630 | Time 1.2010(1.2008) | Loss 3.437859(3.441591) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4631 | Time 1.1964(1.2005) | Loss 3.402901(3.438883) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4632 | Time 1.2084(1.2010) | Loss 3.367420(3.433880) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4633 | Time 1.2028(1.2012) | Loss 3.419829(3.432897) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4634 | Time 1.2098(1.2018) | Loss 3.465526(3.435181) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4635 | Time 1.2062(1.2021) | Loss 3.490515(3.439054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4636 | Time 1.1989(1.2019) | Loss 3.433525(3.438667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4637 | Time 1.1796(1.2003) | Loss 3.388286(3.435140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4638 | Time 1.2015(1.2004) | Loss 3.366688(3.430349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4639 | Time 1.1809(1.1990) | Loss 3.476024(3.433546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4640 | Time 1.1824(1.1979) | Loss 3.389281(3.430447) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4641 | Time 1.1881(1.1972) | Loss 3.519678(3.436694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4642 | Time 1.1811(1.1960) | Loss 3.507288(3.441635) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4643 | Time 1.1920(1.1958) | Loss 3.486078(3.444746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4644 | Time 1.1851(1.1950) | Loss 3.478397(3.447102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4645 | Time 1.1936(1.1949) | Loss 3.370542(3.441743) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4646 | Time 1.1855(1.1943) | Loss 3.478627(3.444324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4647 | Time 1.1855(1.1936) | Loss 3.499636(3.448196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4648 | Time 1.1719(1.1921) | Loss 3.445326(3.447995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4649 | Time 1.1917(1.1921) | Loss 3.411050(3.445409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4650 | Time 1.1644(1.1902) | Loss 3.447587(3.445562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4651 | Time 1.1781(1.1893) | Loss 3.443625(3.445426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4652 | Time 1.1787(1.1886) | Loss 3.411268(3.443035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4653 | Time 1.1899(1.1887) | Loss 3.376470(3.438375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4654 | Time 1.1795(1.1880) | Loss 3.481368(3.441385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4655 | Time 1.1875(1.1880) | Loss 3.515314(3.446560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4656 | Time 1.2144(1.1898) | Loss 3.412877(3.444202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4657 | Time 1.2131(1.1915) | Loss 3.418506(3.442403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4658 | Time 1.1978(1.1919) | Loss 3.494770(3.446069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4659 | Time 1.1955(1.1922) | Loss 3.410951(3.443611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4660 | Time 1.1950(1.1924) | Loss 3.486970(3.446646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4661 | Time 1.1936(1.1924) | Loss 3.429902(3.445474) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4662 | Time 1.2040(1.1932) | Loss 3.523507(3.450936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4663 | Time 1.1848(1.1927) | Loss 3.452399(3.451039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4664 | Time 1.1848(1.1921) | Loss 3.536431(3.457016) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4665 | Time 1.2062(1.1931) | Loss 3.440034(3.455827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4666 | Time 1.2304(1.1957) | Loss 3.543230(3.461946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4667 | Time 1.1810(1.1947) | Loss 3.500107(3.464617) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4668 | Time 1.2055(1.1954) | Loss 3.538893(3.469816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4669 | Time 1.2050(1.1961) | Loss 3.386593(3.463991) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4670 | Time 1.1922(1.1958) | Loss 3.405867(3.459922) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4671 | Time 1.2093(1.1968) | Loss 3.495963(3.462445) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4672 | Time 1.1910(1.1964) | Loss 3.497941(3.464930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4673 | Time 1.1672(1.1943) | Loss 3.470316(3.465307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4674 | Time 1.1702(1.1926) | Loss 3.537691(3.470374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4675 | Time 1.1929(1.1927) | Loss 3.429812(3.467534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4676 | Time 1.1952(1.1928) | Loss 3.510557(3.470546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4677 | Time 1.2042(1.1936) | Loss 3.501255(3.472696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4678 | Time 1.2213(1.1956) | Loss 3.516357(3.475752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4679 | Time 1.2388(1.1986) | Loss 3.426941(3.472335) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4680 | Time 1.2585(1.2028) | Loss 3.534102(3.476659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4681 | Time 1.2672(1.2073) | Loss 3.524316(3.479995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4682 | Time 1.2836(1.2126) | Loss 3.534016(3.483776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4683 | Time 1.3354(1.2212) | Loss 3.464533(3.482429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4684 | Time 1.2147(1.2208) | Loss 3.483317(3.482491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4685 | Time 1.2552(1.2232) | Loss 3.404406(3.477025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4686 | Time 1.2126(1.2224) | Loss 3.456917(3.475618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4687 | Time 1.2075(1.2214) | Loss 3.417827(3.471572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4688 | Time 1.1950(1.2196) | Loss 3.437184(3.469165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4689 | Time 1.1836(1.2170) | Loss 3.417122(3.465522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4690 | Time 1.1901(1.2151) | Loss 3.435719(3.463436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4691 | Time 1.1658(1.2117) | Loss 3.480176(3.464608) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4692 | Time 1.1711(1.2088) | Loss 3.456073(3.464010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4693 | Time 1.1860(1.2072) | Loss 3.445177(3.462692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4694 | Time 1.1762(1.2051) | Loss 3.367353(3.456018) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4695 | Time 1.1723(1.2028) | Loss 3.388712(3.451307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4696 | Time 1.1643(1.2001) | Loss 3.392715(3.447205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4697 | Time 1.1798(1.1987) | Loss 3.378388(3.442388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4698 | Time 1.2005(1.1988) | Loss 3.451403(3.443019) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4699 | Time 1.1867(1.1979) | Loss 3.460530(3.444245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4700 | Time 1.1652(1.1957) | Loss 3.500634(3.448192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4700 | Test Loss 3.401056 | NFE 20
Skipping vis as data dimension is >2
Iter 4701 | Time 1.1741(1.1941) | Loss 3.466510(3.449475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4702 | Time 1.1630(1.1920) | Loss 3.449287(3.449461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4703 | Time 1.1701(1.1904) | Loss 3.414885(3.447041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4704 | Time 1.1670(1.1888) | Loss 3.456261(3.447686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4705 | Time 1.1765(1.1879) | Loss 3.426082(3.446174) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4706 | Time 1.2506(1.1923) | Loss 3.459426(3.447102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4707 | Time 1.1698(1.1907) | Loss 3.365244(3.441372) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4708 | Time 1.1725(1.1895) | Loss 3.444751(3.441608) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4709 | Time 1.1836(1.1891) | Loss 3.482152(3.444446) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4710 | Time 1.2087(1.1904) | Loss 3.527632(3.450269) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4711 | Time 1.1727(1.1892) | Loss 3.376320(3.445093) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4712 | Time 1.1713(1.1879) | Loss 3.485554(3.447925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4713 | Time 1.1676(1.1865) | Loss 3.332811(3.439867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4714 | Time 1.1692(1.1853) | Loss 3.428860(3.439097) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4715 | Time 1.2069(1.1868) | Loss 3.455196(3.440224) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4716 | Time 1.1801(1.1863) | Loss 3.377876(3.435859) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4717 | Time 1.1942(1.1869) | Loss 3.368605(3.431151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4718 | Time 1.1775(1.1862) | Loss 3.388112(3.428139) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4719 | Time 1.1755(1.1855) | Loss 3.371217(3.424154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4720 | Time 1.1737(1.1847) | Loss 3.369643(3.420338) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4721 | Time 1.1741(1.1839) | Loss 3.357786(3.415960) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4722 | Time 1.1583(1.1821) | Loss 3.356705(3.411812) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4723 | Time 1.1626(1.1808) | Loss 3.279656(3.402561) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4724 | Time 1.1873(1.1812) | Loss 3.364023(3.399863) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4725 | Time 1.1831(1.1813) | Loss 3.397791(3.399718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4726 | Time 1.1664(1.1803) | Loss 3.369338(3.397592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4727 | Time 1.1732(1.1798) | Loss 3.322088(3.392306) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4728 | Time 1.1883(1.1804) | Loss 3.385327(3.391818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4729 | Time 1.1707(1.1797) | Loss 3.346639(3.388655) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4730 | Time 1.1693(1.1790) | Loss 3.388869(3.388670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4731 | Time 1.1822(1.1792) | Loss 3.345159(3.385625) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4732 | Time 1.1806(1.1793) | Loss 3.378670(3.385138) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4733 | Time 1.1892(1.1800) | Loss 3.410330(3.386901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4734 | Time 1.1858(1.1804) | Loss 3.406285(3.388258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4735 | Time 1.1923(1.1812) | Loss 3.346624(3.385344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4736 | Time 1.1827(1.1814) | Loss 3.322553(3.380948) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4737 | Time 1.1764(1.1810) | Loss 3.349471(3.378745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4738 | Time 1.2519(1.1860) | Loss 3.375112(3.378491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4739 | Time 1.1905(1.1863) | Loss 3.323076(3.374612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4740 | Time 1.1937(1.1868) | Loss 3.378861(3.374909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4741 | Time 1.1843(1.1866) | Loss 3.404817(3.377003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4742 | Time 1.1839(1.1864) | Loss 3.417882(3.379864) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4743 | Time 1.1937(1.1869) | Loss 3.443353(3.384308) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4744 | Time 1.1956(1.1876) | Loss 3.443384(3.388444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4745 | Time 1.1938(1.1880) | Loss 3.426921(3.391137) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4746 | Time 1.1900(1.1881) | Loss 3.457781(3.395802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4747 | Time 1.1855(1.1879) | Loss 3.373188(3.394219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4748 | Time 1.1872(1.1879) | Loss 3.337906(3.390277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4749 | Time 1.1941(1.1883) | Loss 3.418532(3.392255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4750 | Time 1.1779(1.1876) | Loss 3.474530(3.398014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4751 | Time 1.1976(1.1883) | Loss 3.466934(3.402839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4752 | Time 1.1936(1.1887) | Loss 3.404875(3.402981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4753 | Time 1.1915(1.1889) | Loss 3.495503(3.409458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4754 | Time 1.2012(1.1897) | Loss 3.432329(3.411059) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4755 | Time 1.1861(1.1895) | Loss 3.454396(3.414092) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4756 | Time 1.1862(1.1892) | Loss 3.493723(3.419667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4757 | Time 1.1652(1.1876) | Loss 3.478616(3.423793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4758 | Time 1.1791(1.1870) | Loss 3.404428(3.422438) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4759 | Time 1.1716(1.1859) | Loss 3.308491(3.414461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4760 | Time 1.1859(1.1859) | Loss 3.382345(3.412213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4761 | Time 1.1803(1.1855) | Loss 3.325907(3.406172) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4762 | Time 1.1672(1.1842) | Loss 3.360881(3.403001) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4763 | Time 1.1926(1.1848) | Loss 3.422998(3.404401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4764 | Time 1.1828(1.1847) | Loss 3.318467(3.398386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4765 | Time 1.2154(1.1868) | Loss 3.399935(3.398494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4766 | Time 1.1877(1.1869) | Loss 3.444154(3.401690) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4767 | Time 1.1894(1.1871) | Loss 3.319528(3.395939) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4768 | Time 1.1839(1.1868) | Loss 3.461744(3.400545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4769 | Time 1.1796(1.1863) | Loss 3.465833(3.405115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4770 | Time 1.1960(1.1870) | Loss 3.376553(3.403116) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4771 | Time 1.1893(1.1872) | Loss 3.420732(3.404349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4772 | Time 1.1841(1.1870) | Loss 3.362458(3.401417) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4773 | Time 1.2128(1.1888) | Loss 3.463081(3.405733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4774 | Time 1.2431(1.1926) | Loss 3.442203(3.408286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4775 | Time 1.1893(1.1923) | Loss 3.396683(3.407474) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4776 | Time 1.1752(1.1911) | Loss 3.431906(3.409184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4777 | Time 1.1863(1.1908) | Loss 3.443701(3.411600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4778 | Time 1.1895(1.1907) | Loss 3.514489(3.418803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4779 | Time 1.1731(1.1895) | Loss 3.451510(3.421092) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4780 | Time 1.1551(1.1871) | Loss 3.462436(3.423986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4781 | Time 1.1648(1.1855) | Loss 3.421818(3.423834) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4782 | Time 1.1756(1.1848) | Loss 3.447332(3.425479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4783 | Time 1.2184(1.1872) | Loss 3.517721(3.431936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4784 | Time 1.1667(1.1857) | Loss 3.400507(3.429736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4785 | Time 1.1925(1.1862) | Loss 3.429995(3.429754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4786 | Time 1.1823(1.1859) | Loss 3.432829(3.429969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4787 | Time 1.1711(1.1849) | Loss 3.395473(3.427555) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4788 | Time 1.1884(1.1851) | Loss 3.376667(3.423993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4789 | Time 1.1884(1.1854) | Loss 3.323214(3.416938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4790 | Time 1.1780(1.1849) | Loss 3.382219(3.414508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4791 | Time 1.1872(1.1850) | Loss 3.383873(3.412363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4792 | Time 1.1909(1.1854) | Loss 3.338560(3.407197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4793 | Time 1.2063(1.1869) | Loss 3.325644(3.401488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4794 | Time 1.2114(1.1886) | Loss 3.354425(3.398194) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4795 | Time 1.2189(1.1907) | Loss 3.377916(3.396774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4796 | Time 1.2073(1.1919) | Loss 3.344998(3.393150) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4797 | Time 1.2176(1.1937) | Loss 3.383794(3.392495) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4798 | Time 1.2005(1.1942) | Loss 3.296126(3.385749) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4799 | Time 1.1973(1.1944) | Loss 3.332877(3.382048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4800 | Time 1.1989(1.1947) | Loss 3.439509(3.386070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4800 | Test Loss 3.323694 | NFE 20
Skipping vis as data dimension is >2
Iter 4801 | Time 1.2552(1.1989) | Loss 3.381323(3.385738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4802 | Time 1.2377(1.2016) | Loss 3.361987(3.384076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4803 | Time 1.2131(1.2024) | Loss 3.420077(3.386596) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4804 | Time 1.2164(1.2034) | Loss 3.358231(3.384610) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4805 | Time 1.2056(1.2036) | Loss 3.424536(3.387405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4806 | Time 1.2333(1.2057) | Loss 3.411393(3.389084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4807 | Time 1.2039(1.2055) | Loss 3.407593(3.390380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4808 | Time 1.1948(1.2048) | Loss 3.463233(3.395479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4809 | Time 1.2564(1.2084) | Loss 3.343642(3.391851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4810 | Time 1.2233(1.2094) | Loss 3.452833(3.396120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4811 | Time 1.1984(1.2087) | Loss 3.422206(3.397946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4812 | Time 1.2004(1.2081) | Loss 3.445958(3.401307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4813 | Time 1.1872(1.2066) | Loss 3.484627(3.407139) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4814 | Time 1.1724(1.2042) | Loss 3.441848(3.409569) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4815 | Time 1.1751(1.2022) | Loss 3.308956(3.402526) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4816 | Time 1.1912(1.2014) | Loss 3.406928(3.402834) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4817 | Time 1.1959(1.2010) | Loss 3.378910(3.401159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4818 | Time 1.3970(1.2148) | Loss 3.372182(3.399131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4819 | Time 1.4177(1.2290) | Loss 3.352624(3.395875) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4820 | Time 1.2497(1.2304) | Loss 3.282001(3.387904) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4821 | Time 1.2530(1.2320) | Loss 3.367027(3.386443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4822 | Time 1.2743(1.2350) | Loss 3.325299(3.382163) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4823 | Time 1.1948(1.2321) | Loss 3.366286(3.381051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4824 | Time 1.1925(1.2294) | Loss 3.407996(3.382937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4825 | Time 1.2360(1.2298) | Loss 3.398773(3.384046) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4826 | Time 1.2472(1.2310) | Loss 3.378682(3.383670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4827 | Time 1.2477(1.2322) | Loss 3.317595(3.379045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4828 | Time 1.2335(1.2323) | Loss 3.350749(3.377064) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4829 | Time 1.2225(1.2316) | Loss 3.292608(3.371152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4830 | Time 1.2622(1.2338) | Loss 3.272628(3.364256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4831 | Time 1.2178(1.2326) | Loss 3.373794(3.364923) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4832 | Time 1.2016(1.2305) | Loss 3.346158(3.363610) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4833 | Time 1.2209(1.2298) | Loss 3.292711(3.358647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4834 | Time 1.2035(1.2280) | Loss 3.245621(3.350735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4835 | Time 1.2214(1.2275) | Loss 3.292847(3.346683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4836 | Time 1.1998(1.2256) | Loss 3.312432(3.344285) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4837 | Time 1.2382(1.2264) | Loss 3.281306(3.339877) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4838 | Time 1.2585(1.2287) | Loss 3.274569(3.335305) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4839 | Time 1.2967(1.2334) | Loss 3.301704(3.332953) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4840 | Time 1.2121(1.2320) | Loss 3.343466(3.333689) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4841 | Time 1.2310(1.2319) | Loss 3.306261(3.331769) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4842 | Time 1.2445(1.2328) | Loss 3.314495(3.330560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4843 | Time 1.2302(1.2326) | Loss 3.374837(3.333659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4844 | Time 1.2104(1.2310) | Loss 3.395753(3.338006) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4845 | Time 1.2185(1.2302) | Loss 3.325741(3.337147) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4846 | Time 1.2357(1.2305) | Loss 3.370419(3.339476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4847 | Time 1.2137(1.2294) | Loss 3.371427(3.341713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4848 | Time 1.2290(1.2293) | Loss 3.444854(3.348933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4849 | Time 1.1982(1.2272) | Loss 3.403692(3.352766) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4850 | Time 1.1666(1.2229) | Loss 3.318931(3.350397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4851 | Time 1.2036(1.2216) | Loss 3.433988(3.356249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4852 | Time 1.1849(1.2190) | Loss 3.381043(3.357984) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4853 | Time 1.1775(1.2161) | Loss 3.335719(3.356426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4854 | Time 1.1761(1.2133) | Loss 3.409403(3.360134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4855 | Time 1.1899(1.2117) | Loss 3.378872(3.361446) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4856 | Time 1.1934(1.2104) | Loss 3.308328(3.357728) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4857 | Time 1.1629(1.2071) | Loss 3.382764(3.359480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4858 | Time 1.1689(1.2044) | Loss 3.345170(3.358478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4859 | Time 1.1680(1.2018) | Loss 3.319153(3.355726) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4860 | Time 1.1608(1.1990) | Loss 3.383491(3.357669) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4861 | Time 1.1777(1.1975) | Loss 3.375669(3.358929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4862 | Time 1.1552(1.1945) | Loss 3.351599(3.358416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4863 | Time 1.1798(1.1935) | Loss 3.405857(3.361737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4864 | Time 1.1779(1.1924) | Loss 3.296223(3.357151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4865 | Time 1.1656(1.1905) | Loss 3.373247(3.358278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4866 | Time 1.1835(1.1900) | Loss 3.317572(3.355428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4867 | Time 1.1596(1.1879) | Loss 3.417376(3.359765) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4868 | Time 1.1754(1.1870) | Loss 3.334731(3.358012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4869 | Time 1.1775(1.1864) | Loss 3.328406(3.355940) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4870 | Time 1.1776(1.1858) | Loss 3.269755(3.349907) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4871 | Time 1.1557(1.1836) | Loss 3.330753(3.348566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4872 | Time 1.1593(1.1819) | Loss 3.372339(3.350230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4873 | Time 1.1574(1.1802) | Loss 3.341612(3.349627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4874 | Time 1.1845(1.1805) | Loss 3.373572(3.351303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4875 | Time 1.1540(1.1787) | Loss 3.359746(3.351894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4876 | Time 1.1698(1.1780) | Loss 3.365708(3.352861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4877 | Time 1.1925(1.1791) | Loss 3.478040(3.361624) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4878 | Time 1.1728(1.1786) | Loss 3.405654(3.364706) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4879 | Time 1.1661(1.1777) | Loss 3.366922(3.364861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4880 | Time 1.1795(1.1779) | Loss 3.285011(3.359271) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4881 | Time 1.1921(1.1789) | Loss 3.418415(3.363411) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4882 | Time 1.1704(1.1783) | Loss 3.389032(3.365205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4883 | Time 1.1612(1.1771) | Loss 3.373702(3.365800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4884 | Time 1.1713(1.1767) | Loss 3.379346(3.366748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4885 | Time 1.1734(1.1764) | Loss 3.265819(3.359683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4886 | Time 1.1722(1.1761) | Loss 3.411544(3.363313) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4887 | Time 1.1827(1.1766) | Loss 3.280855(3.357541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4888 | Time 1.1767(1.1766) | Loss 3.359302(3.357664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4889 | Time 1.1691(1.1761) | Loss 3.416933(3.361813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4890 | Time 1.1695(1.1756) | Loss 3.330635(3.359631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4891 | Time 1.1806(1.1760) | Loss 3.333271(3.357786) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4892 | Time 1.1662(1.1753) | Loss 3.348688(3.357149) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4893 | Time 1.1687(1.1748) | Loss 3.232242(3.348405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4894 | Time 1.1745(1.1748) | Loss 3.287414(3.344136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4895 | Time 1.1679(1.1743) | Loss 3.278510(3.339542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4896 | Time 1.1704(1.1740) | Loss 3.241822(3.332702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4897 | Time 1.1849(1.1748) | Loss 3.307194(3.330916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4898 | Time 1.1733(1.1747) | Loss 3.316975(3.329940) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4899 | Time 1.1989(1.1764) | Loss 3.349196(3.331288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4900 | Time 1.1783(1.1765) | Loss 3.347859(3.332448) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 4900 | Test Loss 3.317915 | NFE 20
Skipping vis as data dimension is >2
Iter 4901 | Time 1.1818(1.1769) | Loss 3.341892(3.333109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4902 | Time 1.2275(1.1804) | Loss 3.326378(3.332638) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4903 | Time 1.1844(1.1807) | Loss 3.315589(3.331445) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4904 | Time 1.1756(1.1804) | Loss 3.389120(3.335482) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4905 | Time 1.1826(1.1805) | Loss 3.343867(3.336069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4906 | Time 1.1886(1.1811) | Loss 3.401652(3.340660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4907 | Time 1.1916(1.1818) | Loss 3.383058(3.343627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4908 | Time 1.1956(1.1828) | Loss 3.291374(3.339970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4909 | Time 1.1735(1.1821) | Loss 3.420246(3.345589) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4910 | Time 1.1644(1.1809) | Loss 3.346348(3.345642) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4911 | Time 1.1874(1.1813) | Loss 3.367976(3.347206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4912 | Time 1.1593(1.1798) | Loss 3.452127(3.354550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4913 | Time 1.1701(1.1791) | Loss 3.370480(3.355665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4914 | Time 1.1680(1.1783) | Loss 3.407837(3.359317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4915 | Time 1.2409(1.1827) | Loss 3.397077(3.361960) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4916 | Time 1.1782(1.1824) | Loss 3.367960(3.362380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4917 | Time 1.2230(1.1853) | Loss 3.422383(3.366581) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4918 | Time 1.2640(1.1908) | Loss 3.391934(3.368355) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4919 | Time 1.2209(1.1929) | Loss 3.334241(3.365967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4920 | Time 1.1794(1.1919) | Loss 3.408612(3.368952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4921 | Time 1.1607(1.1897) | Loss 3.376360(3.369471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4922 | Time 1.1909(1.1898) | Loss 3.360315(3.368830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4923 | Time 1.1971(1.1903) | Loss 3.384722(3.369942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4924 | Time 1.2034(1.1912) | Loss 3.374290(3.370247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4925 | Time 1.1692(1.1897) | Loss 3.364668(3.369856) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4926 | Time 1.1878(1.1896) | Loss 3.398983(3.371895) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4927 | Time 1.2090(1.1909) | Loss 3.374212(3.372057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4928 | Time 1.1805(1.1902) | Loss 3.377331(3.372427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4929 | Time 1.1996(1.1909) | Loss 3.341191(3.370240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4930 | Time 1.1980(1.1914) | Loss 3.357646(3.369358) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4931 | Time 1.2087(1.1926) | Loss 3.347408(3.367822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4932 | Time 1.2168(1.1943) | Loss 3.366348(3.367719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4933 | Time 1.2413(1.1976) | Loss 3.415994(3.371098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4934 | Time 1.2081(1.1983) | Loss 3.354350(3.369926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4935 | Time 1.2137(1.1994) | Loss 3.478281(3.377510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4936 | Time 1.2292(1.2015) | Loss 3.491909(3.385518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4937 | Time 1.2840(1.2072) | Loss 3.448805(3.389948) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4938 | Time 1.2669(1.2114) | Loss 3.457257(3.394660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4939 | Time 1.2103(1.2113) | Loss 3.419554(3.396403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4940 | Time 1.2126(1.2114) | Loss 3.424559(3.398374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4941 | Time 1.2891(1.2169) | Loss 3.396776(3.398262) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4942 | Time 1.2835(1.2215) | Loss 3.420747(3.399836) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4943 | Time 1.2854(1.2260) | Loss 3.376184(3.398180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4944 | Time 1.2811(1.2299) | Loss 3.399600(3.398279) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4945 | Time 1.2619(1.2321) | Loss 3.381714(3.397120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4946 | Time 1.2318(1.2321) | Loss 3.365220(3.394887) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4947 | Time 1.1903(1.2291) | Loss 3.344684(3.391373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4948 | Time 1.1892(1.2264) | Loss 3.375984(3.390295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4949 | Time 1.1876(1.2236) | Loss 3.241350(3.379869) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4950 | Time 1.2916(1.2284) | Loss 3.344196(3.377372) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4951 | Time 1.2725(1.2315) | Loss 3.248579(3.368357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4952 | Time 1.2132(1.2302) | Loss 3.284957(3.362519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4953 | Time 1.6069(1.2566) | Loss 3.292448(3.357614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4954 | Time 1.2754(1.2579) | Loss 3.295325(3.353254) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4955 | Time 1.2254(1.2556) | Loss 3.305114(3.349884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4956 | Time 1.3070(1.2592) | Loss 3.241542(3.342300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4957 | Time 1.3049(1.2624) | Loss 3.322038(3.340882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4958 | Time 1.2288(1.2601) | Loss 3.310332(3.338743) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4959 | Time 1.2338(1.2582) | Loss 3.273769(3.334195) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4960 | Time 1.2674(1.2589) | Loss 3.293963(3.331379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4961 | Time 1.2376(1.2574) | Loss 3.289310(3.328434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4962 | Time 1.2539(1.2571) | Loss 3.333470(3.328786) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4963 | Time 1.2537(1.2569) | Loss 3.366869(3.331452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4964 | Time 1.2783(1.2584) | Loss 3.316175(3.330383) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4965 | Time 1.2299(1.2564) | Loss 3.352578(3.331936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4966 | Time 1.2023(1.2526) | Loss 3.370894(3.334663) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4967 | Time 1.2393(1.2517) | Loss 3.425217(3.341002) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4968 | Time 1.2344(1.2505) | Loss 3.360621(3.342375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4969 | Time 1.2294(1.2490) | Loss 3.381268(3.345098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4970 | Time 1.2086(1.2462) | Loss 3.430931(3.351106) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4971 | Time 1.2276(1.2449) | Loss 3.366707(3.352198) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4972 | Time 1.2250(1.2435) | Loss 3.420161(3.356956) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4973 | Time 1.2757(1.2457) | Loss 3.415511(3.361055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4974 | Time 1.2516(1.2461) | Loss 3.328836(3.358799) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4975 | Time 1.2521(1.2466) | Loss 3.370876(3.359645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4976 | Time 1.2447(1.2464) | Loss 3.318664(3.356776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4977 | Time 1.2680(1.2479) | Loss 3.424051(3.361485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4978 | Time 1.2375(1.2472) | Loss 3.330146(3.359292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4979 | Time 1.2403(1.2467) | Loss 3.327580(3.357072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4980 | Time 1.2362(1.2460) | Loss 3.261957(3.350414) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4981 | Time 1.2378(1.2454) | Loss 3.326524(3.348741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4982 | Time 1.2588(1.2463) | Loss 3.278889(3.343852) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4983 | Time 1.2108(1.2439) | Loss 3.354896(3.344625) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4984 | Time 1.1720(1.2388) | Loss 3.275288(3.339771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4985 | Time 1.1879(1.2353) | Loss 3.360131(3.341196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4986 | Time 1.1729(1.2309) | Loss 3.357277(3.342322) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4987 | Time 1.1727(1.2268) | Loss 3.261305(3.336651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4988 | Time 1.1744(1.2232) | Loss 3.284837(3.333024) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4989 | Time 1.1836(1.2204) | Loss 3.272266(3.328771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4990 | Time 1.1703(1.2169) | Loss 3.315617(3.327850) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4991 | Time 1.1854(1.2147) | Loss 3.275706(3.324200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4992 | Time 1.1908(1.2130) | Loss 3.362904(3.326909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4993 | Time 1.1938(1.2117) | Loss 3.287592(3.324157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4994 | Time 1.1751(1.2091) | Loss 3.322000(3.324006) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4995 | Time 1.1658(1.2061) | Loss 3.230248(3.317443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4996 | Time 1.1782(1.2041) | Loss 3.346155(3.319453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4997 | Time 1.1700(1.2017) | Loss 3.432565(3.327371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4998 | Time 1.1738(1.1998) | Loss 3.364340(3.329959) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 4999 | Time 1.1730(1.1979) | Loss 3.329123(3.329900) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5000 | Time 1.1710(1.1960) | Loss 3.354150(3.331598) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5000 | Test Loss 3.443315 | NFE 20
Skipping vis as data dimension is >2
Iter 5001 | Time 1.1728(1.1944) | Loss 3.301843(3.329515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5002 | Time 1.1776(1.1932) | Loss 3.413821(3.335416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5003 | Time 1.1668(1.1914) | Loss 3.383782(3.338802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5004 | Time 1.1657(1.1896) | Loss 3.368029(3.340848) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5005 | Time 1.1716(1.1883) | Loss 3.331179(3.340171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5006 | Time 1.1816(1.1879) | Loss 3.332020(3.339600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5007 | Time 1.1772(1.1871) | Loss 3.360402(3.341056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5008 | Time 1.1726(1.1861) | Loss 3.419102(3.346520) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5009 | Time 1.2024(1.1872) | Loss 3.311825(3.344091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5010 | Time 1.1772(1.1865) | Loss 3.404513(3.348321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5011 | Time 1.1935(1.1870) | Loss 3.365309(3.349510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5012 | Time 1.1789(1.1865) | Loss 3.357604(3.350076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5013 | Time 1.1810(1.1861) | Loss 3.325089(3.348327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5014 | Time 1.2110(1.1878) | Loss 3.298234(3.344821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5015 | Time 1.1947(1.1883) | Loss 3.336678(3.344251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5016 | Time 1.1930(1.1886) | Loss 3.333881(3.343525) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5017 | Time 1.2045(1.1897) | Loss 3.318231(3.341754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5018 | Time 1.1943(1.1901) | Loss 3.313217(3.339757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5019 | Time 1.1826(1.1895) | Loss 3.259414(3.334133) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5020 | Time 1.1754(1.1885) | Loss 3.265491(3.329328) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5021 | Time 1.1687(1.1872) | Loss 3.271370(3.325271) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5022 | Time 1.1767(1.1864) | Loss 3.216616(3.317665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5023 | Time 1.1977(1.1872) | Loss 3.233772(3.311792) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5024 | Time 1.1953(1.1878) | Loss 3.227055(3.305861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5025 | Time 1.2368(1.1912) | Loss 3.223397(3.300088) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5026 | Time 1.1842(1.1907) | Loss 3.188282(3.292262) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5027 | Time 1.1789(1.1899) | Loss 3.190361(3.285129) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5028 | Time 1.1845(1.1895) | Loss 3.175282(3.277440) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5029 | Time 1.1900(1.1896) | Loss 3.225575(3.273809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5030 | Time 1.1903(1.1896) | Loss 3.115058(3.262696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5031 | Time 1.1752(1.1886) | Loss 3.250590(3.261849) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5032 | Time 1.1809(1.1881) | Loss 3.234234(3.259916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5033 | Time 1.1581(1.1860) | Loss 3.197264(3.255530) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5034 | Time 1.1802(1.1856) | Loss 3.239081(3.254379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5035 | Time 1.1963(1.1863) | Loss 3.280494(3.256207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5036 | Time 1.1895(1.1865) | Loss 3.284624(3.258196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5037 | Time 1.2155(1.1886) | Loss 3.347924(3.264477) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5038 | Time 1.1868(1.1884) | Loss 3.335658(3.269460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5039 | Time 1.1734(1.1874) | Loss 3.247915(3.267952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5040 | Time 1.1748(1.1865) | Loss 3.260728(3.267446) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5041 | Time 1.1845(1.1864) | Loss 3.345970(3.272943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5042 | Time 1.1765(1.1857) | Loss 3.330946(3.277003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5043 | Time 1.2071(1.1872) | Loss 3.305364(3.278988) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5044 | Time 1.2033(1.1883) | Loss 3.263561(3.277908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5045 | Time 1.2319(1.1914) | Loss 3.353088(3.283171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5046 | Time 1.2457(1.1952) | Loss 3.317942(3.285605) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5047 | Time 1.2445(1.1986) | Loss 3.331208(3.288797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5048 | Time 1.2342(1.2011) | Loss 3.334524(3.291998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5049 | Time 1.2528(1.2047) | Loss 3.302757(3.292751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5050 | Time 1.2560(1.2083) | Loss 3.348259(3.296637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5051 | Time 1.2371(1.2103) | Loss 3.338932(3.299597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5052 | Time 1.2224(1.2112) | Loss 3.361333(3.303919) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5053 | Time 1.1981(1.2103) | Loss 3.321105(3.305122) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5054 | Time 1.2087(1.2101) | Loss 3.259559(3.301932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5055 | Time 1.2082(1.2100) | Loss 3.259225(3.298943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5056 | Time 1.2116(1.2101) | Loss 3.277906(3.297470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5057 | Time 1.2274(1.2113) | Loss 3.231875(3.292879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5058 | Time 1.2127(1.2114) | Loss 3.300043(3.293380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5059 | Time 1.2297(1.2127) | Loss 3.265798(3.291449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5060 | Time 1.3152(1.2199) | Loss 3.221221(3.286533) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5061 | Time 1.2375(1.2211) | Loss 3.229188(3.282519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5062 | Time 1.2336(1.2220) | Loss 3.206537(3.277200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5063 | Time 1.2367(1.2230) | Loss 3.270130(3.276705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5064 | Time 1.2232(1.2230) | Loss 3.255774(3.275240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5065 | Time 1.1994(1.2214) | Loss 3.228410(3.271962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5066 | Time 1.2053(1.2203) | Loss 3.315118(3.274983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5067 | Time 1.2048(1.2192) | Loss 3.248121(3.273103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5068 | Time 1.2080(1.2184) | Loss 3.228105(3.269953) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5069 | Time 1.2128(1.2180) | Loss 3.230669(3.267203) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5070 | Time 1.2158(1.2179) | Loss 3.186926(3.261584) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5071 | Time 1.2096(1.2173) | Loss 3.159872(3.254464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5072 | Time 1.2214(1.2176) | Loss 3.292678(3.257139) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5073 | Time 1.2255(1.2181) | Loss 3.176602(3.251501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5074 | Time 1.2113(1.2177) | Loss 3.159564(3.245066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5075 | Time 1.2114(1.2172) | Loss 3.120515(3.236347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5076 | Time 1.2028(1.2162) | Loss 3.234558(3.236222) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5077 | Time 1.2546(1.2189) | Loss 3.220309(3.235108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5078 | Time 1.1916(1.2170) | Loss 3.288947(3.238877) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5079 | Time 1.2010(1.2159) | Loss 3.207658(3.236691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5080 | Time 1.1878(1.2139) | Loss 3.344606(3.244245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5081 | Time 1.2184(1.2142) | Loss 3.288364(3.247334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5082 | Time 1.2188(1.2145) | Loss 3.280062(3.249625) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5083 | Time 1.2248(1.2153) | Loss 3.199323(3.246104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5084 | Time 1.2497(1.2177) | Loss 3.208307(3.243458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5085 | Time 1.3239(1.2251) | Loss 3.218077(3.241681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5086 | Time 1.2829(1.2291) | Loss 3.234015(3.241144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5087 | Time 1.2746(1.2323) | Loss 3.287616(3.244397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5088 | Time 1.2797(1.2356) | Loss 3.274880(3.246531) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5089 | Time 1.2941(1.2397) | Loss 3.274787(3.248509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5090 | Time 1.2785(1.2425) | Loss 3.259283(3.249263) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5091 | Time 1.3012(1.2466) | Loss 3.301743(3.252937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5092 | Time 1.3004(1.2503) | Loss 3.304981(3.256580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5093 | Time 1.3348(1.2562) | Loss 3.278103(3.258087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5094 | Time 1.2821(1.2581) | Loss 3.342322(3.263983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5095 | Time 1.2773(1.2594) | Loss 3.318855(3.267824) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5096 | Time 1.2494(1.2587) | Loss 3.322222(3.271632) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5097 | Time 1.2761(1.2599) | Loss 3.335268(3.276087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5098 | Time 1.2493(1.2592) | Loss 3.280295(3.276381) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5099 | Time 1.2571(1.2590) | Loss 3.201471(3.271137) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5100 | Time 1.2564(1.2589) | Loss 3.329443(3.275219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5100 | Test Loss 3.199593 | NFE 20
Skipping vis as data dimension is >2
Iter 5101 | Time 1.2327(1.2570) | Loss 3.262622(3.274337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5102 | Time 1.2452(1.2562) | Loss 3.275373(3.274410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5103 | Time 1.2377(1.2549) | Loss 3.225707(3.271000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5104 | Time 1.2436(1.2541) | Loss 3.243718(3.269091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5105 | Time 1.2340(1.2527) | Loss 3.178606(3.262757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5106 | Time 1.2613(1.2533) | Loss 3.297558(3.265193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5107 | Time 1.2342(1.2520) | Loss 3.259943(3.264825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5108 | Time 1.2372(1.2509) | Loss 3.233809(3.262654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5109 | Time 1.2400(1.2502) | Loss 3.280648(3.263914) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5110 | Time 1.2500(1.2502) | Loss 3.262515(3.263816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5111 | Time 1.2918(1.2531) | Loss 3.228151(3.261319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5112 | Time 1.2847(1.2553) | Loss 3.355230(3.267893) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5113 | Time 1.2567(1.2554) | Loss 3.233429(3.265480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5114 | Time 1.2590(1.2556) | Loss 3.291688(3.267315) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5115 | Time 1.2219(1.2533) | Loss 3.311464(3.270405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5116 | Time 1.2488(1.2530) | Loss 3.301507(3.272583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5117 | Time 1.2406(1.2521) | Loss 3.222735(3.269093) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5118 | Time 1.2540(1.2522) | Loss 3.250152(3.267767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5119 | Time 1.2528(1.2523) | Loss 3.358297(3.274104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5120 | Time 1.2541(1.2524) | Loss 3.308429(3.276507) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5121 | Time 1.2635(1.2532) | Loss 3.244397(3.274259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5122 | Time 1.2300(1.2515) | Loss 3.316641(3.277226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5123 | Time 1.2111(1.2487) | Loss 3.325681(3.280618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5124 | Time 1.2348(1.2477) | Loss 3.348016(3.285336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5125 | Time 1.2023(1.2446) | Loss 3.396601(3.293124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5126 | Time 1.1889(1.2407) | Loss 3.320169(3.295017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5127 | Time 1.2048(1.2382) | Loss 3.319927(3.296761) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5128 | Time 1.1986(1.2354) | Loss 3.315405(3.298066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5129 | Time 1.2535(1.2367) | Loss 3.283429(3.297042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5130 | Time 1.1893(1.2333) | Loss 3.329181(3.299291) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5131 | Time 1.2232(1.2326) | Loss 3.385545(3.305329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5132 | Time 1.1908(1.2297) | Loss 3.334635(3.307380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5133 | Time 1.1888(1.2268) | Loss 3.325710(3.308664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5134 | Time 1.1963(1.2247) | Loss 3.324714(3.309787) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5135 | Time 1.1972(1.2228) | Loss 3.387285(3.315212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5136 | Time 1.1884(1.2204) | Loss 3.307351(3.314662) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5137 | Time 1.2042(1.2192) | Loss 3.276164(3.311967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5138 | Time 1.1966(1.2177) | Loss 3.357571(3.315159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5139 | Time 1.1773(1.2148) | Loss 3.366500(3.318753) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5140 | Time 1.2128(1.2147) | Loss 3.265913(3.315054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5141 | Time 1.2046(1.2140) | Loss 3.282428(3.312770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5142 | Time 1.2265(1.2149) | Loss 3.256120(3.308805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5143 | Time 1.2873(1.2199) | Loss 3.335835(3.310697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5144 | Time 1.2656(1.2231) | Loss 3.312725(3.310839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5145 | Time 1.2163(1.2227) | Loss 3.261979(3.307419) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5146 | Time 1.2058(1.2215) | Loss 3.358650(3.311005) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5147 | Time 1.2061(1.2204) | Loss 3.242452(3.306206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5148 | Time 1.2034(1.2192) | Loss 3.353574(3.309522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5149 | Time 1.2128(1.2188) | Loss 3.274278(3.307055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5150 | Time 1.2146(1.2185) | Loss 3.282946(3.305367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5151 | Time 1.2451(1.2203) | Loss 3.334032(3.307374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5152 | Time 1.2316(1.2211) | Loss 3.360677(3.311105) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5153 | Time 1.2461(1.2229) | Loss 3.247656(3.306664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5154 | Time 1.2379(1.2239) | Loss 3.300864(3.306258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5155 | Time 1.2463(1.2255) | Loss 3.314615(3.306843) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5156 | Time 1.2289(1.2257) | Loss 3.310332(3.307087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5157 | Time 1.1809(1.2226) | Loss 3.322376(3.308157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5158 | Time 1.1846(1.2199) | Loss 3.373994(3.312766) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5159 | Time 1.2515(1.2221) | Loss 3.344303(3.314973) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5160 | Time 1.2488(1.2240) | Loss 3.256761(3.310898) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5161 | Time 1.1779(1.2208) | Loss 3.305685(3.310534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5162 | Time 1.1782(1.2178) | Loss 3.406018(3.317217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5163 | Time 1.1923(1.2160) | Loss 3.279451(3.314574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5164 | Time 1.2270(1.2168) | Loss 3.298833(3.313472) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5165 | Time 1.2360(1.2181) | Loss 3.262867(3.309930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5166 | Time 1.2546(1.2207) | Loss 3.378952(3.314761) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5167 | Time 1.2460(1.2225) | Loss 3.321426(3.315228) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5168 | Time 1.2575(1.2249) | Loss 3.275097(3.312419) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5169 | Time 1.2739(1.2283) | Loss 3.248222(3.307925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5170 | Time 1.2836(1.2322) | Loss 3.309154(3.308011) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5171 | Time 1.2787(1.2355) | Loss 3.323419(3.309089) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5172 | Time 1.2653(1.2376) | Loss 3.241280(3.304343) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5173 | Time 1.2737(1.2401) | Loss 3.324157(3.305730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5174 | Time 1.2679(1.2420) | Loss 3.255984(3.302247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5175 | Time 1.2408(1.2419) | Loss 3.328245(3.304067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5176 | Time 1.2327(1.2413) | Loss 3.342967(3.306790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5177 | Time 1.2293(1.2405) | Loss 3.258741(3.303427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5178 | Time 1.2253(1.2394) | Loss 3.330695(3.305336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5179 | Time 1.2165(1.2378) | Loss 3.370067(3.309867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5180 | Time 1.2274(1.2371) | Loss 3.291656(3.308592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5181 | Time 1.2373(1.2371) | Loss 3.269348(3.305845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5182 | Time 1.2322(1.2367) | Loss 3.333056(3.307750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5183 | Time 1.2123(1.2350) | Loss 3.276069(3.305532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5184 | Time 1.2341(1.2350) | Loss 3.334977(3.307593) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5185 | Time 1.2321(1.2348) | Loss 3.300758(3.307115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5186 | Time 1.2311(1.2345) | Loss 3.326418(3.308466) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5187 | Time 1.2526(1.2358) | Loss 3.297633(3.307708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5188 | Time 1.2458(1.2365) | Loss 3.373219(3.312294) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5189 | Time 1.2486(1.2373) | Loss 3.351686(3.315051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5190 | Time 1.2491(1.2381) | Loss 3.411398(3.321795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5191 | Time 1.2383(1.2382) | Loss 3.307163(3.320771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5192 | Time 1.2487(1.2389) | Loss 3.383239(3.325144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5193 | Time 1.2361(1.2387) | Loss 3.291204(3.322768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5194 | Time 1.2025(1.2362) | Loss 3.372910(3.326278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5195 | Time 1.2038(1.2339) | Loss 3.302983(3.324647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5196 | Time 1.2213(1.2330) | Loss 3.398176(3.329794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5197 | Time 1.1969(1.2305) | Loss 3.240649(3.323554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5198 | Time 1.2737(1.2335) | Loss 3.359326(3.326058) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5199 | Time 1.2096(1.2318) | Loss 3.343595(3.327286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5200 | Time 1.1904(1.2289) | Loss 3.343049(3.328389) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5200 | Test Loss 3.335309 | NFE 20
Skipping vis as data dimension is >2
Iter 5201 | Time 1.2056(1.2273) | Loss 3.297879(3.326253) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5202 | Time 1.1887(1.2246) | Loss 3.339349(3.327170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5203 | Time 1.2004(1.2229) | Loss 3.348205(3.328643) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5204 | Time 1.2057(1.2217) | Loss 3.273944(3.324814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5205 | Time 1.2134(1.2211) | Loss 3.302487(3.323251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5206 | Time 1.1743(1.2178) | Loss 3.258897(3.318746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5207 | Time 1.1930(1.2161) | Loss 3.276505(3.315789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5208 | Time 1.1921(1.2144) | Loss 3.290670(3.314031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5209 | Time 1.2104(1.2141) | Loss 3.205317(3.306421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5210 | Time 1.1835(1.2120) | Loss 3.191863(3.298402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5211 | Time 1.1927(1.2107) | Loss 3.219884(3.292906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5212 | Time 1.1891(1.2091) | Loss 3.169597(3.284274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5213 | Time 1.1866(1.2076) | Loss 3.295690(3.285073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5214 | Time 1.1802(1.2056) | Loss 3.258908(3.283242) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5215 | Time 1.2003(1.2053) | Loss 3.158762(3.274528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5216 | Time 1.1851(1.2039) | Loss 3.191530(3.268718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5217 | Time 1.1984(1.2035) | Loss 3.158600(3.261010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5218 | Time 1.2078(1.2038) | Loss 3.198655(3.256645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5219 | Time 1.1921(1.2030) | Loss 3.122621(3.247263) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5220 | Time 1.1814(1.2015) | Loss 3.214882(3.244997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5221 | Time 1.1708(1.1993) | Loss 3.252898(3.245550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5222 | Time 1.1677(1.1971) | Loss 3.169678(3.240239) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5223 | Time 1.1754(1.1956) | Loss 3.179903(3.236015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5224 | Time 1.2224(1.1975) | Loss 3.286500(3.239549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5225 | Time 1.1820(1.1964) | Loss 3.216459(3.237933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5226 | Time 1.1906(1.1960) | Loss 3.217582(3.236508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5227 | Time 1.1815(1.1950) | Loss 3.229116(3.235991) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5228 | Time 1.1767(1.1937) | Loss 3.239510(3.236237) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5229 | Time 1.1771(1.1925) | Loss 3.279972(3.239299) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5230 | Time 1.1589(1.1902) | Loss 3.264074(3.241033) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5231 | Time 1.1705(1.1888) | Loss 3.279660(3.243737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5232 | Time 1.1643(1.1871) | Loss 3.273876(3.245847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5233 | Time 1.1850(1.1869) | Loss 3.301369(3.249733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5234 | Time 1.2083(1.1884) | Loss 3.370703(3.258201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5235 | Time 1.2225(1.1908) | Loss 3.299540(3.261095) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5236 | Time 1.2136(1.1924) | Loss 3.189465(3.256081) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5237 | Time 1.1877(1.1921) | Loss 3.179902(3.250748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5238 | Time 1.2020(1.1928) | Loss 3.340174(3.257008) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5239 | Time 1.2717(1.1983) | Loss 3.273735(3.258179) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5240 | Time 1.1935(1.1980) | Loss 3.190306(3.253428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5241 | Time 1.1644(1.1956) | Loss 3.187514(3.248814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5242 | Time 1.1869(1.1950) | Loss 3.218846(3.246716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5243 | Time 1.1751(1.1936) | Loss 3.233761(3.245809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5244 | Time 1.1905(1.1934) | Loss 3.336310(3.252144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5245 | Time 1.1702(1.1918) | Loss 3.284296(3.254395) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5246 | Time 1.1684(1.1901) | Loss 3.295869(3.257298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5247 | Time 1.1628(1.1882) | Loss 3.339386(3.263044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5248 | Time 1.1659(1.1866) | Loss 3.232089(3.260877) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5249 | Time 1.1963(1.1873) | Loss 3.299223(3.263562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5250 | Time 1.1754(1.1865) | Loss 3.278702(3.264621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5251 | Time 1.1822(1.1862) | Loss 3.367987(3.271857) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5252 | Time 1.2681(1.1919) | Loss 3.309006(3.274458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5253 | Time 1.2681(1.1972) | Loss 3.285315(3.275218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5254 | Time 1.2225(1.1990) | Loss 3.270395(3.274880) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5255 | Time 1.2141(1.2001) | Loss 3.318451(3.277930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5256 | Time 1.2294(1.2021) | Loss 3.280948(3.278141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5257 | Time 1.2051(1.2023) | Loss 3.307293(3.280182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5258 | Time 1.2371(1.2048) | Loss 3.270373(3.279495) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5259 | Time 1.2030(1.2046) | Loss 3.238312(3.276612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5260 | Time 1.2067(1.2048) | Loss 3.202469(3.271422) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5261 | Time 1.2009(1.2045) | Loss 3.234079(3.268808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5262 | Time 1.2837(1.2101) | Loss 3.311081(3.271767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5263 | Time 1.2068(1.2098) | Loss 3.194262(3.266342) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5264 | Time 1.2013(1.2092) | Loss 3.244130(3.264787) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5265 | Time 1.1951(1.2083) | Loss 3.250101(3.263759) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5266 | Time 1.1866(1.2067) | Loss 3.246201(3.262530) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5267 | Time 1.2075(1.2068) | Loss 3.212342(3.259017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5268 | Time 1.2117(1.2071) | Loss 3.308641(3.262491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5269 | Time 1.2274(1.2086) | Loss 3.209304(3.258768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5270 | Time 1.2403(1.2108) | Loss 3.246144(3.257884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5271 | Time 1.2123(1.2109) | Loss 3.324866(3.262573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5272 | Time 1.1971(1.2099) | Loss 3.268903(3.263016) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5273 | Time 1.1864(1.2083) | Loss 3.245007(3.261755) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5274 | Time 1.1801(1.2063) | Loss 3.333754(3.266795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5275 | Time 1.1747(1.2041) | Loss 3.197580(3.261950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5276 | Time 1.1889(1.2030) | Loss 3.264675(3.262141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5277 | Time 1.1981(1.2027) | Loss 3.291836(3.264219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5278 | Time 1.2316(1.2047) | Loss 3.267931(3.264479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5279 | Time 1.2453(1.2075) | Loss 3.270579(3.264906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5280 | Time 1.2048(1.2073) | Loss 3.335433(3.269843) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5281 | Time 1.2094(1.2075) | Loss 3.388133(3.278123) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5282 | Time 1.1924(1.2064) | Loss 3.288868(3.278876) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5283 | Time 1.1851(1.2049) | Loss 3.260059(3.277558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5284 | Time 1.1942(1.2042) | Loss 3.300060(3.279133) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5285 | Time 1.2059(1.2043) | Loss 3.283875(3.279465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5286 | Time 1.1876(1.2031) | Loss 3.274971(3.279151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5287 | Time 1.1840(1.2018) | Loss 3.187733(3.272752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5288 | Time 1.1797(1.2002) | Loss 3.244724(3.270790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5289 | Time 1.1748(1.1985) | Loss 3.211348(3.266629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5290 | Time 1.1815(1.1973) | Loss 3.218885(3.263287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5291 | Time 1.1846(1.1964) | Loss 3.195432(3.258537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5292 | Time 1.2060(1.1971) | Loss 3.191389(3.253837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5293 | Time 1.1851(1.1962) | Loss 3.219551(3.251437) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5294 | Time 1.1737(1.1946) | Loss 3.181293(3.246526) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5295 | Time 1.1812(1.1937) | Loss 3.151569(3.239879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5296 | Time 1.1857(1.1931) | Loss 3.217336(3.238301) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5297 | Time 1.1824(1.1924) | Loss 3.073599(3.226772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5298 | Time 1.1698(1.1908) | Loss 3.113120(3.218817) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5299 | Time 1.1708(1.1894) | Loss 3.169664(3.215376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5300 | Time 1.1668(1.1878) | Loss 3.168143(3.212070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5300 | Test Loss 3.175513 | NFE 20
Skipping vis as data dimension is >2
Iter 5301 | Time 1.1905(1.1880) | Loss 3.152420(3.207894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5302 | Time 1.1740(1.1870) | Loss 3.226820(3.209219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5303 | Time 1.1792(1.1865) | Loss 3.269618(3.213447) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5304 | Time 1.1722(1.1855) | Loss 3.199001(3.212436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5305 | Time 1.1783(1.1850) | Loss 3.238884(3.214287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5306 | Time 1.2245(1.1877) | Loss 3.262914(3.217691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5307 | Time 1.2537(1.1924) | Loss 3.306205(3.223887) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5308 | Time 1.2279(1.1948) | Loss 3.225243(3.223982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5309 | Time 1.2455(1.1984) | Loss 3.251023(3.225875) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5310 | Time 1.2272(1.2004) | Loss 3.285192(3.230027) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5311 | Time 1.2629(1.2048) | Loss 3.139448(3.223686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5312 | Time 1.2312(1.2066) | Loss 3.224581(3.223749) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5313 | Time 1.2202(1.2076) | Loss 3.236188(3.224620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5314 | Time 1.2108(1.2078) | Loss 3.230628(3.225040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5315 | Time 1.2088(1.2079) | Loss 3.128561(3.218287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5316 | Time 1.2399(1.2101) | Loss 3.139810(3.212793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5317 | Time 1.2118(1.2102) | Loss 3.163685(3.209356) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5318 | Time 1.2151(1.2106) | Loss 3.167726(3.206442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5319 | Time 1.2239(1.2115) | Loss 3.102715(3.199181) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5320 | Time 1.2061(1.2111) | Loss 3.111510(3.193044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5321 | Time 1.2107(1.2111) | Loss 3.199045(3.193464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5322 | Time 1.2285(1.2123) | Loss 3.166013(3.191542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5323 | Time 1.2087(1.2121) | Loss 3.175877(3.190446) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5324 | Time 1.2051(1.2116) | Loss 3.188052(3.190278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5325 | Time 1.2212(1.2122) | Loss 3.276654(3.196325) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5326 | Time 1.2214(1.2129) | Loss 3.214706(3.197611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5327 | Time 1.2255(1.2138) | Loss 3.113293(3.191709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5328 | Time 1.2239(1.2145) | Loss 3.123014(3.186900) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5329 | Time 1.1987(1.2134) | Loss 3.264539(3.192335) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5330 | Time 1.2216(1.2140) | Loss 3.245238(3.196038) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5331 | Time 1.2523(1.2166) | Loss 3.258183(3.200388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5332 | Time 1.2028(1.2157) | Loss 3.262013(3.204702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5333 | Time 1.2540(1.2184) | Loss 3.269333(3.209226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5334 | Time 1.2229(1.2187) | Loss 3.247518(3.211907) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5335 | Time 1.2158(1.2185) | Loss 3.293515(3.217619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5336 | Time 1.2140(1.2182) | Loss 3.203624(3.216640) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5337 | Time 1.2122(1.2177) | Loss 3.297540(3.222303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5338 | Time 1.1920(1.2159) | Loss 3.182164(3.219493) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5339 | Time 1.2130(1.2157) | Loss 3.255281(3.221998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5340 | Time 1.2205(1.2161) | Loss 3.220958(3.221925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5341 | Time 1.2149(1.2160) | Loss 3.257859(3.224441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5342 | Time 1.2105(1.2156) | Loss 3.274795(3.227965) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5343 | Time 1.2024(1.2147) | Loss 3.193493(3.225552) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5344 | Time 1.2081(1.2142) | Loss 3.226997(3.225654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5345 | Time 1.1897(1.2125) | Loss 3.189095(3.223094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5346 | Time 1.2159(1.2127) | Loss 3.175300(3.219749) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5347 | Time 1.2098(1.2125) | Loss 3.177908(3.216820) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5348 | Time 1.1936(1.2112) | Loss 3.197375(3.215459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5349 | Time 1.1985(1.2103) | Loss 3.221052(3.215850) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5350 | Time 1.1906(1.2089) | Loss 3.213496(3.215686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5351 | Time 1.1871(1.2074) | Loss 3.142486(3.210562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5352 | Time 1.2087(1.2075) | Loss 3.225218(3.211587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5353 | Time 1.2045(1.2073) | Loss 3.174230(3.208972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5354 | Time 1.2057(1.2072) | Loss 3.109192(3.201988) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5355 | Time 1.1967(1.2064) | Loss 3.170802(3.199805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5356 | Time 1.3019(1.2131) | Loss 3.197833(3.199667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5357 | Time 1.2391(1.2149) | Loss 3.171097(3.197667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5358 | Time 1.2248(1.2156) | Loss 3.179465(3.196393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5359 | Time 1.2443(1.2176) | Loss 3.174350(3.194850) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5360 | Time 1.2844(1.2223) | Loss 3.163629(3.192664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5361 | Time 1.2190(1.2221) | Loss 3.169963(3.191075) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5362 | Time 1.2123(1.2214) | Loss 3.213557(3.192649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5363 | Time 1.2199(1.2213) | Loss 3.239751(3.195946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5364 | Time 1.2137(1.2208) | Loss 3.268006(3.200990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5365 | Time 1.2273(1.2212) | Loss 3.230309(3.203043) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5366 | Time 1.2103(1.2205) | Loss 3.299667(3.209806) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5367 | Time 1.2282(1.2210) | Loss 3.307162(3.216621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5368 | Time 1.2281(1.2215) | Loss 3.271746(3.220480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5369 | Time 1.1945(1.2196) | Loss 3.306555(3.226505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5370 | Time 1.2063(1.2187) | Loss 3.324835(3.233388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5371 | Time 1.2170(1.2186) | Loss 3.297343(3.237865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5372 | Time 1.2193(1.2186) | Loss 3.243051(3.238228) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5373 | Time 1.2063(1.2178) | Loss 3.274906(3.240796) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5374 | Time 1.2220(1.2180) | Loss 3.247162(3.241241) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5375 | Time 1.2284(1.2188) | Loss 3.190280(3.237674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5376 | Time 1.2469(1.2207) | Loss 3.222034(3.236579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5377 | Time 1.2177(1.2205) | Loss 3.242518(3.236995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5378 | Time 1.1943(1.2187) | Loss 3.233716(3.236765) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5379 | Time 1.1739(1.2156) | Loss 3.210456(3.234924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5380 | Time 1.1838(1.2133) | Loss 3.150855(3.229039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5381 | Time 1.2468(1.2157) | Loss 3.241287(3.229896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5382 | Time 1.2010(1.2147) | Loss 3.158638(3.224908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5383 | Time 1.2029(1.2138) | Loss 3.235941(3.225681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5384 | Time 1.1877(1.2120) | Loss 3.184574(3.222803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5385 | Time 1.1932(1.2107) | Loss 3.278541(3.226705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5386 | Time 1.1813(1.2086) | Loss 3.195410(3.224514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5387 | Time 1.2286(1.2100) | Loss 3.143890(3.218870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5388 | Time 1.2191(1.2107) | Loss 3.175009(3.215800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5389 | Time 1.2124(1.2108) | Loss 3.181849(3.213424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5390 | Time 1.2151(1.2111) | Loss 3.195521(3.212170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5391 | Time 1.2244(1.2120) | Loss 3.271305(3.216310) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5392 | Time 1.2373(1.2138) | Loss 3.208413(3.215757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5393 | Time 1.2550(1.2167) | Loss 3.241780(3.217579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5394 | Time 1.2507(1.2191) | Loss 3.195479(3.216032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5395 | Time 1.1917(1.2171) | Loss 3.224084(3.216595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5396 | Time 1.2276(1.2179) | Loss 3.215943(3.216550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5397 | Time 1.2180(1.2179) | Loss 3.247043(3.218684) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5398 | Time 1.2265(1.2185) | Loss 3.174437(3.215587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5399 | Time 1.2870(1.2233) | Loss 3.188499(3.213691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5400 | Time 1.2167(1.2228) | Loss 3.240342(3.215556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5400 | Test Loss 3.227455 | NFE 20
Skipping vis as data dimension is >2
Iter 5401 | Time 1.1849(1.2202) | Loss 3.218326(3.215750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5402 | Time 1.1768(1.2171) | Loss 3.215059(3.215702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5403 | Time 1.1724(1.2140) | Loss 3.240438(3.217433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5404 | Time 1.1777(1.2115) | Loss 3.202706(3.216402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5405 | Time 1.1869(1.2097) | Loss 3.246776(3.218529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5406 | Time 1.1827(1.2078) | Loss 3.335097(3.226688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5407 | Time 1.1978(1.2071) | Loss 3.145535(3.221008) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5408 | Time 1.1836(1.2055) | Loss 3.298790(3.226452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5409 | Time 1.1903(1.2044) | Loss 3.320019(3.233002) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5410 | Time 1.1864(1.2032) | Loss 3.218641(3.231997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5411 | Time 1.1835(1.2018) | Loss 3.217471(3.230980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5412 | Time 1.1824(1.2004) | Loss 3.267591(3.233543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5413 | Time 1.1817(1.1991) | Loss 3.225830(3.233003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5414 | Time 1.2411(1.2021) | Loss 3.260716(3.234943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5415 | Time 1.1836(1.2008) | Loss 3.241576(3.235407) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5416 | Time 1.2011(1.2008) | Loss 3.207786(3.233474) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5417 | Time 1.1994(1.2007) | Loss 3.196606(3.230893) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5418 | Time 1.1861(1.1997) | Loss 3.194347(3.228335) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5419 | Time 1.1758(1.1980) | Loss 3.254172(3.230143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5420 | Time 1.1898(1.1974) | Loss 3.351464(3.238636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5421 | Time 1.1722(1.1957) | Loss 3.187910(3.235085) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5422 | Time 1.1852(1.1949) | Loss 3.231194(3.234813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5423 | Time 1.1927(1.1948) | Loss 3.219799(3.233762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5424 | Time 1.1761(1.1935) | Loss 3.221270(3.232887) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5425 | Time 1.1800(1.1925) | Loss 3.255473(3.234468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5426 | Time 1.1908(1.1924) | Loss 3.191942(3.231491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5427 | Time 1.1965(1.1927) | Loss 3.212627(3.230171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5428 | Time 1.2276(1.1951) | Loss 3.138939(3.223785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5429 | Time 1.2229(1.1971) | Loss 3.219488(3.223484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5430 | Time 1.1938(1.1968) | Loss 3.242080(3.224786) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5431 | Time 1.1849(1.1960) | Loss 3.273914(3.228225) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5432 | Time 1.2031(1.1965) | Loss 3.237270(3.228858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5433 | Time 1.1836(1.1956) | Loss 3.210659(3.227584) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5434 | Time 1.1914(1.1953) | Loss 3.191525(3.225060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5435 | Time 1.2031(1.1959) | Loss 3.205895(3.223718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5436 | Time 1.2094(1.1968) | Loss 3.206288(3.222498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5437 | Time 1.2087(1.1976) | Loss 3.219954(3.222320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5438 | Time 1.1855(1.1968) | Loss 3.127290(3.215668) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5439 | Time 1.2134(1.1979) | Loss 3.201799(3.214697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5440 | Time 1.2001(1.1981) | Loss 3.292606(3.220151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5441 | Time 1.2121(1.1991) | Loss 3.216236(3.219877) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5442 | Time 1.2510(1.2027) | Loss 3.206711(3.218955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5443 | Time 1.2367(1.2051) | Loss 3.259957(3.221825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5444 | Time 1.4843(1.2246) | Loss 3.282914(3.226101) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5445 | Time 1.2827(1.2287) | Loss 3.188035(3.223437) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5446 | Time 1.2632(1.2311) | Loss 3.245527(3.224983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5447 | Time 1.2877(1.2351) | Loss 3.297292(3.230045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5448 | Time 1.2587(1.2367) | Loss 3.292621(3.234425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5449 | Time 1.2361(1.2367) | Loss 3.208785(3.232630) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5450 | Time 1.2450(1.2373) | Loss 3.270447(3.235277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5451 | Time 1.2433(1.2377) | Loss 3.256657(3.236774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5452 | Time 1.2780(1.2405) | Loss 3.281567(3.239909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5453 | Time 1.2731(1.2428) | Loss 3.192563(3.236595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5454 | Time 1.2569(1.2438) | Loss 3.186377(3.233080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5455 | Time 1.2655(1.2453) | Loss 3.223814(3.232431) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5456 | Time 1.3030(1.2493) | Loss 3.286148(3.236191) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5457 | Time 1.2935(1.2524) | Loss 3.251233(3.237244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5458 | Time 1.2475(1.2521) | Loss 3.249688(3.238115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5459 | Time 1.2740(1.2536) | Loss 3.219197(3.236791) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5460 | Time 1.2651(1.2544) | Loss 3.252002(3.237856) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5461 | Time 1.2682(1.2554) | Loss 3.206628(3.235670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5462 | Time 1.2631(1.2559) | Loss 3.218506(3.234468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5463 | Time 1.2471(1.2553) | Loss 3.102414(3.225225) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5464 | Time 1.2677(1.2562) | Loss 3.150918(3.220023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5465 | Time 1.2676(1.2570) | Loss 3.184801(3.217558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5466 | Time 1.2910(1.2594) | Loss 3.119367(3.210684) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5467 | Time 1.3057(1.2626) | Loss 3.201794(3.210062) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5468 | Time 1.3134(1.2662) | Loss 3.180543(3.207996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5469 | Time 1.2861(1.2676) | Loss 3.167245(3.205143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5470 | Time 1.2751(1.2681) | Loss 3.175427(3.203063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5471 | Time 1.2430(1.2663) | Loss 3.160800(3.200104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5472 | Time 1.2882(1.2679) | Loss 3.146034(3.196320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5473 | Time 1.3028(1.2703) | Loss 3.243641(3.199632) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5474 | Time 1.2833(1.2712) | Loss 3.215560(3.200747) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5475 | Time 1.2840(1.2721) | Loss 3.225897(3.202508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5476 | Time 1.2998(1.2740) | Loss 3.144104(3.198419) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5477 | Time 1.3136(1.2768) | Loss 3.203819(3.198797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5478 | Time 1.2973(1.2782) | Loss 3.142120(3.194830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5479 | Time 1.2805(1.2784) | Loss 3.210092(3.195898) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5480 | Time 1.2527(1.2766) | Loss 3.216972(3.197373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5481 | Time 1.2550(1.2751) | Loss 3.249540(3.201025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5482 | Time 1.2960(1.2766) | Loss 3.235658(3.203449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5483 | Time 1.2873(1.2773) | Loss 3.292080(3.209654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5484 | Time 1.2649(1.2764) | Loss 3.230012(3.211079) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5485 | Time 1.2474(1.2744) | Loss 3.258957(3.214430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5486 | Time 1.2559(1.2731) | Loss 3.323044(3.222033) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5487 | Time 1.2463(1.2712) | Loss 3.299330(3.227444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5488 | Time 1.2388(1.2690) | Loss 3.248499(3.228918) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5489 | Time 1.2117(1.2650) | Loss 3.289781(3.233178) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5490 | Time 1.1869(1.2595) | Loss 3.264684(3.235384) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5491 | Time 1.1839(1.2542) | Loss 3.341364(3.242802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5492 | Time 1.2029(1.2506) | Loss 3.239175(3.242548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5493 | Time 1.2066(1.2475) | Loss 3.287027(3.245662) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5494 | Time 1.2622(1.2486) | Loss 3.308208(3.250040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5495 | Time 1.2079(1.2457) | Loss 3.282710(3.252327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5496 | Time 1.2012(1.2426) | Loss 3.271886(3.253696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5497 | Time 1.2192(1.2410) | Loss 3.163017(3.247349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5498 | Time 1.2405(1.2409) | Loss 3.196261(3.243772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5499 | Time 1.1979(1.2379) | Loss 3.248338(3.244092) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5500 | Time 1.2063(1.2357) | Loss 3.178243(3.239483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5500 | Test Loss 3.199054 | NFE 20
Skipping vis as data dimension is >2
Iter 5501 | Time 1.2273(1.2351) | Loss 3.160857(3.233979) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5502 | Time 1.2085(1.2333) | Loss 3.190465(3.230933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5503 | Time 1.1866(1.2300) | Loss 3.198318(3.228650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5504 | Time 1.1985(1.2278) | Loss 3.182852(3.225444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5505 | Time 1.2032(1.2261) | Loss 3.132914(3.218967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5506 | Time 1.2157(1.2253) | Loss 3.129486(3.212703) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5507 | Time 1.2287(1.2256) | Loss 3.172121(3.209862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5508 | Time 1.2140(1.2248) | Loss 3.145607(3.205365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5509 | Time 1.2034(1.2233) | Loss 3.183399(3.203827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5510 | Time 1.2145(1.2226) | Loss 3.126427(3.198409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5511 | Time 1.2195(1.2224) | Loss 3.187325(3.197633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5512 | Time 1.2069(1.2213) | Loss 3.205437(3.198179) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5513 | Time 1.2396(1.2226) | Loss 3.237276(3.200916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5514 | Time 1.2084(1.2216) | Loss 3.143884(3.196924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5515 | Time 1.2200(1.2215) | Loss 3.214145(3.198129) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5516 | Time 1.2602(1.2242) | Loss 3.199835(3.198249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5517 | Time 1.2777(1.2280) | Loss 3.261890(3.202704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5518 | Time 1.2745(1.2312) | Loss 3.170020(3.200416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5519 | Time 1.2616(1.2333) | Loss 3.280748(3.206039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5520 | Time 1.2461(1.2342) | Loss 3.217171(3.206818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5521 | Time 1.2136(1.2328) | Loss 3.161158(3.203622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5522 | Time 1.2120(1.2313) | Loss 3.237594(3.206000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5523 | Time 1.2164(1.2303) | Loss 3.230916(3.207744) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5524 | Time 1.2316(1.2304) | Loss 3.282967(3.213010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5525 | Time 1.2385(1.2309) | Loss 3.271798(3.217125) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5526 | Time 1.2546(1.2326) | Loss 3.282286(3.221686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5527 | Time 1.2448(1.2335) | Loss 3.344849(3.230308) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5528 | Time 1.2454(1.2343) | Loss 3.185135(3.227146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5529 | Time 1.2567(1.2359) | Loss 3.250442(3.228776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5530 | Time 1.2071(1.2338) | Loss 3.267499(3.231487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5531 | Time 1.2082(1.2321) | Loss 3.269983(3.234182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5532 | Time 1.2342(1.2322) | Loss 3.262934(3.236194) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5533 | Time 1.2499(1.2334) | Loss 3.246873(3.236942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5534 | Time 1.2747(1.2363) | Loss 3.246806(3.237632) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5535 | Time 1.2511(1.2374) | Loss 3.263487(3.239442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5536 | Time 1.2413(1.2376) | Loss 3.279263(3.242230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5537 | Time 1.2348(1.2374) | Loss 3.238652(3.241979) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5538 | Time 1.2268(1.2367) | Loss 3.217230(3.240247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5539 | Time 1.2019(1.2343) | Loss 3.234449(3.239841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5540 | Time 1.2172(1.2331) | Loss 3.238399(3.239740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5541 | Time 1.2112(1.2315) | Loss 3.223953(3.238635) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5542 | Time 1.2130(1.2302) | Loss 3.291981(3.242369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5543 | Time 1.2418(1.2310) | Loss 3.175010(3.237654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5544 | Time 1.2580(1.2329) | Loss 3.186533(3.234076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5545 | Time 1.2495(1.2341) | Loss 3.156428(3.228640) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5546 | Time 1.2462(1.2349) | Loss 3.249166(3.230077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5547 | Time 1.2132(1.2334) | Loss 3.219210(3.229316) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5548 | Time 1.2382(1.2338) | Loss 3.225352(3.229039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5549 | Time 1.2447(1.2345) | Loss 3.212505(3.227881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5550 | Time 1.2812(1.2378) | Loss 3.146866(3.222210) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5551 | Time 1.2635(1.2396) | Loss 3.204447(3.220967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5552 | Time 1.2529(1.2405) | Loss 3.198265(3.219378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5553 | Time 1.3125(1.2456) | Loss 3.142511(3.213997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5554 | Time 1.2393(1.2451) | Loss 3.232717(3.215307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5555 | Time 1.2500(1.2455) | Loss 3.265034(3.218788) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5556 | Time 1.2532(1.2460) | Loss 3.257795(3.221519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5557 | Time 1.2471(1.2461) | Loss 3.221861(3.221543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5558 | Time 1.2589(1.2470) | Loss 3.215842(3.221144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5559 | Time 1.2443(1.2468) | Loss 3.225427(3.221444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5560 | Time 1.2495(1.2470) | Loss 3.265433(3.224523) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5561 | Time 1.2791(1.2492) | Loss 3.286161(3.228837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5562 | Time 1.2813(1.2515) | Loss 3.212070(3.227664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5563 | Time 1.2774(1.2533) | Loss 3.159295(3.222878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5564 | Time 1.2864(1.2556) | Loss 3.209256(3.221924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5565 | Time 1.2443(1.2548) | Loss 3.237184(3.222993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5566 | Time 1.2610(1.2552) | Loss 3.213202(3.222307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5567 | Time 1.2307(1.2535) | Loss 3.131286(3.215936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5568 | Time 1.2327(1.2521) | Loss 3.264349(3.219325) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5569 | Time 1.2919(1.2549) | Loss 3.193135(3.217491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5570 | Time 1.2751(1.2563) | Loss 3.067744(3.207009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5571 | Time 1.2551(1.2562) | Loss 3.151030(3.203091) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5572 | Time 1.2426(1.2552) | Loss 3.163006(3.200285) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5573 | Time 1.2563(1.2553) | Loss 3.148552(3.196663) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5574 | Time 1.2038(1.2517) | Loss 3.181381(3.195594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5575 | Time 1.2316(1.2503) | Loss 3.135871(3.191413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5576 | Time 1.2464(1.2500) | Loss 3.142118(3.187962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5577 | Time 1.2899(1.2528) | Loss 3.139892(3.184597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5578 | Time 1.2484(1.2525) | Loss 3.170293(3.183596) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5579 | Time 1.2577(1.2529) | Loss 3.179685(3.183322) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5580 | Time 1.2679(1.2539) | Loss 3.195209(3.184154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5581 | Time 1.2970(1.2569) | Loss 3.240406(3.188092) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5582 | Time 1.2077(1.2535) | Loss 3.165796(3.186531) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5583 | Time 1.2057(1.2501) | Loss 3.264559(3.191993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5584 | Time 1.2031(1.2469) | Loss 3.265134(3.197113) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5585 | Time 1.2537(1.2473) | Loss 3.371332(3.209308) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5586 | Time 1.2235(1.2457) | Loss 3.230076(3.210762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5587 | Time 1.2205(1.2439) | Loss 3.331560(3.219218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5588 | Time 1.2272(1.2427) | Loss 3.247392(3.221190) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5589 | Time 1.2254(1.2415) | Loss 3.329354(3.228762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5590 | Time 1.2044(1.2389) | Loss 3.368276(3.238528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5591 | Time 1.2061(1.2366) | Loss 3.379692(3.248409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5592 | Time 1.2123(1.2349) | Loss 3.240454(3.247852) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5593 | Time 1.2247(1.2342) | Loss 3.321324(3.252995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5594 | Time 1.2141(1.2328) | Loss 3.343437(3.259326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5595 | Time 1.2148(1.2315) | Loss 3.309992(3.262873) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5596 | Time 1.2081(1.2299) | Loss 3.347996(3.268831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5597 | Time 1.1919(1.2272) | Loss 3.323270(3.272642) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5598 | Time 1.2137(1.2263) | Loss 3.330685(3.276705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5599 | Time 1.2110(1.2252) | Loss 3.271413(3.276335) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5600 | Time 1.2193(1.2248) | Loss 3.272874(3.276092) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5600 | Test Loss 3.277444 | NFE 20
Skipping vis as data dimension is >2
Iter 5601 | Time 1.2247(1.2248) | Loss 3.273717(3.275926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5602 | Time 1.2055(1.2234) | Loss 3.275302(3.275882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5603 | Time 1.2289(1.2238) | Loss 3.211551(3.271379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5604 | Time 1.2390(1.2249) | Loss 3.206735(3.266854) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5605 | Time 1.2259(1.2250) | Loss 3.222258(3.263732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5606 | Time 1.2451(1.2264) | Loss 3.220114(3.260679) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5607 | Time 1.2409(1.2274) | Loss 3.213843(3.257401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5608 | Time 1.2212(1.2270) | Loss 3.175910(3.251696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5609 | Time 1.2018(1.2252) | Loss 3.200800(3.248134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5610 | Time 1.2432(1.2265) | Loss 3.184206(3.243659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5611 | Time 1.2212(1.2261) | Loss 3.173544(3.238751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5612 | Time 1.2258(1.2261) | Loss 3.162770(3.233432) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5613 | Time 1.2322(1.2265) | Loss 3.177023(3.229483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5614 | Time 1.2066(1.2251) | Loss 3.194973(3.227068) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5615 | Time 1.1921(1.2228) | Loss 3.196758(3.224946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5616 | Time 1.1863(1.2202) | Loss 3.247913(3.226554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5617 | Time 1.2203(1.2202) | Loss 3.244318(3.227797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5618 | Time 1.2170(1.2200) | Loss 3.238220(3.228527) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5619 | Time 1.2093(1.2193) | Loss 3.181570(3.225240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5620 | Time 1.2537(1.2217) | Loss 3.248236(3.226849) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5621 | Time 1.1986(1.2201) | Loss 3.268423(3.229760) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5622 | Time 1.1945(1.2183) | Loss 3.209639(3.228351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5623 | Time 1.2102(1.2177) | Loss 3.176685(3.224735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5624 | Time 1.2128(1.2174) | Loss 3.171608(3.221016) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5625 | Time 1.1947(1.2158) | Loss 3.174190(3.217738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5626 | Time 1.2085(1.2153) | Loss 3.349082(3.226932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5627 | Time 1.1893(1.2135) | Loss 3.283566(3.230896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5628 | Time 1.1834(1.2114) | Loss 3.326135(3.237563) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5629 | Time 1.2039(1.2108) | Loss 3.305138(3.242293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5630 | Time 1.1864(1.2091) | Loss 3.272777(3.244427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5631 | Time 1.1887(1.2077) | Loss 3.293990(3.247897) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5632 | Time 1.1820(1.2059) | Loss 3.332725(3.253835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5633 | Time 1.1926(1.2050) | Loss 3.284465(3.255979) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5634 | Time 1.1973(1.2044) | Loss 3.266675(3.256727) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5635 | Time 1.1594(1.2013) | Loss 3.210165(3.253468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5636 | Time 1.1606(1.1984) | Loss 3.244570(3.252845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5637 | Time 1.1694(1.1964) | Loss 3.177296(3.247557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5638 | Time 1.1453(1.1928) | Loss 3.222339(3.245792) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5639 | Time 1.1587(1.1904) | Loss 3.266944(3.247272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5640 | Time 1.1643(1.1886) | Loss 3.207066(3.244458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5641 | Time 1.1571(1.1864) | Loss 3.212270(3.242205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5642 | Time 1.1596(1.1845) | Loss 3.268567(3.244050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5643 | Time 1.1550(1.1825) | Loss 3.233363(3.243302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5644 | Time 1.1452(1.1798) | Loss 3.164215(3.237766) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5645 | Time 1.1432(1.1773) | Loss 3.177963(3.233580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5646 | Time 1.1862(1.1779) | Loss 3.083481(3.223073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5647 | Time 1.2002(1.1795) | Loss 3.124386(3.216165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5648 | Time 1.1778(1.1793) | Loss 3.216865(3.216214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5649 | Time 1.1892(1.1800) | Loss 3.164591(3.212600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5650 | Time 1.1803(1.1800) | Loss 3.149562(3.208187) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5651 | Time 1.1815(1.1801) | Loss 3.140565(3.203454) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5652 | Time 1.1670(1.1792) | Loss 3.234924(3.205657) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5653 | Time 1.1590(1.1778) | Loss 3.177622(3.203694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5654 | Time 1.2115(1.1802) | Loss 3.196732(3.203207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5655 | Time 1.1874(1.1807) | Loss 3.164371(3.200488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5656 | Time 1.1717(1.1801) | Loss 3.161649(3.197770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5657 | Time 1.1815(1.1802) | Loss 3.266472(3.202579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5658 | Time 1.1736(1.1797) | Loss 3.190350(3.201723) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5659 | Time 1.1728(1.1792) | Loss 3.142457(3.197574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5660 | Time 1.1831(1.1795) | Loss 3.191615(3.197157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5661 | Time 1.1780(1.1794) | Loss 3.193670(3.196913) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5662 | Time 1.1694(1.1787) | Loss 3.177860(3.195579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5663 | Time 1.1698(1.1781) | Loss 3.155583(3.192780) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5664 | Time 1.1500(1.1761) | Loss 3.292423(3.199755) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5665 | Time 1.1660(1.1754) | Loss 3.210064(3.200476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5666 | Time 1.1688(1.1749) | Loss 3.299838(3.207432) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5667 | Time 1.1766(1.1750) | Loss 3.321677(3.215429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5668 | Time 1.1703(1.1747) | Loss 3.307681(3.221886) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5669 | Time 1.1744(1.1747) | Loss 3.308847(3.227974) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5670 | Time 1.1599(1.1736) | Loss 3.236521(3.228572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5671 | Time 1.1689(1.1733) | Loss 3.251902(3.230205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5672 | Time 1.1817(1.1739) | Loss 3.188774(3.227305) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5673 | Time 1.1615(1.1730) | Loss 3.249075(3.228829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5674 | Time 1.1597(1.1721) | Loss 3.150742(3.223363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5675 | Time 1.1627(1.1714) | Loss 3.206438(3.222178) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5676 | Time 1.2132(1.1744) | Loss 3.201376(3.220722) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5677 | Time 1.2219(1.1777) | Loss 3.231864(3.221502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5678 | Time 1.2270(1.1811) | Loss 3.259225(3.224142) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5679 | Time 1.1872(1.1816) | Loss 3.295123(3.229111) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5680 | Time 1.1847(1.1818) | Loss 3.252681(3.230761) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5681 | Time 1.1974(1.1829) | Loss 3.125985(3.223427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5682 | Time 1.2001(1.1841) | Loss 3.242896(3.224789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5683 | Time 1.1877(1.1843) | Loss 3.208514(3.223650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5684 | Time 1.1997(1.1854) | Loss 3.214885(3.223037) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5685 | Time 1.1805(1.1851) | Loss 3.171463(3.219426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5686 | Time 1.2108(1.1869) | Loss 3.136220(3.213602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5687 | Time 1.2956(1.1945) | Loss 3.199129(3.212589) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5688 | Time 1.2968(1.2016) | Loss 3.205560(3.212097) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5689 | Time 1.2789(1.2070) | Loss 3.227475(3.213173) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5690 | Time 1.2978(1.2134) | Loss 3.305192(3.219615) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5691 | Time 1.2242(1.2142) | Loss 3.211105(3.219019) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5692 | Time 1.2517(1.2168) | Loss 3.257663(3.221724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5693 | Time 1.2378(1.2183) | Loss 3.229182(3.222246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5694 | Time 1.2171(1.2182) | Loss 3.230362(3.222814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5695 | Time 1.2218(1.2184) | Loss 3.182086(3.219963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5696 | Time 1.2239(1.2188) | Loss 3.247981(3.221924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5697 | Time 1.2014(1.2176) | Loss 3.224801(3.222126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5698 | Time 1.2246(1.2181) | Loss 3.169247(3.218424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5699 | Time 1.1969(1.2166) | Loss 3.249619(3.220608) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5700 | Time 1.1948(1.2151) | Loss 3.199958(3.219162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5700 | Test Loss 3.305277 | NFE 20
Skipping vis as data dimension is >2
Iter 5701 | Time 1.2189(1.2153) | Loss 3.254301(3.221622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5702 | Time 1.2133(1.2152) | Loss 3.165107(3.217666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5703 | Time 1.2268(1.2160) | Loss 3.234150(3.218820) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5704 | Time 1.2254(1.2167) | Loss 3.281292(3.223193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5705 | Time 1.2147(1.2165) | Loss 3.252108(3.225217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5706 | Time 1.2448(1.2185) | Loss 3.235734(3.225953) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5707 | Time 1.2154(1.2183) | Loss 3.260517(3.228373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5708 | Time 1.2357(1.2195) | Loss 3.331819(3.235614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5709 | Time 1.2125(1.2190) | Loss 3.318997(3.241451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5710 | Time 1.2051(1.2180) | Loss 3.272243(3.243606) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5711 | Time 1.1992(1.2167) | Loss 3.319515(3.248920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5712 | Time 1.1949(1.2152) | Loss 3.240830(3.248353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5713 | Time 1.1949(1.2138) | Loss 3.393940(3.258545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5714 | Time 1.2194(1.2142) | Loss 3.315864(3.262557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5715 | Time 1.2034(1.2134) | Loss 3.310656(3.265924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5716 | Time 1.1963(1.2122) | Loss 3.342670(3.271296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5717 | Time 1.2472(1.2147) | Loss 3.325699(3.275104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5718 | Time 1.2313(1.2158) | Loss 3.263590(3.274298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5719 | Time 1.2268(1.2166) | Loss 3.261622(3.273411) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5720 | Time 1.2252(1.2172) | Loss 3.333983(3.277651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5721 | Time 1.2102(1.2167) | Loss 3.261905(3.276549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5722 | Time 1.1920(1.2150) | Loss 3.196856(3.270970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5723 | Time 1.1877(1.2131) | Loss 3.260618(3.270246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5724 | Time 1.1786(1.2107) | Loss 3.194812(3.264965) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5725 | Time 1.1823(1.2087) | Loss 3.210341(3.261142) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5726 | Time 1.1915(1.2075) | Loss 3.181149(3.255542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5727 | Time 1.1816(1.2057) | Loss 3.153613(3.248407) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5728 | Time 1.1839(1.2041) | Loss 3.167780(3.242763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5729 | Time 1.2212(1.2053) | Loss 3.178616(3.238273) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5730 | Time 1.2040(1.2052) | Loss 3.156524(3.232550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5731 | Time 1.2006(1.2049) | Loss 3.202573(3.230452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5732 | Time 1.2153(1.2056) | Loss 3.146702(3.224590) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5733 | Time 1.2107(1.2060) | Loss 3.133014(3.218179) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5734 | Time 1.1993(1.2055) | Loss 3.172324(3.214969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5735 | Time 1.2036(1.2054) | Loss 3.209025(3.214553) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5736 | Time 1.2123(1.2059) | Loss 3.179902(3.212128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5737 | Time 1.1897(1.2047) | Loss 3.250568(3.214819) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5738 | Time 1.1962(1.2041) | Loss 3.277240(3.219188) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5739 | Time 1.2070(1.2043) | Loss 3.292387(3.224312) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5740 | Time 1.2194(1.2054) | Loss 3.237125(3.225209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5741 | Time 1.1885(1.2042) | Loss 3.192979(3.222953) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5742 | Time 1.1948(1.2036) | Loss 3.238710(3.224056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5743 | Time 1.2067(1.2038) | Loss 3.185511(3.221358) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5744 | Time 1.2132(1.2044) | Loss 3.164737(3.217394) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5745 | Time 1.2168(1.2053) | Loss 3.216429(3.217327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5746 | Time 1.2148(1.2060) | Loss 3.234737(3.218545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5747 | Time 1.2032(1.2058) | Loss 3.236630(3.219811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5748 | Time 1.2280(1.2073) | Loss 3.177025(3.216816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5749 | Time 1.2035(1.2071) | Loss 3.256551(3.219598) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5750 | Time 1.1947(1.2062) | Loss 3.236322(3.220768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5751 | Time 1.1865(1.2048) | Loss 3.308634(3.226919) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5752 | Time 1.2038(1.2047) | Loss 3.162766(3.222428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5753 | Time 1.1962(1.2042) | Loss 3.173102(3.218976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5754 | Time 1.2066(1.2043) | Loss 3.162949(3.215054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5755 | Time 1.1954(1.2037) | Loss 3.222130(3.215549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5756 | Time 1.2254(1.2052) | Loss 3.183980(3.213339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5757 | Time 1.2372(1.2075) | Loss 3.175757(3.210708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5758 | Time 1.2092(1.2076) | Loss 3.144245(3.206056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5759 | Time 1.2267(1.2089) | Loss 3.158369(3.202718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5760 | Time 1.2045(1.2086) | Loss 3.209214(3.203173) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5761 | Time 1.2485(1.2114) | Loss 3.137873(3.198602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5762 | Time 1.2352(1.2131) | Loss 3.221154(3.200180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5763 | Time 1.2376(1.2148) | Loss 3.152822(3.196865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5764 | Time 1.2019(1.2139) | Loss 3.172624(3.195168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5765 | Time 1.2184(1.2142) | Loss 3.155003(3.192357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5766 | Time 1.2515(1.2168) | Loss 3.201974(3.193030) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5767 | Time 1.2349(1.2181) | Loss 3.191922(3.192952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5768 | Time 1.2328(1.2191) | Loss 3.226079(3.195271) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5769 | Time 1.2149(1.2188) | Loss 3.268019(3.200364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5770 | Time 1.2275(1.2194) | Loss 3.156677(3.197306) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5771 | Time 1.1995(1.2180) | Loss 3.231381(3.199691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5772 | Time 1.1893(1.2160) | Loss 3.284175(3.205605) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5773 | Time 1.2002(1.2149) | Loss 3.254928(3.209057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5774 | Time 1.1697(1.2117) | Loss 3.230547(3.210562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5775 | Time 1.1681(1.2087) | Loss 3.265651(3.214418) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5776 | Time 1.1946(1.2077) | Loss 3.183012(3.212219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5777 | Time 1.1869(1.2062) | Loss 3.194450(3.210976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5778 | Time 1.1978(1.2057) | Loss 3.273592(3.215359) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5779 | Time 1.2072(1.2058) | Loss 3.208493(3.214878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5780 | Time 1.1942(1.2049) | Loss 3.168209(3.211611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5781 | Time 1.1795(1.2032) | Loss 3.222837(3.212397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5782 | Time 1.1714(1.2009) | Loss 3.191758(3.210952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5783 | Time 1.1775(1.1993) | Loss 3.217770(3.211430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5784 | Time 1.1901(1.1987) | Loss 3.104643(3.203955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5785 | Time 1.1999(1.1988) | Loss 3.194184(3.203271) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5786 | Time 1.1927(1.1983) | Loss 3.121132(3.197521) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5787 | Time 1.2057(1.1988) | Loss 3.229277(3.199744) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5788 | Time 1.1935(1.1985) | Loss 3.298017(3.206623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5789 | Time 1.1877(1.1977) | Loss 3.159352(3.203314) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5790 | Time 1.2405(1.2007) | Loss 3.179159(3.201623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5791 | Time 1.2187(1.2020) | Loss 3.158443(3.198601) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5792 | Time 1.2277(1.2038) | Loss 3.258171(3.202771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5793 | Time 1.2272(1.2054) | Loss 3.216800(3.203753) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5794 | Time 1.1948(1.2047) | Loss 3.204990(3.203839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5795 | Time 1.2121(1.2052) | Loss 3.184769(3.202504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5796 | Time 1.2043(1.2051) | Loss 3.215354(3.203404) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5797 | Time 1.2198(1.2062) | Loss 3.256144(3.207096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5798 | Time 1.2052(1.2061) | Loss 3.133349(3.201933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5799 | Time 1.2341(1.2080) | Loss 3.214513(3.202814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5800 | Time 1.2069(1.2080) | Loss 3.200154(3.202628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5800 | Test Loss 3.186871 | NFE 20
Skipping vis as data dimension is >2
Iter 5801 | Time 1.1967(1.2072) | Loss 3.209036(3.203076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5802 | Time 1.1804(1.2053) | Loss 3.189766(3.202145) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5803 | Time 1.1793(1.2035) | Loss 3.224858(3.203735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5804 | Time 1.1719(1.2013) | Loss 3.253441(3.207214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5805 | Time 1.1721(1.1992) | Loss 3.160961(3.203976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5806 | Time 1.1957(1.1990) | Loss 3.268795(3.208514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5807 | Time 1.1795(1.1976) | Loss 3.243662(3.210974) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5808 | Time 1.2129(1.1987) | Loss 3.232502(3.212481) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5809 | Time 1.1983(1.1987) | Loss 3.136845(3.207186) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5810 | Time 1.1756(1.1970) | Loss 3.140058(3.202487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5811 | Time 1.1675(1.1950) | Loss 3.252651(3.205999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5812 | Time 1.1725(1.1934) | Loss 3.161942(3.202915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5813 | Time 1.1895(1.1931) | Loss 3.170946(3.200677) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5814 | Time 1.2042(1.1939) | Loss 3.211951(3.201466) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5815 | Time 1.1957(1.1940) | Loss 3.086797(3.193439) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5816 | Time 1.2017(1.1946) | Loss 3.164793(3.191434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5817 | Time 1.2304(1.1971) | Loss 3.112097(3.185881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5818 | Time 1.2088(1.1979) | Loss 3.129001(3.181899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5819 | Time 1.2088(1.1987) | Loss 3.222246(3.184723) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5820 | Time 1.2133(1.1997) | Loss 3.168697(3.183601) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5821 | Time 1.2102(1.2004) | Loss 3.213520(3.185696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5822 | Time 1.2116(1.2012) | Loss 3.150144(3.183207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5823 | Time 1.2265(1.2030) | Loss 3.241347(3.187277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5824 | Time 1.2010(1.2028) | Loss 3.110065(3.181872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5825 | Time 1.1868(1.2017) | Loss 3.240383(3.185968) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5826 | Time 1.1820(1.2003) | Loss 3.202848(3.187149) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5827 | Time 1.1793(1.1989) | Loss 3.136084(3.183575) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5828 | Time 1.2005(1.1990) | Loss 3.251344(3.188319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5829 | Time 1.1944(1.1987) | Loss 3.199626(3.189110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5830 | Time 1.1704(1.1967) | Loss 3.248217(3.193248) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5831 | Time 1.1652(1.1945) | Loss 3.207821(3.194268) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5832 | Time 1.1865(1.1939) | Loss 3.149331(3.191122) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5833 | Time 1.1806(1.1930) | Loss 3.174829(3.189982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5834 | Time 1.1657(1.1911) | Loss 3.249106(3.194120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5835 | Time 1.1650(1.1892) | Loss 3.182202(3.193286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5836 | Time 1.1630(1.1874) | Loss 3.167631(3.191490) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5837 | Time 1.1980(1.1881) | Loss 3.249045(3.195519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5838 | Time 1.1778(1.1874) | Loss 3.163066(3.193247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5839 | Time 1.2226(1.1899) | Loss 3.157781(3.190765) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5840 | Time 1.2142(1.1916) | Loss 3.218557(3.192710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5841 | Time 1.2068(1.1927) | Loss 3.182842(3.192019) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5842 | Time 1.1991(1.1931) | Loss 3.158683(3.189686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5843 | Time 1.2090(1.1942) | Loss 3.135313(3.185880) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5844 | Time 1.1918(1.1941) | Loss 3.136896(3.182451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5845 | Time 1.2172(1.1957) | Loss 3.141994(3.179619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5846 | Time 1.2051(1.1963) | Loss 3.146403(3.177294) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5847 | Time 1.2065(1.1970) | Loss 3.171811(3.176910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5848 | Time 1.2151(1.1983) | Loss 3.183915(3.177400) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5849 | Time 1.2147(1.1995) | Loss 3.118354(3.173267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5850 | Time 1.2380(1.2022) | Loss 3.145882(3.171350) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5851 | Time 1.1848(1.2009) | Loss 3.108677(3.166963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5852 | Time 1.1775(1.1993) | Loss 3.133553(3.164624) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5853 | Time 1.1890(1.1986) | Loss 3.113538(3.161048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5854 | Time 1.1989(1.1986) | Loss 3.132132(3.159024) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5855 | Time 1.2269(1.2006) | Loss 3.125498(3.156677) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5856 | Time 1.2637(1.2050) | Loss 3.091028(3.152082) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5857 | Time 1.2560(1.2086) | Loss 3.126121(3.150265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5858 | Time 1.2541(1.2118) | Loss 3.078293(3.145227) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5859 | Time 1.1896(1.2102) | Loss 3.130987(3.144230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5860 | Time 1.2230(1.2111) | Loss 3.107118(3.141632) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5861 | Time 1.2351(1.2128) | Loss 3.122935(3.140323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5862 | Time 1.2064(1.2123) | Loss 3.149003(3.140931) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5863 | Time 1.1897(1.2108) | Loss 3.168303(3.142847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5864 | Time 1.1907(1.2093) | Loss 3.206772(3.147322) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5865 | Time 1.1882(1.2079) | Loss 3.152871(3.147710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5866 | Time 1.1863(1.2064) | Loss 3.179530(3.149937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5867 | Time 1.2041(1.2062) | Loss 3.289656(3.159718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5868 | Time 1.1891(1.2050) | Loss 3.222795(3.164133) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5869 | Time 1.1928(1.2042) | Loss 3.247214(3.169949) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5870 | Time 1.2404(1.2067) | Loss 3.260435(3.176283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5871 | Time 1.2295(1.2083) | Loss 3.299825(3.184931) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5872 | Time 1.2327(1.2100) | Loss 3.192840(3.185484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5873 | Time 1.1949(1.2089) | Loss 3.218278(3.187780) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5874 | Time 1.2130(1.2092) | Loss 3.242941(3.191641) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5875 | Time 1.2109(1.2093) | Loss 3.249063(3.195661) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5876 | Time 1.2183(1.2100) | Loss 3.205935(3.196380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5877 | Time 1.2195(1.2106) | Loss 3.306915(3.204117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5878 | Time 1.2135(1.2108) | Loss 3.280089(3.209435) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5879 | Time 1.2222(1.2116) | Loss 3.222524(3.210352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5880 | Time 1.2094(1.2115) | Loss 3.185929(3.208642) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5881 | Time 1.2187(1.2120) | Loss 3.228718(3.210047) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5882 | Time 1.2416(1.2141) | Loss 3.239424(3.212104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5883 | Time 1.2365(1.2156) | Loss 3.198005(3.211117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5884 | Time 1.2192(1.2159) | Loss 3.206787(3.210814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5885 | Time 1.2300(1.2169) | Loss 3.140543(3.205895) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5886 | Time 1.2393(1.2184) | Loss 3.157411(3.202501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5887 | Time 1.2172(1.2183) | Loss 3.170732(3.200277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5888 | Time 1.2077(1.2176) | Loss 3.217544(3.201486) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5889 | Time 1.2022(1.2165) | Loss 3.128126(3.196351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5890 | Time 1.2017(1.2155) | Loss 3.171126(3.194585) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5891 | Time 1.2274(1.2163) | Loss 3.166255(3.192602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5892 | Time 1.2030(1.2154) | Loss 3.165957(3.190737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5893 | Time 1.2020(1.2144) | Loss 3.164910(3.188929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5894 | Time 1.2063(1.2139) | Loss 3.210306(3.190425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5895 | Time 1.2132(1.2138) | Loss 3.134901(3.186538) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5896 | Time 1.2024(1.2130) | Loss 3.142016(3.183422) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5897 | Time 1.2017(1.2122) | Loss 3.197881(3.184434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5898 | Time 1.1987(1.2113) | Loss 3.230399(3.187652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5899 | Time 1.2325(1.2128) | Loss 3.174516(3.186732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5900 | Time 1.2050(1.2122) | Loss 3.161038(3.184934) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 5900 | Test Loss 3.171528 | NFE 20
Skipping vis as data dimension is >2
Iter 5901 | Time 1.2218(1.2129) | Loss 3.204194(3.186282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5902 | Time 1.2655(1.2166) | Loss 3.152300(3.183903) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5903 | Time 1.2443(1.2185) | Loss 3.200408(3.185058) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5904 | Time 1.2252(1.2190) | Loss 3.235775(3.188609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5905 | Time 1.2735(1.2228) | Loss 3.240448(3.192237) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5906 | Time 1.2352(1.2237) | Loss 3.192446(3.192252) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5907 | Time 1.2199(1.2234) | Loss 3.128432(3.187785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5908 | Time 1.2055(1.2222) | Loss 3.258399(3.192728) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5909 | Time 1.1783(1.2191) | Loss 3.178313(3.191719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5910 | Time 1.1886(1.2169) | Loss 3.188797(3.191514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5911 | Time 1.2779(1.2212) | Loss 3.221975(3.193646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5912 | Time 1.2722(1.2248) | Loss 3.219984(3.195490) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5913 | Time 1.2284(1.2250) | Loss 3.207178(3.196308) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5914 | Time 1.1984(1.2232) | Loss 3.212136(3.197416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5915 | Time 1.1867(1.2206) | Loss 3.162371(3.194963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5916 | Time 1.1982(1.2191) | Loss 3.128608(3.190318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5917 | Time 1.1973(1.2175) | Loss 3.177469(3.189419) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5918 | Time 1.2027(1.2165) | Loss 3.177378(3.188576) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5919 | Time 1.2013(1.2154) | Loss 3.180016(3.187977) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5920 | Time 1.2506(1.2179) | Loss 3.167488(3.186542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5921 | Time 1.2660(1.2213) | Loss 3.133350(3.182819) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5922 | Time 1.2653(1.2243) | Loss 3.112021(3.177863) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5923 | Time 1.2067(1.2231) | Loss 3.129983(3.174512) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5924 | Time 1.1992(1.2214) | Loss 3.181673(3.175013) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5925 | Time 1.2361(1.2225) | Loss 3.192034(3.176204) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5926 | Time 1.2277(1.2228) | Loss 3.159628(3.175044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5927 | Time 1.2212(1.2227) | Loss 3.184786(3.175726) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5928 | Time 1.2187(1.2224) | Loss 3.151777(3.174049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5929 | Time 1.2236(1.2225) | Loss 3.118152(3.170137) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5930 | Time 1.2223(1.2225) | Loss 3.106991(3.165716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5931 | Time 1.2283(1.2229) | Loss 3.183560(3.166966) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5932 | Time 1.2263(1.2231) | Loss 3.190819(3.168635) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5933 | Time 1.2329(1.2238) | Loss 3.138723(3.166541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5934 | Time 1.1997(1.2221) | Loss 3.114539(3.162901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5935 | Time 1.2158(1.2217) | Loss 3.148939(3.161924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5936 | Time 1.2010(1.2203) | Loss 3.159608(3.161762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5937 | Time 1.1971(1.2186) | Loss 3.088353(3.156623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5938 | Time 1.1885(1.2165) | Loss 3.026738(3.147531) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5939 | Time 1.1934(1.2149) | Loss 3.123255(3.145832) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5940 | Time 1.1946(1.2135) | Loss 3.236001(3.152144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5941 | Time 1.1981(1.2124) | Loss 3.198055(3.155357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5942 | Time 1.2057(1.2119) | Loss 3.236625(3.161046) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5943 | Time 1.2275(1.2130) | Loss 3.236434(3.166323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5944 | Time 1.2221(1.2137) | Loss 3.243841(3.171750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5945 | Time 1.2136(1.2137) | Loss 3.200697(3.173776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5946 | Time 1.2262(1.2145) | Loss 3.204244(3.175909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5947 | Time 1.2250(1.2153) | Loss 3.083864(3.169466) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5948 | Time 1.2105(1.2149) | Loss 3.202531(3.171780) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5949 | Time 1.2197(1.2153) | Loss 3.180462(3.172388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5950 | Time 1.2219(1.2157) | Loss 3.099968(3.167319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5951 | Time 1.2210(1.2161) | Loss 3.162131(3.166955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5952 | Time 1.2525(1.2186) | Loss 3.154164(3.166060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5953 | Time 1.2486(1.2207) | Loss 3.152799(3.165132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5954 | Time 1.2212(1.2208) | Loss 3.124537(3.162290) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5955 | Time 1.2330(1.2216) | Loss 3.092310(3.157392) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5956 | Time 1.2622(1.2245) | Loss 3.145143(3.156534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5957 | Time 1.2360(1.2253) | Loss 3.110484(3.153311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5958 | Time 1.2303(1.2256) | Loss 3.141909(3.152512) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5959 | Time 1.2161(1.2250) | Loss 3.019648(3.143212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5960 | Time 1.2097(1.2239) | Loss 3.118308(3.141469) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5961 | Time 1.2609(1.2265) | Loss 3.143220(3.141591) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5962 | Time 1.2054(1.2250) | Loss 3.143606(3.141732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5963 | Time 1.2267(1.2251) | Loss 3.045884(3.135023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5964 | Time 1.2095(1.2240) | Loss 3.053819(3.129339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5965 | Time 1.2154(1.2234) | Loss 3.105936(3.127700) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5966 | Time 1.2094(1.2224) | Loss 3.073508(3.123907) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5967 | Time 1.2572(1.2249) | Loss 3.151637(3.125848) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5968 | Time 1.2766(1.2285) | Loss 3.123162(3.125660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5969 | Time 1.2214(1.2280) | Loss 3.114571(3.124884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5970 | Time 1.2053(1.2264) | Loss 3.164761(3.127675) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5971 | Time 1.2063(1.2250) | Loss 3.115293(3.126808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5972 | Time 1.2076(1.2238) | Loss 3.108747(3.125544) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5973 | Time 1.1976(1.2219) | Loss 3.197101(3.130553) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5974 | Time 1.1877(1.2196) | Loss 3.112730(3.129305) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5975 | Time 1.1666(1.2158) | Loss 3.167719(3.131994) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5976 | Time 1.1735(1.2129) | Loss 3.162305(3.134116) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5977 | Time 1.1838(1.2108) | Loss 3.259751(3.142911) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5978 | Time 1.1719(1.2081) | Loss 3.145790(3.143112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5979 | Time 1.1989(1.2075) | Loss 3.156874(3.144075) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5980 | Time 1.2073(1.2075) | Loss 3.258392(3.152078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5981 | Time 1.2204(1.2084) | Loss 3.161547(3.152741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5982 | Time 1.2134(1.2087) | Loss 3.164080(3.153534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5983 | Time 1.1803(1.2067) | Loss 3.179781(3.155372) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5984 | Time 1.1755(1.2045) | Loss 3.160008(3.155696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5985 | Time 1.1767(1.2026) | Loss 3.170642(3.156742) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5986 | Time 1.1850(1.2014) | Loss 3.149005(3.156201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5987 | Time 1.1764(1.1996) | Loss 3.158155(3.156338) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5988 | Time 1.1765(1.1980) | Loss 3.218449(3.160685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5989 | Time 1.1801(1.1968) | Loss 3.195904(3.163151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5990 | Time 1.1803(1.1956) | Loss 3.127044(3.160623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5991 | Time 1.1837(1.1948) | Loss 3.175330(3.161653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5992 | Time 1.1880(1.1943) | Loss 3.169295(3.162188) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5993 | Time 1.2079(1.1952) | Loss 3.135756(3.160337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5994 | Time 1.1975(1.1954) | Loss 3.122440(3.157685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5995 | Time 1.2033(1.1960) | Loss 3.128467(3.155639) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5996 | Time 1.1899(1.1955) | Loss 3.037644(3.147380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5997 | Time 1.1824(1.1946) | Loss 3.060188(3.141276) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5998 | Time 1.1884(1.1942) | Loss 3.107686(3.138925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 5999 | Time 1.1891(1.1938) | Loss 3.106449(3.136652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6000 | Time 1.1840(1.1931) | Loss 3.150603(3.137628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6000 | Test Loss 3.152617 | NFE 20
Skipping vis as data dimension is >2
Iter 6001 | Time 1.1875(1.1927) | Loss 3.050884(3.131556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6002 | Time 1.2020(1.1934) | Loss 3.033608(3.124700) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6003 | Time 1.1913(1.1932) | Loss 3.107084(3.123467) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6004 | Time 1.1910(1.1931) | Loss 3.150924(3.125389) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6005 | Time 1.2172(1.1948) | Loss 3.054193(3.120405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6006 | Time 1.2299(1.1972) | Loss 3.103585(3.119227) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6007 | Time 1.2212(1.1989) | Loss 3.096675(3.117649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6008 | Time 1.2044(1.1993) | Loss 3.097791(3.116259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6009 | Time 1.1950(1.1990) | Loss 3.112810(3.116017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6010 | Time 1.1956(1.1988) | Loss 3.091485(3.114300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6011 | Time 1.1853(1.1978) | Loss 3.111429(3.114099) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6012 | Time 1.1827(1.1968) | Loss 3.157982(3.117171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6013 | Time 1.1965(1.1967) | Loss 3.157728(3.120010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6014 | Time 1.1933(1.1965) | Loss 3.147608(3.121942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6015 | Time 1.1877(1.1959) | Loss 3.154497(3.124221) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6016 | Time 1.1923(1.1956) | Loss 3.223073(3.131140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6017 | Time 1.1888(1.1951) | Loss 3.304248(3.143258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6018 | Time 1.2530(1.1992) | Loss 3.218389(3.148517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6019 | Time 1.2580(1.2033) | Loss 3.169633(3.149995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6020 | Time 1.1899(1.2024) | Loss 3.220463(3.154928) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6021 | Time 1.1940(1.2018) | Loss 3.276875(3.163464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6022 | Time 1.1949(1.2013) | Loss 3.181354(3.164716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6023 | Time 1.2363(1.2038) | Loss 3.192564(3.166666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6024 | Time 1.1935(1.2030) | Loss 3.200741(3.169051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6025 | Time 1.1942(1.2024) | Loss 3.155231(3.168084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6026 | Time 1.1997(1.2022) | Loss 3.187116(3.169416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6027 | Time 1.1943(1.2017) | Loss 3.184570(3.170477) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6028 | Time 1.1820(1.2003) | Loss 3.189412(3.171802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6029 | Time 1.1768(1.1987) | Loss 3.189434(3.173036) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6030 | Time 1.2218(1.2003) | Loss 3.161973(3.172262) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6031 | Time 1.1986(1.2002) | Loss 3.163857(3.171674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6032 | Time 1.1968(1.1999) | Loss 3.212061(3.174501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6033 | Time 1.1880(1.1991) | Loss 3.130335(3.171409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6034 | Time 1.2282(1.2011) | Loss 3.142324(3.169373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6035 | Time 1.1993(1.2010) | Loss 3.165507(3.169103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6036 | Time 1.2218(1.2025) | Loss 3.145684(3.167463) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6037 | Time 1.2222(1.2038) | Loss 3.121633(3.164255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6038 | Time 1.2286(1.2056) | Loss 3.124400(3.161465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6039 | Time 1.2146(1.2062) | Loss 3.174931(3.162408) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6040 | Time 1.2127(1.2067) | Loss 3.207135(3.165539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6041 | Time 1.3161(1.2143) | Loss 3.204464(3.168264) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6042 | Time 1.2374(1.2159) | Loss 3.211338(3.171279) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6043 | Time 1.2168(1.2160) | Loss 3.130262(3.168408) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6044 | Time 1.2160(1.2160) | Loss 3.214503(3.171634) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6045 | Time 1.1909(1.2142) | Loss 3.184490(3.172534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6046 | Time 1.1923(1.2127) | Loss 3.128777(3.169471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6047 | Time 1.1906(1.2111) | Loss 3.227376(3.173524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6048 | Time 1.1901(1.2097) | Loss 3.230022(3.177479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6049 | Time 1.1908(1.2084) | Loss 3.237066(3.181650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6050 | Time 1.1839(1.2066) | Loss 3.217251(3.184142) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6051 | Time 1.1951(1.2058) | Loss 3.234719(3.187683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6052 | Time 1.2083(1.2060) | Loss 3.227365(3.190460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6053 | Time 1.1900(1.2049) | Loss 3.188195(3.190302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6054 | Time 1.1805(1.2032) | Loss 3.235487(3.193465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6055 | Time 1.2046(1.2033) | Loss 3.206455(3.194374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6056 | Time 1.2080(1.2036) | Loss 3.210508(3.195504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6057 | Time 1.1815(1.2021) | Loss 3.135028(3.191270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6058 | Time 1.1955(1.2016) | Loss 3.193476(3.191425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6059 | Time 1.2010(1.2016) | Loss 3.119791(3.186410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6060 | Time 1.2336(1.2038) | Loss 3.171746(3.185384) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6061 | Time 1.2551(1.2074) | Loss 3.123380(3.181044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6062 | Time 1.2099(1.2076) | Loss 3.153635(3.179125) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6063 | Time 1.2381(1.2097) | Loss 3.155528(3.177473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6064 | Time 1.2498(1.2125) | Loss 3.170567(3.176990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6065 | Time 1.2031(1.2119) | Loss 3.153672(3.175357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6066 | Time 1.1995(1.2110) | Loss 3.139102(3.172820) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6067 | Time 1.1960(1.2100) | Loss 3.133985(3.170101) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6068 | Time 1.2196(1.2106) | Loss 3.174845(3.170433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6069 | Time 1.2263(1.2117) | Loss 3.117753(3.166746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6070 | Time 1.2613(1.2152) | Loss 3.168537(3.166871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6071 | Time 1.2217(1.2157) | Loss 3.154159(3.165981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6072 | Time 1.2142(1.2155) | Loss 3.177510(3.166788) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6073 | Time 1.2124(1.2153) | Loss 3.151818(3.165740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6074 | Time 1.2138(1.2152) | Loss 3.154611(3.164961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6075 | Time 1.2377(1.2168) | Loss 3.167196(3.165118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6076 | Time 1.2270(1.2175) | Loss 3.199852(3.167549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6077 | Time 1.2114(1.2171) | Loss 3.114484(3.163835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6078 | Time 1.2051(1.2162) | Loss 3.124147(3.161056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6079 | Time 1.2126(1.2160) | Loss 3.124267(3.158481) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6080 | Time 1.2153(1.2159) | Loss 3.150377(3.157914) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6081 | Time 1.2010(1.2149) | Loss 3.193938(3.160436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6082 | Time 1.2637(1.2183) | Loss 3.155455(3.160087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6083 | Time 1.2691(1.2219) | Loss 3.118880(3.157202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6084 | Time 1.2595(1.2245) | Loss 3.168795(3.158014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6085 | Time 1.2678(1.2275) | Loss 3.155910(3.157867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6086 | Time 1.2801(1.2312) | Loss 3.155218(3.157681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6087 | Time 1.2719(1.2341) | Loss 3.189917(3.159938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6088 | Time 1.2835(1.2375) | Loss 3.120268(3.157161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6089 | Time 1.2416(1.2378) | Loss 3.207118(3.160658) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6090 | Time 1.2024(1.2353) | Loss 3.163042(3.160825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6091 | Time 1.2271(1.2348) | Loss 3.112133(3.157416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6092 | Time 1.2219(1.2339) | Loss 3.070287(3.151317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6093 | Time 1.2235(1.2331) | Loss 3.257613(3.158758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6094 | Time 1.2162(1.2319) | Loss 3.083833(3.153513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6095 | Time 1.2091(1.2303) | Loss 3.204277(3.157067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6096 | Time 1.2109(1.2290) | Loss 3.240930(3.162937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6097 | Time 1.2141(1.2279) | Loss 3.173624(3.163685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6098 | Time 1.2182(1.2273) | Loss 3.175890(3.164540) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6099 | Time 1.1966(1.2251) | Loss 3.185642(3.166017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6100 | Time 1.1934(1.2229) | Loss 3.170822(3.166353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6100 | Test Loss 3.193510 | NFE 20
Skipping vis as data dimension is >2
Iter 6101 | Time 1.1862(1.2203) | Loss 3.185387(3.167686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6102 | Time 1.1724(1.2170) | Loss 3.174804(3.168184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6103 | Time 1.2212(1.2173) | Loss 3.107234(3.163917) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6104 | Time 1.1894(1.2153) | Loss 3.174196(3.164637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6105 | Time 1.2024(1.2144) | Loss 3.183207(3.165937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6106 | Time 1.2231(1.2150) | Loss 3.200762(3.168374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6107 | Time 1.2062(1.2144) | Loss 3.182511(3.169364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6108 | Time 1.1900(1.2127) | Loss 3.212883(3.172410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6109 | Time 1.1878(1.2109) | Loss 3.164213(3.171837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6110 | Time 1.1899(1.2095) | Loss 3.223033(3.175420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6111 | Time 1.1995(1.2088) | Loss 3.082875(3.168942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6112 | Time 1.2016(1.2083) | Loss 3.173518(3.169262) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6113 | Time 1.2080(1.2083) | Loss 3.129554(3.166483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6114 | Time 1.2094(1.2083) | Loss 3.156105(3.165756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6115 | Time 1.2133(1.2087) | Loss 3.225260(3.169922) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6116 | Time 1.2034(1.2083) | Loss 3.263959(3.176504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6117 | Time 1.2099(1.2084) | Loss 3.148210(3.174524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6118 | Time 1.2074(1.2083) | Loss 3.203750(3.176569) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6119 | Time 1.2131(1.2087) | Loss 3.141503(3.174115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6120 | Time 1.1944(1.2077) | Loss 3.104337(3.169230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6121 | Time 1.1956(1.2068) | Loss 3.140352(3.167209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6122 | Time 1.2126(1.2072) | Loss 3.086797(3.161580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6123 | Time 1.2246(1.2085) | Loss 3.184461(3.163182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6124 | Time 1.2139(1.2088) | Loss 3.142482(3.161733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6125 | Time 1.1940(1.2078) | Loss 3.183308(3.163243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6126 | Time 1.1916(1.2067) | Loss 3.186117(3.164844) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6127 | Time 1.2056(1.2066) | Loss 3.172478(3.165379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6128 | Time 1.1637(1.2036) | Loss 3.166540(3.165460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6129 | Time 1.1991(1.2033) | Loss 3.206420(3.168327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6130 | Time 1.2110(1.2038) | Loss 3.152796(3.167240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6131 | Time 1.2311(1.2057) | Loss 3.203356(3.169768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6132 | Time 1.2227(1.2069) | Loss 3.142086(3.167830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6133 | Time 1.2263(1.2083) | Loss 3.231900(3.172315) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6134 | Time 1.2227(1.2093) | Loss 3.262063(3.178597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6135 | Time 1.2035(1.2089) | Loss 3.255628(3.183990) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6136 | Time 1.1934(1.2078) | Loss 3.211898(3.185943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6137 | Time 1.1950(1.2069) | Loss 3.175982(3.185246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6138 | Time 1.2002(1.2064) | Loss 3.138066(3.181943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6139 | Time 1.1845(1.2049) | Loss 3.104792(3.176543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6140 | Time 1.1957(1.2042) | Loss 3.188166(3.177356) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6141 | Time 1.1834(1.2028) | Loss 3.142258(3.174900) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6142 | Time 1.2191(1.2039) | Loss 3.142434(3.172627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6143 | Time 1.1837(1.2025) | Loss 3.110176(3.168255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6144 | Time 1.1827(1.2011) | Loss 3.048517(3.159874) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6145 | Time 1.1853(1.2000) | Loss 3.075864(3.153993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6146 | Time 1.1984(1.1999) | Loss 3.094084(3.149799) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6147 | Time 1.1834(1.1988) | Loss 3.129292(3.148364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6148 | Time 1.1721(1.1969) | Loss 3.094107(3.144566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6149 | Time 1.1914(1.1965) | Loss 3.032100(3.136693) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6150 | Time 1.1960(1.1965) | Loss 3.127161(3.136026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6151 | Time 1.1846(1.1956) | Loss 3.096290(3.133244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6152 | Time 1.1944(1.1956) | Loss 3.087145(3.130017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6153 | Time 1.1982(1.1957) | Loss 3.115952(3.129033) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6154 | Time 1.2082(1.1966) | Loss 3.016655(3.121166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6155 | Time 1.1797(1.1954) | Loss 3.084162(3.118576) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6156 | Time 1.1753(1.1940) | Loss 3.016897(3.111459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6157 | Time 1.1841(1.1933) | Loss 3.034466(3.106069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6158 | Time 1.2154(1.1949) | Loss 3.083694(3.104503) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6159 | Time 1.2192(1.1966) | Loss 3.101909(3.104321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6160 | Time 1.2186(1.1981) | Loss 3.150167(3.107530) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6161 | Time 1.2205(1.1997) | Loss 3.057239(3.104010) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6162 | Time 1.2334(1.2020) | Loss 3.129516(3.105796) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6163 | Time 1.1818(1.2006) | Loss 3.131905(3.107623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6164 | Time 1.1775(1.1990) | Loss 3.151345(3.110684) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6165 | Time 1.1814(1.1978) | Loss 3.116824(3.111114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6166 | Time 1.1996(1.1979) | Loss 3.130580(3.112476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6167 | Time 1.1733(1.1962) | Loss 3.126939(3.113489) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6168 | Time 1.1699(1.1943) | Loss 3.102429(3.112714) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6169 | Time 1.1746(1.1930) | Loss 3.168040(3.116587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6170 | Time 1.2086(1.1941) | Loss 3.168057(3.120190) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6171 | Time 1.1977(1.1943) | Loss 3.191362(3.125172) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6172 | Time 1.1946(1.1943) | Loss 3.248029(3.133772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6173 | Time 1.1903(1.1940) | Loss 3.162567(3.135788) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6174 | Time 1.1910(1.1938) | Loss 3.111492(3.134087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6175 | Time 1.1979(1.1941) | Loss 3.179681(3.137279) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6176 | Time 1.2056(1.1949) | Loss 3.202348(3.141834) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6177 | Time 1.1854(1.1943) | Loss 3.169563(3.143775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6178 | Time 1.1797(1.1932) | Loss 3.199675(3.147688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6179 | Time 1.2508(1.1973) | Loss 3.231696(3.153568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6180 | Time 1.2423(1.2004) | Loss 3.118313(3.151100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6181 | Time 1.2039(1.2007) | Loss 3.212592(3.155405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6182 | Time 1.1740(1.1988) | Loss 3.157138(3.155526) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6183 | Time 1.1903(1.1982) | Loss 3.216191(3.159773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6184 | Time 1.2044(1.1986) | Loss 3.197881(3.162440) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6185 | Time 1.2177(1.2000) | Loss 3.201502(3.165175) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6186 | Time 1.1844(1.1989) | Loss 3.260608(3.171855) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6187 | Time 1.1760(1.1973) | Loss 3.249275(3.177274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6188 | Time 1.1769(1.1959) | Loss 3.204342(3.179169) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6189 | Time 1.2058(1.1966) | Loss 3.257396(3.184645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6190 | Time 1.2191(1.1981) | Loss 3.224330(3.187423) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6191 | Time 1.1768(1.1966) | Loss 3.241402(3.191201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6192 | Time 1.1928(1.1964) | Loss 3.195010(3.191468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6193 | Time 1.1812(1.1953) | Loss 3.191405(3.191464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6194 | Time 1.1711(1.1936) | Loss 3.159961(3.189258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6195 | Time 1.1709(1.1920) | Loss 3.235144(3.192470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6196 | Time 1.1848(1.1915) | Loss 3.153974(3.189776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6197 | Time 1.2076(1.1926) | Loss 3.206034(3.190914) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6198 | Time 1.2299(1.1952) | Loss 3.201983(3.191689) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6199 | Time 1.2315(1.1978) | Loss 3.159318(3.189423) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6200 | Time 1.2293(1.2000) | Loss 3.128414(3.185152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6200 | Test Loss 3.204369 | NFE 20
Skipping vis as data dimension is >2
Iter 6201 | Time 1.2194(1.2013) | Loss 3.189506(3.185457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6202 | Time 1.2396(1.2040) | Loss 3.156294(3.183415) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6203 | Time 1.2428(1.2067) | Loss 3.140304(3.180398) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6204 | Time 1.2176(1.2075) | Loss 3.141101(3.177647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6205 | Time 1.2331(1.2093) | Loss 3.068901(3.170035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6206 | Time 1.2427(1.2116) | Loss 3.123533(3.166779) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6207 | Time 1.2549(1.2147) | Loss 3.108218(3.162680) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6208 | Time 1.2133(1.2146) | Loss 3.072633(3.156377) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6209 | Time 1.2169(1.2147) | Loss 3.117866(3.153681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6210 | Time 1.2238(1.2154) | Loss 3.234546(3.159342) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6211 | Time 1.2111(1.2151) | Loss 3.141420(3.158087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6212 | Time 1.2003(1.2140) | Loss 3.075401(3.152299) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6213 | Time 1.1837(1.2119) | Loss 3.141521(3.151545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6214 | Time 1.1963(1.2108) | Loss 3.145667(3.151133) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6215 | Time 1.1877(1.2092) | Loss 3.214378(3.155560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6216 | Time 1.1956(1.2082) | Loss 3.202488(3.158845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6217 | Time 1.1963(1.2074) | Loss 3.150176(3.158238) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6218 | Time 1.2016(1.2070) | Loss 3.103868(3.154433) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6219 | Time 1.2537(1.2103) | Loss 3.186705(3.156692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6220 | Time 1.2114(1.2103) | Loss 3.210376(3.160450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6221 | Time 1.1971(1.2094) | Loss 3.138215(3.158893) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6222 | Time 1.1909(1.2081) | Loss 3.106813(3.155248) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6223 | Time 1.2189(1.2089) | Loss 3.131212(3.153565) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6224 | Time 1.2449(1.2114) | Loss 3.181759(3.155539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6225 | Time 1.2260(1.2124) | Loss 3.148530(3.155048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6226 | Time 1.1894(1.2108) | Loss 3.096665(3.150961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6227 | Time 1.1945(1.2097) | Loss 3.120646(3.148839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6228 | Time 1.2075(1.2095) | Loss 3.184581(3.151341) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6229 | Time 1.2157(1.2099) | Loss 3.225068(3.156502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6230 | Time 1.2480(1.2126) | Loss 3.140236(3.155363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6231 | Time 1.2495(1.2152) | Loss 3.136577(3.154048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6232 | Time 1.2887(1.2203) | Loss 3.052759(3.146958) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6233 | Time 1.2174(1.2201) | Loss 3.147524(3.146998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6234 | Time 1.1921(1.2182) | Loss 3.067008(3.141398) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6235 | Time 1.2152(1.2180) | Loss 3.144867(3.141641) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6236 | Time 1.2058(1.2171) | Loss 3.005773(3.132130) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6237 | Time 1.2123(1.2168) | Loss 3.172507(3.134957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6238 | Time 1.2009(1.2157) | Loss 3.066932(3.130195) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6239 | Time 1.2159(1.2157) | Loss 3.036654(3.123647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6240 | Time 1.1877(1.2137) | Loss 3.002966(3.115200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6241 | Time 1.2034(1.2130) | Loss 3.106614(3.114599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6242 | Time 1.1979(1.2120) | Loss 2.960650(3.103822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6243 | Time 1.2123(1.2120) | Loss 3.005821(3.096962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6244 | Time 1.2040(1.2114) | Loss 3.011431(3.090975) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6245 | Time 1.1906(1.2100) | Loss 3.031709(3.086826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6246 | Time 1.1942(1.2089) | Loss 3.020746(3.082201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6247 | Time 1.1893(1.2075) | Loss 3.062221(3.080802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6248 | Time 1.2084(1.2076) | Loss 3.060044(3.079349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6249 | Time 1.2007(1.2071) | Loss 3.115431(3.081875) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6250 | Time 1.1926(1.2061) | Loss 3.103472(3.083386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6251 | Time 1.2313(1.2078) | Loss 3.093584(3.084100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6252 | Time 1.2309(1.2094) | Loss 3.090160(3.084524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6253 | Time 1.2242(1.2105) | Loss 3.087093(3.084704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6254 | Time 1.2275(1.2117) | Loss 3.115397(3.086853) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6255 | Time 1.2164(1.2120) | Loss 3.094916(3.087417) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6256 | Time 1.2055(1.2115) | Loss 3.138710(3.091008) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6257 | Time 1.2135(1.2117) | Loss 3.174680(3.096865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6258 | Time 1.1989(1.2108) | Loss 3.187610(3.103217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6259 | Time 1.1892(1.2093) | Loss 3.113400(3.103930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6260 | Time 1.1985(1.2085) | Loss 3.133166(3.105976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6261 | Time 1.1841(1.2068) | Loss 3.149022(3.108989) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6262 | Time 1.1856(1.2053) | Loss 3.180774(3.114014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6263 | Time 1.2261(1.2068) | Loss 3.134236(3.115430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6264 | Time 1.2194(1.2077) | Loss 3.162830(3.118748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6265 | Time 1.1971(1.2069) | Loss 3.099526(3.117402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6266 | Time 1.2146(1.2075) | Loss 3.151613(3.119797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6267 | Time 1.1834(1.2058) | Loss 3.132489(3.120685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6268 | Time 1.1858(1.2044) | Loss 3.127127(3.121136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6269 | Time 1.1839(1.2029) | Loss 3.151693(3.123275) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6270 | Time 1.1917(1.2022) | Loss 3.176313(3.126988) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6271 | Time 1.2008(1.2021) | Loss 3.167157(3.129800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6272 | Time 1.2001(1.2019) | Loss 3.115579(3.128804) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6273 | Time 1.1964(1.2015) | Loss 3.202769(3.133982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6274 | Time 1.1961(1.2012) | Loss 3.182399(3.137371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6275 | Time 1.1965(1.2008) | Loss 3.130346(3.136879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6276 | Time 1.2001(1.2008) | Loss 3.096960(3.134085) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6277 | Time 1.1909(1.2001) | Loss 3.150166(3.135211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6278 | Time 1.2398(1.2029) | Loss 3.110749(3.133498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6279 | Time 1.2038(1.2029) | Loss 3.156637(3.135118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6280 | Time 1.2817(1.2084) | Loss 3.122516(3.134236) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6281 | Time 1.2728(1.2130) | Loss 3.107410(3.132358) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6282 | Time 1.2743(1.2172) | Loss 3.066510(3.127749) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6283 | Time 1.2052(1.2164) | Loss 3.059634(3.122981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6284 | Time 1.2046(1.2156) | Loss 3.054825(3.118210) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6285 | Time 1.2253(1.2163) | Loss 3.093404(3.116473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6286 | Time 1.2125(1.2160) | Loss 3.090872(3.114681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6287 | Time 1.2156(1.2160) | Loss 3.041057(3.109528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6288 | Time 1.2300(1.2169) | Loss 2.984210(3.100755) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6289 | Time 1.2206(1.2172) | Loss 3.027377(3.095619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6290 | Time 1.2100(1.2167) | Loss 3.050386(3.092453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6291 | Time 1.2136(1.2165) | Loss 3.028130(3.087950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6292 | Time 1.2142(1.2163) | Loss 3.074589(3.087015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6293 | Time 1.2734(1.2203) | Loss 3.056066(3.084848) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6294 | Time 1.2208(1.2203) | Loss 3.065932(3.083524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6295 | Time 1.2012(1.2190) | Loss 3.070723(3.082628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6296 | Time 1.1994(1.2176) | Loss 3.068774(3.081658) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6297 | Time 1.1995(1.2164) | Loss 3.026280(3.077782) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6298 | Time 1.2097(1.2159) | Loss 3.102664(3.079524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6299 | Time 1.2283(1.2168) | Loss 3.037034(3.076549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6300 | Time 1.2063(1.2160) | Loss 3.014707(3.072220) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6300 | Test Loss 3.069080 | NFE 20
Skipping vis as data dimension is >2
Iter 6301 | Time 1.2206(1.2164) | Loss 3.095846(3.073874) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6302 | Time 1.2758(1.2205) | Loss 3.060852(3.072963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6303 | Time 1.2598(1.2233) | Loss 3.077528(3.073282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6304 | Time 1.2222(1.2232) | Loss 3.077889(3.073605) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6305 | Time 1.2509(1.2251) | Loss 3.049267(3.071901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6306 | Time 1.2131(1.2243) | Loss 3.068482(3.071662) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6307 | Time 1.2003(1.2226) | Loss 3.108115(3.074213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6308 | Time 1.1931(1.2205) | Loss 3.079199(3.074562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6309 | Time 1.2084(1.2197) | Loss 3.143972(3.079421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6310 | Time 1.2128(1.2192) | Loss 3.081568(3.079571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6311 | Time 1.2252(1.2196) | Loss 3.161350(3.085296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6312 | Time 1.2093(1.2189) | Loss 3.052812(3.083022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6313 | Time 1.2113(1.2184) | Loss 3.107810(3.084757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6314 | Time 1.1913(1.2165) | Loss 3.094884(3.085466) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6315 | Time 1.2092(1.2160) | Loss 3.111343(3.087277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6316 | Time 1.2025(1.2150) | Loss 3.119022(3.089500) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6317 | Time 1.1949(1.2136) | Loss 3.094962(3.089882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6318 | Time 1.1843(1.2116) | Loss 3.174176(3.095783) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6319 | Time 1.1969(1.2105) | Loss 3.073253(3.094205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6320 | Time 1.2085(1.2104) | Loss 3.040685(3.090459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6321 | Time 1.2121(1.2105) | Loss 3.032644(3.086412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6322 | Time 1.2102(1.2105) | Loss 3.197393(3.094181) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6323 | Time 1.2188(1.2111) | Loss 3.125752(3.096391) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6324 | Time 1.2150(1.2113) | Loss 3.069698(3.094522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6325 | Time 1.2079(1.2111) | Loss 3.024716(3.089636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6326 | Time 1.2262(1.2122) | Loss 3.115030(3.091413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6327 | Time 1.2086(1.2119) | Loss 3.057716(3.089054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6328 | Time 1.2205(1.2125) | Loss 3.053265(3.086549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6329 | Time 1.2042(1.2119) | Loss 3.003249(3.080718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6330 | Time 1.2182(1.2124) | Loss 3.053892(3.078840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6331 | Time 1.2248(1.2132) | Loss 3.031847(3.075551) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6332 | Time 1.2367(1.2149) | Loss 3.055556(3.074151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6333 | Time 1.2181(1.2151) | Loss 3.052934(3.072666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6334 | Time 1.2138(1.2150) | Loss 3.087882(3.073731) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6335 | Time 1.2105(1.2147) | Loss 3.111475(3.076373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6336 | Time 1.2109(1.2144) | Loss 3.158943(3.082153) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6337 | Time 1.2128(1.2143) | Loss 3.079416(3.081961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6338 | Time 1.2201(1.2147) | Loss 3.060776(3.080478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6339 | Time 1.2371(1.2163) | Loss 3.085295(3.080816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6340 | Time 1.1881(1.2143) | Loss 3.092184(3.081611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6341 | Time 1.1891(1.2125) | Loss 3.093476(3.082442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6342 | Time 1.1912(1.2111) | Loss 3.166900(3.088354) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6343 | Time 1.2086(1.2109) | Loss 3.155825(3.093077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6344 | Time 1.2004(1.2101) | Loss 3.110652(3.094307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6345 | Time 1.1857(1.2084) | Loss 3.180664(3.100352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6346 | Time 1.1900(1.2071) | Loss 3.137725(3.102968) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6347 | Time 1.1824(1.2054) | Loss 3.090777(3.102115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6348 | Time 1.2226(1.2066) | Loss 3.157838(3.106015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6349 | Time 1.2086(1.2067) | Loss 3.081612(3.104307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6350 | Time 1.2065(1.2067) | Loss 3.140711(3.106855) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6351 | Time 1.2222(1.2078) | Loss 3.066700(3.104045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6352 | Time 1.2097(1.2079) | Loss 3.095646(3.103457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6353 | Time 1.2068(1.2079) | Loss 3.066372(3.100861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6354 | Time 1.2051(1.2077) | Loss 3.037841(3.096449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6355 | Time 1.2216(1.2086) | Loss 3.083337(3.095532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6356 | Time 1.2046(1.2084) | Loss 3.059240(3.092991) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6357 | Time 1.2550(1.2116) | Loss 3.008498(3.087077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6358 | Time 1.2073(1.2113) | Loss 3.128892(3.090004) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6359 | Time 1.2455(1.2137) | Loss 3.020900(3.085166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6360 | Time 1.2167(1.2139) | Loss 2.976498(3.077560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6361 | Time 1.2000(1.2130) | Loss 2.951069(3.068705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6362 | Time 1.2016(1.2122) | Loss 3.003887(3.064168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6363 | Time 1.2096(1.2120) | Loss 2.963938(3.057152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6364 | Time 1.2212(1.2126) | Loss 3.023964(3.054829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6365 | Time 1.2068(1.2122) | Loss 3.072549(3.056069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6366 | Time 1.2162(1.2125) | Loss 3.027066(3.054039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6367 | Time 1.2072(1.2121) | Loss 3.123640(3.058911) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6368 | Time 1.2086(1.2119) | Loss 3.059895(3.058980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6369 | Time 1.2044(1.2113) | Loss 3.182028(3.067593) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6370 | Time 1.2211(1.2120) | Loss 3.179705(3.075441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6371 | Time 1.2115(1.2120) | Loss 3.196616(3.083923) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6372 | Time 1.2432(1.2142) | Loss 3.115386(3.086126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6373 | Time 1.2677(1.2179) | Loss 3.149384(3.090554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6374 | Time 1.2895(1.2229) | Loss 3.170727(3.096166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6375 | Time 1.2794(1.2269) | Loss 3.183028(3.102246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6376 | Time 1.2703(1.2299) | Loss 3.067237(3.099796) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6377 | Time 1.2511(1.2314) | Loss 3.080322(3.098432) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6378 | Time 1.2854(1.2352) | Loss 3.010237(3.092259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6379 | Time 1.2794(1.2383) | Loss 3.115615(3.093894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6380 | Time 1.2539(1.2394) | Loss 3.072816(3.092418) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6381 | Time 1.2561(1.2405) | Loss 2.980348(3.084573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6382 | Time 1.2502(1.2412) | Loss 3.047351(3.081968) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6383 | Time 1.2775(1.2438) | Loss 2.978796(3.074746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6384 | Time 1.2471(1.2440) | Loss 3.025822(3.071321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6385 | Time 1.2036(1.2412) | Loss 3.073734(3.071490) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6386 | Time 1.2409(1.2411) | Loss 3.060386(3.070713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6387 | Time 1.2404(1.2411) | Loss 3.015795(3.066869) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6388 | Time 1.2711(1.2432) | Loss 3.060594(3.066429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6389 | Time 1.2792(1.2457) | Loss 3.005770(3.062183) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6390 | Time 1.2691(1.2473) | Loss 3.043979(3.060909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6391 | Time 1.2342(1.2464) | Loss 3.083195(3.062469) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6392 | Time 1.2186(1.2445) | Loss 2.996307(3.057838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6393 | Time 1.2712(1.2464) | Loss 2.961698(3.051108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6394 | Time 1.3223(1.2517) | Loss 3.002558(3.047709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6395 | Time 1.3190(1.2564) | Loss 3.012866(3.045270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6396 | Time 1.2620(1.2568) | Loss 3.012197(3.042955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6397 | Time 1.2413(1.2557) | Loss 3.026299(3.041789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6398 | Time 1.2240(1.2535) | Loss 3.028226(3.040840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6399 | Time 1.2655(1.2543) | Loss 3.023931(3.039656) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6400 | Time 1.2606(1.2548) | Loss 3.021545(3.038388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6400 | Test Loss 3.008961 | NFE 20
Skipping vis as data dimension is >2
Iter 6401 | Time 1.2037(1.2512) | Loss 2.964333(3.033205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6402 | Time 1.1889(1.2468) | Loss 3.000723(3.030931) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6403 | Time 1.1898(1.2428) | Loss 3.003987(3.029045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6404 | Time 1.1938(1.2394) | Loss 3.032597(3.029293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6405 | Time 1.1992(1.2366) | Loss 3.011517(3.028049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6406 | Time 1.2033(1.2343) | Loss 2.991436(3.025486) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6407 | Time 1.2097(1.2325) | Loss 3.059559(3.027871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6408 | Time 1.2103(1.2310) | Loss 2.970079(3.023826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6409 | Time 1.2027(1.2290) | Loss 3.001599(3.022270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6410 | Time 1.1971(1.2268) | Loss 3.048096(3.024078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6411 | Time 1.2218(1.2264) | Loss 3.028210(3.024367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6412 | Time 1.2150(1.2256) | Loss 3.050977(3.026230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6413 | Time 1.2167(1.2250) | Loss 3.105119(3.031752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6414 | Time 1.2027(1.2234) | Loss 3.062942(3.033935) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6415 | Time 1.1976(1.2216) | Loss 3.053746(3.035322) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6416 | Time 1.1963(1.2199) | Loss 3.069732(3.037731) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6417 | Time 1.1963(1.2182) | Loss 3.061989(3.039429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6418 | Time 1.1951(1.2166) | Loss 3.066373(3.041315) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6419 | Time 1.2040(1.2157) | Loss 3.084716(3.044353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6420 | Time 1.2078(1.2152) | Loss 3.112648(3.049134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6421 | Time 1.1873(1.2132) | Loss 3.102841(3.052893) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6422 | Time 1.1865(1.2113) | Loss 3.088825(3.055408) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6423 | Time 1.2546(1.2144) | Loss 3.128108(3.060497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6424 | Time 1.2753(1.2186) | Loss 3.139545(3.066031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6425 | Time 1.2494(1.2208) | Loss 3.093334(3.067942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6426 | Time 1.2595(1.2235) | Loss 3.085557(3.069175) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6427 | Time 1.2605(1.2261) | Loss 3.149701(3.074812) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6428 | Time 1.2773(1.2297) | Loss 3.208705(3.084184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6429 | Time 1.2251(1.2294) | Loss 3.083614(3.084144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6430 | Time 1.2016(1.2274) | Loss 3.063045(3.082667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6431 | Time 1.2287(1.2275) | Loss 3.149515(3.087347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6432 | Time 1.1894(1.2248) | Loss 3.110249(3.088950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6433 | Time 1.2160(1.2242) | Loss 3.132001(3.091963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6434 | Time 1.2120(1.2234) | Loss 3.102182(3.092679) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6435 | Time 1.1976(1.2216) | Loss 3.064852(3.090731) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6436 | Time 1.2885(1.2262) | Loss 3.151027(3.094952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6437 | Time 1.2451(1.2276) | Loss 3.143185(3.098328) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6438 | Time 1.2522(1.2293) | Loss 3.063351(3.095880) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6439 | Time 1.2374(1.2298) | Loss 3.112548(3.097046) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6440 | Time 1.2310(1.2299) | Loss 3.127871(3.099204) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6441 | Time 1.2284(1.2298) | Loss 3.118058(3.100524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6442 | Time 1.2238(1.2294) | Loss 3.160529(3.104724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6443 | Time 1.2200(1.2287) | Loss 3.144225(3.107489) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6444 | Time 1.2269(1.2286) | Loss 3.163549(3.111414) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6445 | Time 1.2240(1.2283) | Loss 3.114222(3.111610) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6446 | Time 1.2565(1.2303) | Loss 3.144657(3.113923) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6447 | Time 1.2339(1.2305) | Loss 3.089674(3.112226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6448 | Time 1.2217(1.2299) | Loss 3.150164(3.114882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6449 | Time 1.2325(1.2301) | Loss 3.144922(3.116984) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6450 | Time 1.2031(1.2282) | Loss 3.050603(3.112338) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6451 | Time 1.2222(1.2278) | Loss 3.090958(3.110841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6452 | Time 1.2426(1.2288) | Loss 3.110552(3.110821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6453 | Time 1.2412(1.2297) | Loss 3.090012(3.109364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6454 | Time 1.2560(1.2315) | Loss 3.111811(3.109536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6455 | Time 1.2299(1.2314) | Loss 3.122692(3.110456) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6456 | Time 1.2350(1.2317) | Loss 3.072524(3.107801) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6457 | Time 1.2466(1.2327) | Loss 3.110304(3.107976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6458 | Time 1.2635(1.2349) | Loss 3.050342(3.103942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6459 | Time 1.2442(1.2355) | Loss 3.068716(3.101476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6460 | Time 1.2496(1.2365) | Loss 3.015970(3.095491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6461 | Time 1.2139(1.2349) | Loss 2.999513(3.088772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6462 | Time 1.2168(1.2337) | Loss 3.150742(3.093110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6463 | Time 1.2394(1.2341) | Loss 3.065067(3.091147) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6464 | Time 1.2327(1.2340) | Loss 3.100965(3.091834) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6465 | Time 1.2321(1.2338) | Loss 3.043198(3.088430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6466 | Time 1.2090(1.2321) | Loss 3.080596(3.087881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6467 | Time 1.2173(1.2311) | Loss 3.092590(3.088211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6468 | Time 1.2178(1.2301) | Loss 3.008376(3.082623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6469 | Time 1.2226(1.2296) | Loss 3.091048(3.083212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6470 | Time 1.2630(1.2319) | Loss 3.112690(3.085276) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6471 | Time 1.2308(1.2319) | Loss 3.082057(3.085051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6472 | Time 1.2363(1.2322) | Loss 3.105394(3.086475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6473 | Time 1.2487(1.2333) | Loss 3.050465(3.083954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6474 | Time 1.2183(1.2323) | Loss 3.096740(3.084849) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6475 | Time 1.2207(1.2315) | Loss 3.105487(3.086294) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6476 | Time 1.2167(1.2304) | Loss 3.089794(3.086539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6477 | Time 1.2215(1.2298) | Loss 3.112371(3.088347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6478 | Time 1.2053(1.2281) | Loss 3.082134(3.087912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6479 | Time 1.2143(1.2271) | Loss 3.106887(3.089240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6480 | Time 1.2136(1.2262) | Loss 3.071364(3.087989) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6481 | Time 1.2553(1.2282) | Loss 3.099964(3.088827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6482 | Time 1.2519(1.2299) | Loss 3.146397(3.092857) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6483 | Time 1.2293(1.2298) | Loss 3.104774(3.093691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6484 | Time 1.2176(1.2290) | Loss 3.058007(3.091193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6485 | Time 1.2328(1.2293) | Loss 3.083332(3.090643) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6486 | Time 1.2121(1.2281) | Loss 3.031591(3.086509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6487 | Time 1.2229(1.2277) | Loss 3.124397(3.089162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6488 | Time 1.2149(1.2268) | Loss 3.036507(3.085476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6489 | Time 1.2182(1.2262) | Loss 3.129161(3.088534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6490 | Time 1.2200(1.2258) | Loss 3.084796(3.088272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6491 | Time 1.2141(1.2249) | Loss 3.091528(3.088500) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6492 | Time 1.2417(1.2261) | Loss 3.116459(3.090457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6493 | Time 1.2336(1.2266) | Loss 3.148853(3.094545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6494 | Time 1.2419(1.2277) | Loss 3.084098(3.093813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6495 | Time 1.2239(1.2274) | Loss 3.087325(3.093359) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6496 | Time 1.2383(1.2282) | Loss 3.038819(3.089541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6497 | Time 1.2267(1.2281) | Loss 3.011851(3.084103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6498 | Time 1.2324(1.2284) | Loss 3.076565(3.083575) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6499 | Time 1.2225(1.2280) | Loss 3.055386(3.081602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6500 | Time 1.2213(1.2275) | Loss 3.020992(3.077359) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6500 | Test Loss 3.034630 | NFE 20
Skipping vis as data dimension is >2
Iter 6501 | Time 1.2233(1.2272) | Loss 3.045828(3.075152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6502 | Time 1.2140(1.2263) | Loss 3.049139(3.073331) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6503 | Time 1.2091(1.2251) | Loss 2.964709(3.065728) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6504 | Time 1.2298(1.2254) | Loss 3.098647(3.068032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6505 | Time 1.2472(1.2270) | Loss 3.097558(3.070099) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6506 | Time 1.2362(1.2276) | Loss 3.002336(3.065355) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6507 | Time 1.2445(1.2288) | Loss 3.021701(3.062300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6508 | Time 1.2296(1.2288) | Loss 3.008547(3.058537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6509 | Time 1.2251(1.2286) | Loss 3.044933(3.057585) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6510 | Time 1.2205(1.2280) | Loss 3.086170(3.059586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6511 | Time 1.2288(1.2281) | Loss 3.062031(3.059757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6512 | Time 1.2170(1.2273) | Loss 3.090246(3.061891) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6513 | Time 1.2035(1.2256) | Loss 3.098970(3.064487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6514 | Time 1.1886(1.2230) | Loss 3.151083(3.070548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6515 | Time 1.2110(1.2222) | Loss 3.157587(3.076641) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6516 | Time 1.2103(1.2214) | Loss 3.084510(3.077192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6517 | Time 1.2143(1.2209) | Loss 3.115224(3.079854) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6518 | Time 1.2475(1.2227) | Loss 3.107147(3.081765) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6519 | Time 1.2615(1.2254) | Loss 3.119050(3.084375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6520 | Time 1.2369(1.2262) | Loss 3.171150(3.090449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6521 | Time 1.2284(1.2264) | Loss 3.093923(3.090692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6522 | Time 1.2074(1.2251) | Loss 3.091897(3.090776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6523 | Time 1.2323(1.2256) | Loss 3.059125(3.088561) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6524 | Time 1.2209(1.2252) | Loss 3.134924(3.091806) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6525 | Time 1.2359(1.2260) | Loss 3.120840(3.093839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6526 | Time 1.2161(1.2253) | Loss 3.083495(3.093115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6527 | Time 1.2394(1.2263) | Loss 3.053490(3.090341) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6528 | Time 1.2230(1.2261) | Loss 3.156584(3.094978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6529 | Time 1.2298(1.2263) | Loss 3.032261(3.090588) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6530 | Time 1.2582(1.2285) | Loss 3.089929(3.090542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6531 | Time 1.2589(1.2307) | Loss 3.102354(3.091368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6532 | Time 1.2309(1.2307) | Loss 3.118922(3.093297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6533 | Time 1.2241(1.2302) | Loss 3.071239(3.091753) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6534 | Time 1.2080(1.2287) | Loss 3.009693(3.086009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6535 | Time 1.2185(1.2280) | Loss 3.052325(3.083651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6536 | Time 1.2049(1.2263) | Loss 3.116698(3.085964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6537 | Time 1.2143(1.2255) | Loss 3.036495(3.082501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6538 | Time 1.2093(1.2244) | Loss 3.058722(3.080837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6539 | Time 1.2285(1.2247) | Loss 3.000486(3.075212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6540 | Time 1.2498(1.2264) | Loss 2.999676(3.069925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6541 | Time 1.2382(1.2272) | Loss 2.960355(3.062255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6542 | Time 1.2147(1.2264) | Loss 3.062005(3.062237) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6543 | Time 1.2141(1.2255) | Loss 2.934482(3.053295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6544 | Time 1.2135(1.2247) | Loss 3.063746(3.054026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6545 | Time 1.2585(1.2270) | Loss 3.058604(3.054347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6546 | Time 1.2460(1.2284) | Loss 3.056568(3.054502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6547 | Time 1.2274(1.2283) | Loss 3.038802(3.053403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6548 | Time 1.2591(1.2305) | Loss 2.994891(3.049307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6549 | Time 1.2355(1.2308) | Loss 3.136634(3.055420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6550 | Time 1.2151(1.2297) | Loss 3.014611(3.052564) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6551 | Time 1.2237(1.2293) | Loss 3.022523(3.050461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6552 | Time 1.2309(1.2294) | Loss 3.073372(3.052064) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6553 | Time 1.2144(1.2283) | Loss 3.153628(3.059174) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6554 | Time 1.2166(1.2275) | Loss 3.091600(3.061444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6555 | Time 1.2103(1.2263) | Loss 3.104313(3.064445) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6556 | Time 1.1946(1.2241) | Loss 3.102240(3.067090) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6557 | Time 1.1955(1.2221) | Loss 3.187142(3.075494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6558 | Time 1.2063(1.2210) | Loss 3.129132(3.079249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6559 | Time 1.2065(1.2200) | Loss 3.172917(3.085805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6560 | Time 1.2005(1.2186) | Loss 3.161405(3.091097) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6561 | Time 1.2003(1.2173) | Loss 3.125931(3.093536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6562 | Time 1.2077(1.2167) | Loss 3.122328(3.095551) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6563 | Time 1.2606(1.2197) | Loss 3.140724(3.098713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6564 | Time 1.2269(1.2202) | Loss 3.148554(3.102202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6565 | Time 1.1996(1.2188) | Loss 3.164017(3.106529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6566 | Time 1.2090(1.2181) | Loss 3.082144(3.104822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6567 | Time 1.2233(1.2185) | Loss 3.078111(3.102952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6568 | Time 1.2094(1.2178) | Loss 3.063434(3.100186) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6569 | Time 1.2142(1.2176) | Loss 3.057976(3.097231) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6570 | Time 1.2393(1.2191) | Loss 3.086938(3.096511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6571 | Time 1.2409(1.2206) | Loss 3.146748(3.100028) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6572 | Time 1.2418(1.2221) | Loss 3.060850(3.097285) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6573 | Time 1.2130(1.2215) | Loss 3.083635(3.096330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6574 | Time 1.2296(1.2220) | Loss 3.079662(3.095163) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6575 | Time 1.2133(1.2214) | Loss 3.078455(3.093993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6576 | Time 1.2094(1.2206) | Loss 3.158090(3.098480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6577 | Time 1.2133(1.2201) | Loss 3.068322(3.096369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6578 | Time 1.2235(1.2203) | Loss 3.040857(3.092483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6579 | Time 1.2788(1.2244) | Loss 3.109370(3.093665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6580 | Time 1.2469(1.2260) | Loss 3.119659(3.095485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6581 | Time 1.2701(1.2291) | Loss 3.040425(3.091631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6582 | Time 1.2345(1.2295) | Loss 3.023658(3.086873) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6583 | Time 1.2021(1.2275) | Loss 3.069337(3.085645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6584 | Time 1.2052(1.2260) | Loss 3.110210(3.087365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6585 | Time 1.2202(1.2256) | Loss 3.059099(3.085386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6586 | Time 1.2535(1.2275) | Loss 2.969150(3.077250) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6587 | Time 1.2335(1.2279) | Loss 2.995912(3.071556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6588 | Time 1.1988(1.2259) | Loss 3.115755(3.074650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6589 | Time 1.2145(1.2251) | Loss 3.090341(3.075748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6590 | Time 1.1973(1.2232) | Loss 3.046223(3.073681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6591 | Time 1.1986(1.2214) | Loss 3.047118(3.071822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6592 | Time 1.1979(1.2198) | Loss 3.040890(3.069657) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6593 | Time 1.2117(1.2192) | Loss 2.996384(3.064528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6594 | Time 1.2244(1.2196) | Loss 3.042647(3.062996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6595 | Time 1.2161(1.2193) | Loss 3.029080(3.060622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6596 | Time 1.2138(1.2190) | Loss 3.107506(3.063904) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6597 | Time 1.2171(1.2188) | Loss 3.126202(3.068265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6598 | Time 1.2051(1.2179) | Loss 3.090302(3.069807) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6599 | Time 1.2422(1.2196) | Loss 3.104525(3.072237) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6600 | Time 1.2447(1.2213) | Loss 3.125301(3.075952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6600 | Test Loss 3.133008 | NFE 20
Skipping vis as data dimension is >2
Iter 6601 | Time 1.2105(1.2206) | Loss 3.031287(3.072825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6602 | Time 1.2328(1.2214) | Loss 3.119050(3.076061) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6603 | Time 1.2334(1.2223) | Loss 3.124679(3.079464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6604 | Time 1.2387(1.2234) | Loss 3.099083(3.080838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6605 | Time 1.2389(1.2245) | Loss 3.120185(3.083592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6606 | Time 1.2250(1.2245) | Loss 3.055684(3.081638) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6607 | Time 1.1885(1.2220) | Loss 3.085285(3.081894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6608 | Time 1.1948(1.2201) | Loss 2.968532(3.073958) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6609 | Time 1.2107(1.2195) | Loss 3.049895(3.072274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6610 | Time 1.2099(1.2188) | Loss 3.079414(3.072774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6611 | Time 1.2193(1.2188) | Loss 3.039185(3.070423) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6612 | Time 1.1973(1.2173) | Loss 3.035172(3.067955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6613 | Time 1.2074(1.2166) | Loss 3.068863(3.068019) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6614 | Time 1.1962(1.2152) | Loss 3.032557(3.065536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6615 | Time 1.1942(1.2137) | Loss 3.073572(3.066099) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6616 | Time 1.2038(1.2130) | Loss 3.043442(3.064513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6617 | Time 1.1963(1.2119) | Loss 2.966454(3.057649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6618 | Time 1.1932(1.2106) | Loss 3.108916(3.061237) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6619 | Time 1.2013(1.2099) | Loss 3.024238(3.058647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6620 | Time 1.1991(1.2092) | Loss 2.998486(3.054436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6621 | Time 1.2158(1.2096) | Loss 3.088164(3.056797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6622 | Time 1.1928(1.2084) | Loss 3.085373(3.058797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6623 | Time 1.2019(1.2080) | Loss 3.104438(3.061992) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6624 | Time 1.2173(1.2086) | Loss 3.047578(3.060983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6625 | Time 1.1960(1.2078) | Loss 3.068700(3.061523) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6626 | Time 1.2540(1.2110) | Loss 3.141561(3.067126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6627 | Time 1.2089(1.2108) | Loss 3.056052(3.066351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6628 | Time 1.2134(1.2110) | Loss 3.106317(3.069148) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6629 | Time 1.2067(1.2107) | Loss 3.088559(3.070507) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6630 | Time 1.2533(1.2137) | Loss 3.056524(3.069528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6631 | Time 1.2211(1.2142) | Loss 3.081259(3.070349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6632 | Time 1.2112(1.2140) | Loss 3.072412(3.070494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6633 | Time 1.2153(1.2141) | Loss 3.079458(3.071121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6634 | Time 1.2127(1.2140) | Loss 3.088385(3.072330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6635 | Time 1.1961(1.2128) | Loss 3.041903(3.070200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6636 | Time 1.2010(1.2119) | Loss 3.056812(3.069263) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6637 | Time 1.2059(1.2115) | Loss 2.993034(3.063927) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6638 | Time 1.2417(1.2136) | Loss 3.082570(3.065232) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6639 | Time 1.2183(1.2139) | Loss 3.024450(3.062377) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6640 | Time 1.2192(1.2143) | Loss 3.079957(3.063608) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6641 | Time 1.2136(1.2143) | Loss 3.081203(3.064839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6642 | Time 1.2154(1.2143) | Loss 3.023327(3.061933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6643 | Time 1.2072(1.2138) | Loss 3.076485(3.062952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6644 | Time 1.2233(1.2145) | Loss 3.066201(3.063180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6645 | Time 1.2470(1.2168) | Loss 3.090386(3.065084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6646 | Time 1.2313(1.2178) | Loss 3.002801(3.060724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6647 | Time 1.2282(1.2185) | Loss 3.027906(3.058427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6648 | Time 1.2232(1.2189) | Loss 2.989890(3.053629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6649 | Time 1.2032(1.2178) | Loss 3.028228(3.051851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6650 | Time 1.2025(1.2167) | Loss 2.977859(3.046672) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6651 | Time 1.1817(1.2142) | Loss 3.077549(3.048833) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6652 | Time 1.1987(1.2132) | Loss 3.003303(3.045646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6653 | Time 1.2468(1.2155) | Loss 3.106465(3.049903) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6654 | Time 1.2018(1.2145) | Loss 3.033456(3.048752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6655 | Time 1.2164(1.2147) | Loss 3.029337(3.047393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6656 | Time 1.1920(1.2131) | Loss 3.099963(3.051073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6657 | Time 1.2028(1.2124) | Loss 2.965340(3.045072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6658 | Time 1.2119(1.2123) | Loss 3.052659(3.045603) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6659 | Time 1.1968(1.2112) | Loss 3.131727(3.051631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6660 | Time 1.1979(1.2103) | Loss 3.185343(3.060991) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6661 | Time 1.2016(1.2097) | Loss 3.117763(3.064965) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6662 | Time 1.2101(1.2097) | Loss 3.121200(3.068902) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6663 | Time 1.1875(1.2082) | Loss 3.093642(3.070634) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6664 | Time 1.2206(1.2091) | Loss 3.137685(3.075327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6665 | Time 1.2006(1.2085) | Loss 3.043076(3.073070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6666 | Time 1.2158(1.2090) | Loss 3.063155(3.072376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6667 | Time 1.2136(1.2093) | Loss 3.051218(3.070894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6668 | Time 1.2301(1.2108) | Loss 3.146583(3.076193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6669 | Time 1.2009(1.2101) | Loss 3.171438(3.082860) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6670 | Time 1.1922(1.2088) | Loss 3.141465(3.086962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6671 | Time 1.2226(1.2098) | Loss 3.156147(3.091805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6672 | Time 1.2503(1.2126) | Loss 3.135644(3.094874) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6673 | Time 1.2056(1.2121) | Loss 3.145633(3.098427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6674 | Time 1.2142(1.2123) | Loss 3.138861(3.101257) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6675 | Time 1.2191(1.2128) | Loss 3.138785(3.103884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6676 | Time 1.2482(1.2152) | Loss 3.093210(3.103137) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6677 | Time 1.2451(1.2173) | Loss 3.152833(3.106616) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6678 | Time 1.1955(1.2158) | Loss 3.107681(3.106690) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6679 | Time 1.2023(1.2149) | Loss 3.113769(3.107186) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6680 | Time 1.2554(1.2177) | Loss 3.023586(3.101334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6681 | Time 1.2289(1.2185) | Loss 3.096474(3.100994) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6682 | Time 1.2188(1.2185) | Loss 3.076158(3.099255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6683 | Time 1.2143(1.2182) | Loss 3.092286(3.098767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6684 | Time 1.2021(1.2171) | Loss 3.062793(3.096249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6685 | Time 1.1992(1.2158) | Loss 3.058868(3.093632) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6686 | Time 1.2048(1.2151) | Loss 3.024553(3.088797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6687 | Time 1.2458(1.2172) | Loss 3.005422(3.082961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6688 | Time 1.2498(1.2195) | Loss 3.003521(3.077400) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6689 | Time 1.2456(1.2213) | Loss 2.945292(3.068152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6690 | Time 1.2533(1.2236) | Loss 3.027240(3.065288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6691 | Time 1.2485(1.2253) | Loss 2.963013(3.058129) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6692 | Time 1.2637(1.2280) | Loss 3.003434(3.054300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6693 | Time 1.2192(1.2274) | Loss 2.956057(3.047423) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6694 | Time 1.2144(1.2265) | Loss 3.024777(3.045838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6695 | Time 1.2275(1.2265) | Loss 3.020668(3.044076) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6696 | Time 1.2194(1.2260) | Loss 2.962965(3.038398) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6697 | Time 1.2297(1.2263) | Loss 3.039841(3.038499) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6698 | Time 1.2310(1.2266) | Loss 3.048732(3.039216) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6699 | Time 1.2091(1.2254) | Loss 3.032740(3.038762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6700 | Time 1.2163(1.2248) | Loss 3.051861(3.039679) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6700 | Test Loss 3.028257 | NFE 20
Skipping vis as data dimension is >2
Iter 6701 | Time 1.2461(1.2263) | Loss 3.042375(3.039868) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6702 | Time 1.2166(1.2256) | Loss 3.012765(3.037971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6703 | Time 1.2081(1.2244) | Loss 3.049931(3.038808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6704 | Time 1.2134(1.2236) | Loss 3.038382(3.038778) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6705 | Time 1.2270(1.2238) | Loss 3.042195(3.039017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6706 | Time 1.2727(1.2272) | Loss 3.058258(3.040364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6707 | Time 1.2266(1.2272) | Loss 3.004684(3.037867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6708 | Time 1.2288(1.2273) | Loss 3.079595(3.040788) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6709 | Time 1.2227(1.2270) | Loss 3.016791(3.039108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6710 | Time 1.2179(1.2264) | Loss 3.054098(3.040157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6711 | Time 1.2464(1.2278) | Loss 3.060136(3.041556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6712 | Time 1.2781(1.2313) | Loss 3.066411(3.043296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6713 | Time 1.2592(1.2332) | Loss 3.060223(3.044480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6714 | Time 1.2553(1.2348) | Loss 3.030449(3.043498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6715 | Time 1.2196(1.2337) | Loss 3.097857(3.047303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6716 | Time 1.2072(1.2319) | Loss 3.070067(3.048897) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6717 | Time 1.2075(1.2302) | Loss 3.055501(3.049359) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6718 | Time 1.1950(1.2277) | Loss 2.984499(3.044819) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6719 | Time 1.2185(1.2271) | Loss 2.938678(3.037389) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6720 | Time 1.2215(1.2267) | Loss 3.034762(3.037205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6721 | Time 1.2120(1.2256) | Loss 2.969078(3.032436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6722 | Time 1.2094(1.2245) | Loss 3.043098(3.033183) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6723 | Time 1.1946(1.2224) | Loss 3.063860(3.035330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6724 | Time 1.2092(1.2215) | Loss 3.046675(3.036124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6725 | Time 1.2046(1.2203) | Loss 3.081049(3.039269) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6726 | Time 1.2337(1.2212) | Loss 2.961107(3.033798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6727 | Time 1.2478(1.2231) | Loss 3.089183(3.037675) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6728 | Time 1.2386(1.2242) | Loss 3.060117(3.039246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6729 | Time 1.2254(1.2243) | Loss 3.049648(3.039974) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6730 | Time 1.2174(1.2238) | Loss 3.035593(3.039667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6731 | Time 1.2128(1.2230) | Loss 3.042434(3.039861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6732 | Time 1.2036(1.2217) | Loss 3.078696(3.042579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6733 | Time 1.2176(1.2214) | Loss 3.024667(3.041325) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6734 | Time 1.2354(1.2224) | Loss 2.956758(3.035406) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6735 | Time 1.2401(1.2236) | Loss 3.042844(3.035926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6736 | Time 1.2220(1.2235) | Loss 3.000677(3.033459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6737 | Time 1.2199(1.2232) | Loss 3.068527(3.035914) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6738 | Time 1.2404(1.2244) | Loss 3.041193(3.036283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6739 | Time 1.2804(1.2284) | Loss 3.066470(3.038396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6740 | Time 1.2563(1.2303) | Loss 3.068582(3.040509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6741 | Time 1.2395(1.2310) | Loss 2.986504(3.036729) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6742 | Time 1.2339(1.2312) | Loss 3.023243(3.035785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6743 | Time 1.2434(1.2320) | Loss 3.061765(3.037604) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6744 | Time 1.2218(1.2313) | Loss 3.117483(3.043195) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6745 | Time 1.2276(1.2310) | Loss 3.097357(3.046986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6746 | Time 1.2332(1.2312) | Loss 3.090503(3.050033) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6747 | Time 1.2364(1.2316) | Loss 3.080826(3.052188) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6748 | Time 1.2372(1.2320) | Loss 2.980986(3.047204) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6749 | Time 1.2455(1.2329) | Loss 3.045512(3.047086) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6750 | Time 1.2399(1.2334) | Loss 3.101217(3.050875) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6751 | Time 1.2255(1.2328) | Loss 2.938074(3.042979) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6752 | Time 1.2327(1.2328) | Loss 2.992812(3.039467) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6753 | Time 1.2435(1.2336) | Loss 3.007940(3.037260) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6754 | Time 1.2263(1.2331) | Loss 2.962245(3.032009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6755 | Time 1.2221(1.2323) | Loss 3.067071(3.034463) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6756 | Time 1.2246(1.2318) | Loss 2.937571(3.027681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6757 | Time 1.2294(1.2316) | Loss 2.943062(3.021758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6758 | Time 1.2358(1.2319) | Loss 2.939197(3.015978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6759 | Time 1.2697(1.2345) | Loss 3.029873(3.016951) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6760 | Time 1.2614(1.2364) | Loss 2.991607(3.015177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6761 | Time 1.2638(1.2383) | Loss 3.046513(3.017370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6762 | Time 1.2377(1.2383) | Loss 2.981420(3.014854) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6763 | Time 1.2434(1.2387) | Loss 2.997072(3.013609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6764 | Time 1.2391(1.2387) | Loss 3.022882(3.014258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6765 | Time 1.2323(1.2382) | Loss 2.984531(3.012177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6766 | Time 1.2348(1.2380) | Loss 3.068055(3.016089) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6767 | Time 1.2348(1.2378) | Loss 2.959283(3.012112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6768 | Time 1.2296(1.2372) | Loss 3.064325(3.015767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6769 | Time 1.2508(1.2382) | Loss 2.977703(3.013103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6770 | Time 1.2806(1.2411) | Loss 3.012204(3.013040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6771 | Time 1.2490(1.2417) | Loss 3.056020(3.016048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6772 | Time 1.2544(1.2426) | Loss 3.132009(3.024166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6773 | Time 1.2408(1.2424) | Loss 3.089712(3.028754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6774 | Time 1.2693(1.2443) | Loss 3.030409(3.028870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6775 | Time 1.2702(1.2461) | Loss 3.105268(3.034218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6776 | Time 1.2718(1.2479) | Loss 3.000801(3.031878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6777 | Time 1.2512(1.2482) | Loss 3.088466(3.035840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6778 | Time 1.2298(1.2469) | Loss 3.107425(3.040851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6779 | Time 1.2393(1.2463) | Loss 3.031954(3.040228) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6780 | Time 1.2117(1.2439) | Loss 3.047551(3.040740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6781 | Time 1.2131(1.2418) | Loss 3.080478(3.043522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6782 | Time 1.1938(1.2384) | Loss 3.083626(3.046329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6783 | Time 1.1823(1.2345) | Loss 3.110045(3.050790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6784 | Time 1.1944(1.2317) | Loss 3.117059(3.055428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6785 | Time 1.1912(1.2288) | Loss 3.103261(3.058777) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6786 | Time 1.1985(1.2267) | Loss 3.112144(3.062512) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6787 | Time 1.2330(1.2272) | Loss 3.107858(3.065687) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6788 | Time 1.2103(1.2260) | Loss 3.134054(3.070472) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6789 | Time 1.2162(1.2253) | Loss 3.014777(3.066574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6790 | Time 1.2064(1.2240) | Loss 3.033956(3.064290) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6791 | Time 1.2192(1.2236) | Loss 3.057742(3.063832) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6792 | Time 1.2252(1.2237) | Loss 3.034458(3.061776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6793 | Time 1.2802(1.2277) | Loss 3.030272(3.059571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6794 | Time 1.3067(1.2332) | Loss 2.968089(3.053167) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6795 | Time 1.3171(1.2391) | Loss 3.012743(3.050337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6796 | Time 1.2638(1.2408) | Loss 3.014016(3.047795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6797 | Time 1.2701(1.2429) | Loss 2.992533(3.043926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6798 | Time 1.2489(1.2433) | Loss 2.983663(3.039708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6799 | Time 1.2161(1.2414) | Loss 3.019752(3.038311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6800 | Time 1.2209(1.2400) | Loss 2.906041(3.029052) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6800 | Test Loss 2.972904 | NFE 20
Skipping vis as data dimension is >2
Iter 6801 | Time 1.1935(1.2367) | Loss 3.024759(3.028752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6802 | Time 1.2048(1.2345) | Loss 3.021402(3.028237) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6803 | Time 1.2108(1.2328) | Loss 3.032277(3.028520) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6804 | Time 1.2045(1.2308) | Loss 3.033823(3.028891) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6805 | Time 1.2053(1.2290) | Loss 2.970579(3.024809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6806 | Time 1.2081(1.2276) | Loss 3.021065(3.024547) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6807 | Time 1.2246(1.2274) | Loss 2.962631(3.020213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6808 | Time 1.2193(1.2268) | Loss 3.058166(3.022870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6809 | Time 1.2091(1.2256) | Loss 2.968027(3.019031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6810 | Time 1.2170(1.2250) | Loss 2.967547(3.015427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6811 | Time 1.2093(1.2239) | Loss 3.022450(3.015919) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6812 | Time 1.2178(1.2234) | Loss 2.986858(3.013884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6813 | Time 1.2711(1.2268) | Loss 2.933908(3.008286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6814 | Time 1.2213(1.2264) | Loss 3.021154(3.009187) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6815 | Time 1.2172(1.2258) | Loss 3.036116(3.011072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6816 | Time 1.2125(1.2248) | Loss 3.044635(3.013421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6817 | Time 1.2395(1.2259) | Loss 3.039999(3.015282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6818 | Time 1.2179(1.2253) | Loss 3.111644(3.022027) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6819 | Time 1.2694(1.2284) | Loss 2.973228(3.018611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6820 | Time 1.2203(1.2278) | Loss 2.978502(3.015803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6821 | Time 1.2379(1.2285) | Loss 2.976710(3.013067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6822 | Time 1.3086(1.2341) | Loss 2.983490(3.010997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6823 | Time 1.2223(1.2333) | Loss 2.993286(3.009757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6824 | Time 1.1968(1.2307) | Loss 3.015171(3.010136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6825 | Time 1.2152(1.2297) | Loss 2.964320(3.006929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6826 | Time 1.2019(1.2277) | Loss 3.020378(3.007870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6827 | Time 1.2029(1.2260) | Loss 3.022670(3.008906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6828 | Time 1.2321(1.2264) | Loss 3.046947(3.011569) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6829 | Time 1.2040(1.2248) | Loss 2.994492(3.010374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6830 | Time 1.2046(1.2234) | Loss 3.016077(3.010773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6831 | Time 1.1939(1.2214) | Loss 3.058058(3.014083) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6832 | Time 1.2182(1.2211) | Loss 3.063384(3.017534) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6833 | Time 1.2377(1.2223) | Loss 3.015978(3.017425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6834 | Time 1.2163(1.2219) | Loss 3.045242(3.019372) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6835 | Time 1.2017(1.2205) | Loss 3.023880(3.019688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6836 | Time 1.2111(1.2198) | Loss 3.007945(3.018866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6837 | Time 1.2447(1.2216) | Loss 2.949460(3.014007) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6838 | Time 1.2791(1.2256) | Loss 2.949249(3.009474) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6839 | Time 1.2710(1.2288) | Loss 3.068724(3.013622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6840 | Time 1.2843(1.2327) | Loss 2.957992(3.009728) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6841 | Time 1.2670(1.2351) | Loss 3.042978(3.012055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6842 | Time 1.2693(1.2375) | Loss 3.057952(3.015268) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6843 | Time 1.3264(1.2437) | Loss 3.053912(3.017973) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6844 | Time 1.2343(1.2430) | Loss 2.971313(3.014707) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6845 | Time 1.2370(1.2426) | Loss 2.930749(3.008830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6846 | Time 1.2173(1.2408) | Loss 3.063939(3.012687) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6847 | Time 1.2170(1.2392) | Loss 3.048088(3.015165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6848 | Time 1.2110(1.2372) | Loss 3.090613(3.020447) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6849 | Time 1.1834(1.2334) | Loss 3.046452(3.022267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6850 | Time 1.1946(1.2307) | Loss 3.034179(3.023101) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6851 | Time 1.1896(1.2278) | Loss 3.042772(3.024478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6852 | Time 1.1805(1.2245) | Loss 3.114731(3.030796) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6853 | Time 1.1928(1.2223) | Loss 3.068658(3.033446) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6854 | Time 1.2007(1.2208) | Loss 3.045107(3.034262) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6855 | Time 1.1900(1.2186) | Loss 2.997719(3.031704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6856 | Time 1.1919(1.2168) | Loss 3.020777(3.030939) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6857 | Time 1.2150(1.2166) | Loss 3.027769(3.030717) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6858 | Time 1.1813(1.2142) | Loss 3.022742(3.030159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6859 | Time 1.1886(1.2124) | Loss 3.059328(3.032201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6860 | Time 1.1921(1.2109) | Loss 2.968856(3.027767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6861 | Time 1.2250(1.2119) | Loss 3.006578(3.026284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6862 | Time 1.2091(1.2117) | Loss 2.965562(3.022033) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6863 | Time 1.2298(1.2130) | Loss 3.044173(3.023583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6864 | Time 1.2332(1.2144) | Loss 3.001541(3.022040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6865 | Time 1.2590(1.2175) | Loss 2.965788(3.018102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6866 | Time 1.2508(1.2199) | Loss 3.010648(3.017581) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6867 | Time 1.2531(1.2222) | Loss 3.034677(3.018777) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6868 | Time 1.2409(1.2235) | Loss 3.131984(3.026702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6869 | Time 1.2488(1.2253) | Loss 3.000288(3.024853) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6870 | Time 1.2468(1.2268) | Loss 3.084130(3.029002) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6871 | Time 1.2596(1.2291) | Loss 3.023690(3.028630) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6872 | Time 1.2580(1.2311) | Loss 2.997188(3.026429) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6873 | Time 1.2493(1.2324) | Loss 3.058342(3.028663) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6874 | Time 1.2568(1.2341) | Loss 2.925623(3.021450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6875 | Time 1.2546(1.2355) | Loss 3.002451(3.020120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6876 | Time 1.2474(1.2363) | Loss 2.954318(3.015514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6877 | Time 1.2206(1.2352) | Loss 3.031541(3.016636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6878 | Time 1.2203(1.2342) | Loss 3.023809(3.017138) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6879 | Time 1.2424(1.2348) | Loss 3.017870(3.017189) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6880 | Time 1.2318(1.2346) | Loss 3.007056(3.016480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6881 | Time 1.2424(1.2351) | Loss 3.038702(3.018036) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6882 | Time 1.2474(1.2360) | Loss 2.991160(3.016154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6883 | Time 1.2502(1.2370) | Loss 2.932676(3.010311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6884 | Time 1.2439(1.2374) | Loss 2.998325(3.009472) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6885 | Time 1.2553(1.2387) | Loss 2.986701(3.007878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6886 | Time 1.2644(1.2405) | Loss 3.010961(3.008094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6887 | Time 1.2517(1.2413) | Loss 3.094945(3.014173) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6888 | Time 1.2654(1.2430) | Loss 2.996530(3.012938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6889 | Time 1.2517(1.2436) | Loss 3.024156(3.013723) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6890 | Time 1.2458(1.2437) | Loss 3.093945(3.019339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6891 | Time 1.2419(1.2436) | Loss 3.001105(3.018063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6892 | Time 1.2118(1.2414) | Loss 3.110285(3.024518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6893 | Time 1.2291(1.2405) | Loss 3.041756(3.025725) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6894 | Time 1.2161(1.2388) | Loss 3.051963(3.027561) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6895 | Time 1.2203(1.2375) | Loss 3.029898(3.027725) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6896 | Time 1.2518(1.2385) | Loss 3.058183(3.029857) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6897 | Time 1.2567(1.2398) | Loss 3.013386(3.028704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6898 | Time 1.2325(1.2393) | Loss 3.022904(3.028298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6899 | Time 1.2586(1.2406) | Loss 3.065610(3.030910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6900 | Time 1.2508(1.2413) | Loss 3.033899(3.031119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 6900 | Test Loss 2.983106 | NFE 20
Skipping vis as data dimension is >2
Iter 6901 | Time 1.2908(1.2448) | Loss 3.053518(3.032687) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6902 | Time 1.2593(1.2458) | Loss 3.023347(3.032033) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6903 | Time 1.2511(1.2462) | Loss 3.114099(3.037778) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6904 | Time 1.2390(1.2457) | Loss 3.039498(3.037898) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6905 | Time 1.2432(1.2455) | Loss 2.980610(3.033888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6906 | Time 1.2240(1.2440) | Loss 2.979027(3.030048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6907 | Time 1.2163(1.2421) | Loss 2.957397(3.024962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6908 | Time 1.2461(1.2423) | Loss 2.932957(3.018522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6909 | Time 1.2645(1.2439) | Loss 2.973350(3.015360) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6910 | Time 1.2169(1.2420) | Loss 2.972550(3.012363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6911 | Time 1.2524(1.2427) | Loss 2.959701(3.008677) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6912 | Time 1.2603(1.2440) | Loss 2.993785(3.007634) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6913 | Time 1.2674(1.2456) | Loss 2.926630(3.001964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6914 | Time 1.2497(1.2459) | Loss 3.007140(3.002326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6915 | Time 1.2615(1.2470) | Loss 3.005572(3.002554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6916 | Time 1.2557(1.2476) | Loss 3.005978(3.002793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6917 | Time 1.2917(1.2507) | Loss 3.002641(3.002783) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6918 | Time 1.2501(1.2506) | Loss 2.946662(2.998854) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6919 | Time 1.2849(1.2530) | Loss 2.956569(2.995894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6920 | Time 1.2545(1.2531) | Loss 2.998840(2.996100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6921 | Time 1.2646(1.2539) | Loss 2.974316(2.994576) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6922 | Time 1.2605(1.2544) | Loss 2.993470(2.994498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6923 | Time 1.2892(1.2568) | Loss 2.963973(2.992361) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6924 | Time 1.2942(1.2595) | Loss 3.014773(2.993930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6925 | Time 1.2491(1.2587) | Loss 3.038972(2.997083) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6926 | Time 1.2433(1.2576) | Loss 3.000512(2.997323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6927 | Time 1.2828(1.2594) | Loss 3.001677(2.997628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6928 | Time 1.2547(1.2591) | Loss 3.015231(2.998860) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6929 | Time 1.2601(1.2591) | Loss 3.073338(3.004074) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6930 | Time 1.2578(1.2590) | Loss 2.946427(3.000038) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6931 | Time 1.2846(1.2608) | Loss 3.012598(3.000917) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6932 | Time 1.2660(1.2612) | Loss 3.036914(3.003437) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6933 | Time 1.2721(1.2620) | Loss 3.059844(3.007386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6934 | Time 1.2604(1.2619) | Loss 2.983059(3.005683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6935 | Time 1.2574(1.2615) | Loss 3.091106(3.011662) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6936 | Time 1.2489(1.2607) | Loss 3.068998(3.015676) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6937 | Time 1.2539(1.2602) | Loss 3.020849(3.016038) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6938 | Time 1.2876(1.2621) | Loss 3.020483(3.016349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6939 | Time 1.2307(1.2599) | Loss 2.996323(3.014947) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6940 | Time 1.2323(1.2580) | Loss 3.022862(3.015501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6941 | Time 1.2086(1.2545) | Loss 3.078029(3.019878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6942 | Time 1.2115(1.2515) | Loss 2.969327(3.016340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6943 | Time 1.2074(1.2484) | Loss 2.991589(3.014607) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6944 | Time 1.2052(1.2454) | Loss 3.042162(3.016536) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6945 | Time 1.2486(1.2456) | Loss 3.044613(3.018501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6946 | Time 1.2147(1.2434) | Loss 2.990662(3.016553) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6947 | Time 1.2212(1.2419) | Loss 3.065758(3.019997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6948 | Time 1.2309(1.2411) | Loss 3.060115(3.022805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6949 | Time 1.2522(1.2419) | Loss 3.060365(3.025434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6950 | Time 1.2202(1.2404) | Loss 3.076094(3.028981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6951 | Time 1.2110(1.2383) | Loss 3.044884(3.030094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6952 | Time 1.2220(1.2372) | Loss 3.065598(3.032579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6953 | Time 1.2370(1.2372) | Loss 3.127634(3.039233) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6954 | Time 1.2396(1.2373) | Loss 3.124585(3.045208) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6955 | Time 1.2293(1.2368) | Loss 3.020141(3.043453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6956 | Time 1.2163(1.2353) | Loss 3.062014(3.044752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6957 | Time 1.2336(1.2352) | Loss 2.998763(3.041533) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6958 | Time 1.2159(1.2339) | Loss 3.029911(3.040719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6959 | Time 1.2300(1.2336) | Loss 3.007549(3.038397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6960 | Time 1.2542(1.2350) | Loss 3.055173(3.039572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6961 | Time 1.2770(1.2380) | Loss 3.082451(3.042573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6962 | Time 1.2377(1.2380) | Loss 3.106600(3.047055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6963 | Time 1.2598(1.2395) | Loss 3.042617(3.046744) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6964 | Time 1.2159(1.2378) | Loss 2.986079(3.042498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6965 | Time 1.2404(1.2380) | Loss 3.032422(3.041793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6966 | Time 1.2189(1.2367) | Loss 2.966128(3.036496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6967 | Time 1.2380(1.2368) | Loss 3.016847(3.035121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6968 | Time 1.2186(1.2355) | Loss 2.954387(3.029469) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6969 | Time 1.2310(1.2352) | Loss 2.971362(3.025402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6970 | Time 1.2167(1.2339) | Loss 2.956990(3.020613) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6971 | Time 1.2116(1.2323) | Loss 2.909106(3.012807) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6972 | Time 1.2439(1.2331) | Loss 2.962991(3.009320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6973 | Time 1.2236(1.2325) | Loss 2.971197(3.006652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6974 | Time 1.2371(1.2328) | Loss 2.937251(3.001794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6975 | Time 1.2618(1.2348) | Loss 2.908601(2.995270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6976 | Time 1.2278(1.2343) | Loss 2.981297(2.994292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6977 | Time 1.2117(1.2327) | Loss 2.925214(2.989457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6978 | Time 1.2239(1.2321) | Loss 3.011235(2.990981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6979 | Time 1.2451(1.2330) | Loss 2.991292(2.991003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6980 | Time 1.2182(1.2320) | Loss 2.959682(2.988810) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6981 | Time 1.2382(1.2324) | Loss 2.986724(2.988664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6982 | Time 1.2100(1.2309) | Loss 2.993709(2.989017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6983 | Time 1.1894(1.2280) | Loss 3.028002(2.991746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6984 | Time 1.2087(1.2266) | Loss 3.036345(2.994868) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6985 | Time 1.1910(1.2241) | Loss 3.015773(2.996332) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6986 | Time 1.2017(1.2225) | Loss 3.009845(2.997278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6987 | Time 1.2033(1.2212) | Loss 3.067561(3.002197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6988 | Time 1.1953(1.2194) | Loss 3.039226(3.004789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6989 | Time 1.2020(1.2182) | Loss 3.031780(3.006679) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6990 | Time 1.2069(1.2174) | Loss 3.086704(3.012280) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6991 | Time 1.2046(1.2165) | Loss 3.082884(3.017223) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6992 | Time 1.1954(1.2150) | Loss 3.013649(3.016973) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6993 | Time 1.2096(1.2146) | Loss 3.011485(3.016588) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6994 | Time 1.2061(1.2140) | Loss 2.991910(3.014861) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6995 | Time 1.1992(1.2130) | Loss 2.951995(3.010460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6996 | Time 1.2027(1.2123) | Loss 2.971230(3.007714) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6997 | Time 1.2077(1.2120) | Loss 2.928673(3.002181) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6998 | Time 1.2104(1.2118) | Loss 2.947687(2.998367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 6999 | Time 1.2345(1.2134) | Loss 2.961054(2.995755) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7000 | Time 1.2146(1.2135) | Loss 2.974333(2.994255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7000 | Test Loss 2.928828 | NFE 20
Skipping vis as data dimension is >2
Iter 7001 | Time 1.2009(1.2126) | Loss 2.989568(2.993927) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7002 | Time 1.2140(1.2127) | Loss 2.886509(2.986408) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7003 | Time 1.2009(1.2119) | Loss 2.988281(2.986539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7004 | Time 1.2061(1.2115) | Loss 2.944349(2.983586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7005 | Time 1.2214(1.2122) | Loss 2.954883(2.981577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7006 | Time 1.2600(1.2155) | Loss 2.907285(2.976376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7007 | Time 1.2128(1.2153) | Loss 3.022031(2.979572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7008 | Time 1.2121(1.2151) | Loss 2.973386(2.979139) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7009 | Time 1.2272(1.2160) | Loss 3.028926(2.982624) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7010 | Time 1.2070(1.2153) | Loss 2.955434(2.980721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7011 | Time 1.1959(1.2140) | Loss 3.014338(2.983074) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7012 | Time 1.2030(1.2132) | Loss 2.853184(2.973982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7013 | Time 1.1983(1.2122) | Loss 2.889644(2.968078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7014 | Time 1.2170(1.2125) | Loss 2.941135(2.966192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7015 | Time 1.2206(1.2131) | Loss 2.923584(2.963209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7016 | Time 1.2201(1.2136) | Loss 2.926308(2.960626) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7017 | Time 1.2220(1.2142) | Loss 3.016362(2.964528) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7018 | Time 1.2133(1.2141) | Loss 2.968560(2.964810) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7019 | Time 1.1974(1.2129) | Loss 2.947813(2.963620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7020 | Time 1.2065(1.2125) | Loss 3.045330(2.969340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7021 | Time 1.2143(1.2126) | Loss 2.965590(2.969077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7022 | Time 1.2304(1.2138) | Loss 2.984339(2.970146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7023 | Time 1.1977(1.2127) | Loss 2.969682(2.970113) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7024 | Time 1.1909(1.2112) | Loss 2.988425(2.971395) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7025 | Time 1.2340(1.2128) | Loss 3.001174(2.973480) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7026 | Time 1.2498(1.2154) | Loss 2.970955(2.973303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7027 | Time 1.2400(1.2171) | Loss 3.017436(2.976392) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7028 | Time 1.2284(1.2179) | Loss 2.916054(2.972169) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7029 | Time 1.2348(1.2191) | Loss 2.974603(2.972339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7030 | Time 1.2437(1.2208) | Loss 2.942629(2.970259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7031 | Time 1.2306(1.2215) | Loss 2.930950(2.967508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7032 | Time 1.2155(1.2211) | Loss 2.889049(2.962015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7033 | Time 1.2428(1.2226) | Loss 2.921426(2.959174) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7034 | Time 1.2244(1.2227) | Loss 2.964707(2.959562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7035 | Time 1.2169(1.2223) | Loss 2.983194(2.961216) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7036 | Time 1.2112(1.2215) | Loss 2.921359(2.958426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7037 | Time 1.1999(1.2200) | Loss 3.004186(2.961629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7038 | Time 1.2119(1.2194) | Loss 2.957786(2.961360) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7039 | Time 1.2179(1.2193) | Loss 3.001287(2.964155) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7040 | Time 1.2198(1.2194) | Loss 2.966742(2.964336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7041 | Time 1.2129(1.2189) | Loss 3.032660(2.969119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7042 | Time 1.2194(1.2189) | Loss 2.908758(2.964893) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7043 | Time 1.2052(1.2180) | Loss 2.927598(2.962283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7044 | Time 1.2224(1.2183) | Loss 2.979934(2.963518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7045 | Time 1.2109(1.2178) | Loss 3.034492(2.968487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7046 | Time 1.2185(1.2178) | Loss 2.974439(2.968903) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7047 | Time 1.2317(1.2188) | Loss 3.077020(2.976471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7048 | Time 1.2042(1.2178) | Loss 2.995947(2.977835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7049 | Time 1.2187(1.2178) | Loss 3.114040(2.987369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7050 | Time 1.2133(1.2175) | Loss 3.043198(2.991277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7051 | Time 1.1931(1.2158) | Loss 3.059128(2.996027) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7052 | Time 1.2300(1.2168) | Loss 3.064245(3.000802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7053 | Time 1.2845(1.2215) | Loss 3.106055(3.008170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7054 | Time 1.2726(1.2251) | Loss 3.152325(3.018261) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7055 | Time 1.2253(1.2251) | Loss 3.047000(3.020272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7056 | Time 1.2262(1.2252) | Loss 3.052782(3.022548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7057 | Time 1.2161(1.2246) | Loss 3.090339(3.027293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7058 | Time 1.2170(1.2240) | Loss 3.028137(3.027353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7059 | Time 1.2303(1.2245) | Loss 2.920101(3.019845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7060 | Time 1.2488(1.2262) | Loss 3.016635(3.019620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7061 | Time 1.2491(1.2278) | Loss 3.011633(3.019061) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7062 | Time 1.2346(1.2283) | Loss 3.020464(3.019159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7063 | Time 1.2144(1.2273) | Loss 2.935571(3.013308) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7064 | Time 1.2334(1.2277) | Loss 2.894673(3.005004) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7065 | Time 1.2259(1.2276) | Loss 2.972554(3.002732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7066 | Time 1.2595(1.2298) | Loss 3.002931(3.002746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7067 | Time 1.2452(1.2309) | Loss 2.981354(3.001249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7068 | Time 1.2001(1.2288) | Loss 2.938107(2.996829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7069 | Time 1.1921(1.2262) | Loss 3.003631(2.997305) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7070 | Time 1.2254(1.2261) | Loss 2.928276(2.992473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7071 | Time 1.2485(1.2277) | Loss 2.960591(2.990241) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7072 | Time 1.2832(1.2316) | Loss 2.950075(2.987430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7073 | Time 1.2477(1.2327) | Loss 2.929059(2.983344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7074 | Time 1.2698(1.2353) | Loss 2.966249(2.982147) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7075 | Time 1.2379(1.2355) | Loss 3.043844(2.986466) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7076 | Time 1.2294(1.2351) | Loss 2.965596(2.985005) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7077 | Time 1.2327(1.2349) | Loss 2.973436(2.984195) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7078 | Time 1.2324(1.2347) | Loss 2.961991(2.982641) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7079 | Time 1.2443(1.2354) | Loss 3.036630(2.986420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7080 | Time 1.2142(1.2339) | Loss 2.995888(2.987083) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7081 | Time 1.2154(1.2326) | Loss 2.985785(2.986992) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7082 | Time 1.2098(1.2310) | Loss 2.973608(2.986055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7083 | Time 1.2090(1.2295) | Loss 2.950335(2.983555) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7084 | Time 1.2237(1.2291) | Loss 2.962978(2.982114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7085 | Time 1.2005(1.2271) | Loss 2.901531(2.976473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7086 | Time 1.1857(1.2242) | Loss 2.928798(2.973136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7087 | Time 1.1921(1.2219) | Loss 2.912925(2.968921) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7088 | Time 1.2372(1.2230) | Loss 2.926961(2.965984) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7089 | Time 1.2366(1.2239) | Loss 2.869265(2.959214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7090 | Time 1.1973(1.2221) | Loss 2.849952(2.951565) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7091 | Time 1.2023(1.2207) | Loss 2.887917(2.947110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7092 | Time 1.1927(1.2187) | Loss 2.882164(2.942564) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7093 | Time 1.2042(1.2177) | Loss 2.923210(2.941209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7094 | Time 1.1920(1.2159) | Loss 2.897795(2.938170) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7095 | Time 1.2171(1.2160) | Loss 2.905564(2.935888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7096 | Time 1.2109(1.2156) | Loss 2.853480(2.930119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7097 | Time 1.2241(1.2162) | Loss 2.931822(2.930238) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7098 | Time 1.2771(1.2205) | Loss 2.930818(2.930279) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7099 | Time 1.2005(1.2191) | Loss 2.828962(2.923187) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7100 | Time 1.2281(1.2197) | Loss 2.896626(2.921327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7100 | Test Loss 2.892318 | NFE 20
Skipping vis as data dimension is >2
Iter 7101 | Time 1.2090(1.2190) | Loss 2.906550(2.920293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7102 | Time 1.1833(1.2165) | Loss 2.884295(2.917773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7103 | Time 1.2019(1.2155) | Loss 2.957679(2.920567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7104 | Time 1.2015(1.2145) | Loss 2.876482(2.917481) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7105 | Time 1.2051(1.2138) | Loss 2.958802(2.920373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7106 | Time 1.2262(1.2147) | Loss 2.939056(2.921681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7107 | Time 1.2069(1.2141) | Loss 2.931490(2.922368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7108 | Time 1.2091(1.2138) | Loss 3.010433(2.928532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7109 | Time 1.2052(1.2132) | Loss 2.959985(2.930734) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7110 | Time 1.2053(1.2126) | Loss 2.960948(2.932849) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7111 | Time 1.2129(1.2127) | Loss 2.930480(2.932683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7112 | Time 1.2072(1.2123) | Loss 2.931857(2.932625) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7113 | Time 1.2095(1.2121) | Loss 2.962785(2.934736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7114 | Time 1.2148(1.2123) | Loss 2.916043(2.933428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7115 | Time 1.1982(1.2113) | Loss 2.948345(2.934472) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7116 | Time 1.1987(1.2104) | Loss 2.935303(2.934530) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7117 | Time 1.2025(1.2099) | Loss 2.951190(2.935696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7118 | Time 1.1968(1.2089) | Loss 2.897175(2.933000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7119 | Time 1.1971(1.2081) | Loss 2.912822(2.931587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7120 | Time 1.2051(1.2079) | Loss 2.963314(2.933808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7121 | Time 1.1919(1.2068) | Loss 2.875026(2.929694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7122 | Time 1.1962(1.2060) | Loss 2.913250(2.928543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7123 | Time 1.1966(1.2054) | Loss 2.886481(2.925598) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7124 | Time 1.2270(1.2069) | Loss 2.880427(2.922436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7125 | Time 1.2171(1.2076) | Loss 2.911265(2.921654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7126 | Time 1.2765(1.2124) | Loss 2.872550(2.918217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7127 | Time 1.2224(1.2131) | Loss 2.857136(2.913941) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7128 | Time 1.2188(1.2135) | Loss 2.948916(2.916390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7129 | Time 1.2280(1.2145) | Loss 2.911574(2.916052) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7130 | Time 1.2415(1.2164) | Loss 2.895199(2.914593) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7131 | Time 1.2032(1.2155) | Loss 2.873329(2.911704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7132 | Time 1.2046(1.2147) | Loss 2.901698(2.911004) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7133 | Time 1.2470(1.2170) | Loss 2.933420(2.912573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7134 | Time 1.2028(1.2160) | Loss 2.942358(2.914658) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7135 | Time 1.2754(1.2202) | Loss 2.924307(2.915333) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7136 | Time 1.2208(1.2202) | Loss 2.890130(2.913569) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7137 | Time 1.1939(1.2184) | Loss 3.063992(2.924099) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7138 | Time 1.2071(1.2176) | Loss 2.941546(2.925320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7139 | Time 1.2206(1.2178) | Loss 2.990088(2.929854) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7140 | Time 1.2359(1.2191) | Loss 2.947911(2.931118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7141 | Time 1.2118(1.2185) | Loss 3.005507(2.936325) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7142 | Time 1.2086(1.2178) | Loss 2.966606(2.938445) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7143 | Time 1.2144(1.2176) | Loss 2.975549(2.941042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7144 | Time 1.2168(1.2175) | Loss 2.953553(2.941918) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7145 | Time 1.2054(1.2167) | Loss 2.999369(2.945939) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7146 | Time 1.1942(1.2151) | Loss 3.017964(2.950981) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7147 | Time 1.1903(1.2134) | Loss 2.930524(2.949549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7148 | Time 1.1810(1.2111) | Loss 2.996153(2.952811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7149 | Time 1.2034(1.2106) | Loss 2.883521(2.947961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7150 | Time 1.2034(1.2101) | Loss 2.998087(2.951470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7151 | Time 1.2119(1.2102) | Loss 2.976689(2.953235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7152 | Time 1.2465(1.2127) | Loss 2.962854(2.953909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7153 | Time 1.2751(1.2171) | Loss 2.945639(2.953330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7154 | Time 1.2737(1.2211) | Loss 2.997050(2.956390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7155 | Time 1.2740(1.2248) | Loss 3.010559(2.960182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7156 | Time 1.2840(1.2289) | Loss 2.887107(2.955067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7157 | Time 1.2448(1.2300) | Loss 2.972988(2.956321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7158 | Time 1.2381(1.2306) | Loss 2.981172(2.958061) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7159 | Time 1.2580(1.2325) | Loss 2.973520(2.959143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7160 | Time 1.2702(1.2351) | Loss 2.899712(2.954983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7161 | Time 1.2657(1.2373) | Loss 2.960019(2.955335) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7162 | Time 1.2349(1.2371) | Loss 2.955579(2.955352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7163 | Time 1.2515(1.2381) | Loss 2.984545(2.957396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7164 | Time 1.2482(1.2388) | Loss 2.970112(2.958286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7165 | Time 1.2652(1.2407) | Loss 2.985182(2.960169) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7166 | Time 1.2514(1.2414) | Loss 3.000355(2.962982) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7167 | Time 1.2056(1.2389) | Loss 2.992545(2.965051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7168 | Time 1.2109(1.2370) | Loss 2.986305(2.966539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7169 | Time 1.1965(1.2341) | Loss 3.032727(2.971172) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7170 | Time 1.1999(1.2317) | Loss 3.022777(2.974784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7171 | Time 1.1877(1.2286) | Loss 2.967127(2.974248) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7172 | Time 1.1831(1.2255) | Loss 3.071501(2.981056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7173 | Time 1.1867(1.2227) | Loss 3.082703(2.988171) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7174 | Time 1.2012(1.2212) | Loss 3.073256(2.994127) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7175 | Time 1.1936(1.2193) | Loss 3.075139(2.999798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7176 | Time 1.2039(1.2182) | Loss 3.089506(3.006078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7177 | Time 1.2342(1.2193) | Loss 2.986630(3.004716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7178 | Time 1.2089(1.2186) | Loss 3.065895(3.008999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7179 | Time 1.2123(1.2182) | Loss 3.029073(3.010404) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7180 | Time 1.1871(1.2160) | Loss 3.047134(3.012975) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7181 | Time 1.1862(1.2139) | Loss 3.072999(3.017177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7182 | Time 1.1956(1.2126) | Loss 3.013666(3.016931) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7183 | Time 1.1936(1.2113) | Loss 3.053561(3.019495) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7184 | Time 1.2036(1.2108) | Loss 3.007083(3.018626) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7185 | Time 1.2272(1.2119) | Loss 2.907238(3.010829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7186 | Time 1.2159(1.2122) | Loss 2.983057(3.008885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7187 | Time 1.2450(1.2145) | Loss 2.978541(3.006761) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7188 | Time 1.2031(1.2137) | Loss 2.973751(3.004450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7189 | Time 1.2140(1.2137) | Loss 3.005219(3.004504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7190 | Time 1.1956(1.2124) | Loss 2.890824(2.996546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7191 | Time 1.2666(1.2162) | Loss 2.959415(2.993947) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7192 | Time 1.2197(1.2165) | Loss 2.909612(2.988044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7193 | Time 1.1988(1.2152) | Loss 2.980620(2.987524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7194 | Time 1.2036(1.2144) | Loss 2.921606(2.982910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7195 | Time 1.1976(1.2132) | Loss 2.997774(2.983950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7196 | Time 1.1910(1.2117) | Loss 2.876732(2.976445) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7197 | Time 1.1849(1.2098) | Loss 2.959107(2.975231) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7198 | Time 1.2287(1.2111) | Loss 2.953859(2.973735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7199 | Time 1.1968(1.2101) | Loss 2.950388(2.972101) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7200 | Time 1.2007(1.2095) | Loss 2.948341(2.970438) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7200 | Test Loss 2.934136 | NFE 20
Skipping vis as data dimension is >2
Iter 7201 | Time 1.1895(1.2081) | Loss 2.950934(2.969073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7202 | Time 1.1998(1.2075) | Loss 2.916978(2.965426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7203 | Time 1.1909(1.2063) | Loss 2.863267(2.958275) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7204 | Time 1.2117(1.2067) | Loss 2.908144(2.954766) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7205 | Time 1.2020(1.2064) | Loss 3.031018(2.960103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7206 | Time 1.2137(1.2069) | Loss 3.000149(2.962907) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7207 | Time 1.2136(1.2074) | Loss 3.044943(2.968649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7208 | Time 1.1994(1.2068) | Loss 2.963962(2.968321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7209 | Time 1.2225(1.2079) | Loss 3.002992(2.970748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7210 | Time 1.2112(1.2081) | Loss 2.995353(2.972470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7211 | Time 1.2424(1.2105) | Loss 3.008216(2.974973) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7212 | Time 1.2468(1.2131) | Loss 3.026507(2.978580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7213 | Time 1.2707(1.2171) | Loss 3.068694(2.984888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7214 | Time 1.2246(1.2176) | Loss 3.000135(2.985955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7215 | Time 1.2157(1.2175) | Loss 2.984950(2.985885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7216 | Time 1.2183(1.2176) | Loss 3.025063(2.988627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7217 | Time 1.2004(1.2164) | Loss 2.971825(2.987451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7218 | Time 1.2281(1.2172) | Loss 2.992574(2.987810) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7219 | Time 1.2056(1.2164) | Loss 3.040589(2.991504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7220 | Time 1.1913(1.2146) | Loss 3.002882(2.992301) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7221 | Time 1.2125(1.2145) | Loss 2.983141(2.991660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7222 | Time 1.2089(1.2141) | Loss 3.039755(2.995026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7223 | Time 1.1995(1.2130) | Loss 2.953241(2.992101) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7224 | Time 1.2032(1.2124) | Loss 3.056041(2.996577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7225 | Time 1.1868(1.2106) | Loss 3.024144(2.998507) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7226 | Time 1.1999(1.2098) | Loss 3.031450(3.000813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7227 | Time 1.1943(1.2087) | Loss 3.010274(3.001475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7228 | Time 1.2051(1.2085) | Loss 3.048969(3.004800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7229 | Time 1.2151(1.2089) | Loss 3.009625(3.005138) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7230 | Time 1.1907(1.2077) | Loss 3.004682(3.005106) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7231 | Time 1.1983(1.2070) | Loss 3.076611(3.010111) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7232 | Time 1.1978(1.2064) | Loss 2.959538(3.006571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7233 | Time 1.1883(1.2051) | Loss 2.997342(3.005925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7234 | Time 1.1864(1.2038) | Loss 3.010839(3.006269) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7235 | Time 1.1766(1.2019) | Loss 3.047466(3.009153) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7236 | Time 1.1778(1.2002) | Loss 3.038138(3.011182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7237 | Time 1.1743(1.1984) | Loss 2.983799(3.009265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7238 | Time 1.1996(1.1985) | Loss 3.050468(3.012149) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7239 | Time 1.1659(1.1962) | Loss 2.967369(3.009014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7240 | Time 1.1995(1.1964) | Loss 2.992045(3.007827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7241 | Time 1.2254(1.1984) | Loss 3.005742(3.007681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7242 | Time 1.1933(1.1981) | Loss 2.916129(3.001272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7243 | Time 1.2030(1.1984) | Loss 2.942011(2.997124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7244 | Time 1.1841(1.1974) | Loss 3.003977(2.997603) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7245 | Time 1.1843(1.1965) | Loss 3.012971(2.998679) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7246 | Time 1.1868(1.1958) | Loss 3.012567(2.999651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7247 | Time 1.2197(1.1975) | Loss 2.942468(2.995649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7248 | Time 1.1838(1.1965) | Loss 2.972720(2.994044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7249 | Time 1.1763(1.1951) | Loss 2.998226(2.994336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7250 | Time 1.2347(1.1979) | Loss 2.957056(2.991727) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7251 | Time 1.2286(1.2000) | Loss 2.966490(2.989960) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7252 | Time 1.2140(1.2010) | Loss 2.908841(2.984282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7253 | Time 1.2181(1.2022) | Loss 3.079843(2.990971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7254 | Time 1.2014(1.2022) | Loss 3.022616(2.993186) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7255 | Time 1.2268(1.2039) | Loss 2.987667(2.992800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7256 | Time 1.1993(1.2036) | Loss 2.975462(2.991586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7257 | Time 1.1910(1.2027) | Loss 2.989299(2.991426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7258 | Time 1.1794(1.2011) | Loss 2.940820(2.987884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7259 | Time 1.1915(1.2004) | Loss 2.925177(2.983494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7260 | Time 1.2390(1.2031) | Loss 2.903932(2.977925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7261 | Time 1.2189(1.2042) | Loss 2.943696(2.975529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7262 | Time 1.1905(1.2032) | Loss 2.955601(2.974134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7263 | Time 1.2057(1.2034) | Loss 2.934041(2.971327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7264 | Time 1.1839(1.2020) | Loss 2.900728(2.966385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7265 | Time 1.2069(1.2024) | Loss 2.944293(2.964839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7266 | Time 1.2081(1.2028) | Loss 2.905533(2.960687) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7267 | Time 1.1940(1.2022) | Loss 2.902488(2.956614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7268 | Time 1.2151(1.2031) | Loss 2.897518(2.952477) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7269 | Time 1.1930(1.2024) | Loss 2.866511(2.946459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7270 | Time 1.2028(1.2024) | Loss 2.965304(2.947778) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7271 | Time 1.2016(1.2023) | Loss 3.025134(2.953193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7272 | Time 1.2029(1.2024) | Loss 2.892619(2.948953) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7273 | Time 1.2068(1.2027) | Loss 2.980389(2.951154) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7274 | Time 1.1996(1.2025) | Loss 2.941273(2.950462) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7275 | Time 1.1913(1.2017) | Loss 3.002502(2.954105) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7276 | Time 1.1965(1.2013) | Loss 2.968987(2.955146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7277 | Time 1.1882(1.2004) | Loss 3.015946(2.959402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7278 | Time 1.2024(1.2005) | Loss 2.983413(2.961083) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7279 | Time 1.1981(1.2004) | Loss 2.967088(2.961504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7280 | Time 1.1971(1.2001) | Loss 2.917800(2.958444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7281 | Time 1.1830(1.1989) | Loss 2.952670(2.958040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7282 | Time 1.1907(1.1984) | Loss 3.114738(2.969009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7283 | Time 1.1868(1.1976) | Loss 2.979315(2.969730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7284 | Time 1.2093(1.1984) | Loss 2.996917(2.971633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7285 | Time 1.2138(1.1995) | Loss 3.029555(2.975688) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7286 | Time 1.1886(1.1987) | Loss 3.024379(2.979096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7287 | Time 1.1641(1.1963) | Loss 3.008116(2.981128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7288 | Time 1.1836(1.1954) | Loss 3.015821(2.983556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7289 | Time 1.1816(1.1944) | Loss 3.012132(2.985557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7290 | Time 1.1867(1.1939) | Loss 3.009185(2.987211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7291 | Time 1.1939(1.1939) | Loss 2.962576(2.985486) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7292 | Time 1.1932(1.1938) | Loss 2.930729(2.981653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7293 | Time 1.2287(1.1963) | Loss 3.028830(2.984955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7294 | Time 1.1835(1.1954) | Loss 2.936798(2.981584) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7295 | Time 1.1896(1.1950) | Loss 2.968887(2.980696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7296 | Time 1.2212(1.1968) | Loss 2.873572(2.973197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7297 | Time 1.2006(1.1971) | Loss 2.937011(2.970664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7298 | Time 1.1922(1.1967) | Loss 2.996859(2.972498) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7299 | Time 1.2084(1.1976) | Loss 2.963112(2.971841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7300 | Time 1.1965(1.1975) | Loss 2.903517(2.967058) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7300 | Test Loss 2.913642 | NFE 20
Skipping vis as data dimension is >2
Iter 7301 | Time 1.1919(1.1971) | Loss 3.011388(2.970161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7302 | Time 1.1842(1.1962) | Loss 2.932860(2.967550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7303 | Time 1.1878(1.1956) | Loss 2.963936(2.967297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7304 | Time 1.1907(1.1953) | Loss 2.950538(2.966124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7305 | Time 1.1817(1.1943) | Loss 2.941744(2.964417) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7306 | Time 1.2085(1.1953) | Loss 2.980883(2.965570) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7307 | Time 1.1840(1.1945) | Loss 2.956045(2.964903) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7308 | Time 1.1748(1.1931) | Loss 2.968219(2.965135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7309 | Time 1.1857(1.1926) | Loss 2.965601(2.965168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7310 | Time 1.2021(1.1933) | Loss 2.974510(2.965822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7311 | Time 1.1760(1.1921) | Loss 2.937941(2.963870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7312 | Time 1.1716(1.1906) | Loss 2.921139(2.960879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7313 | Time 1.1677(1.1890) | Loss 2.965353(2.961192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7314 | Time 1.1796(1.1884) | Loss 2.906885(2.957391) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7315 | Time 1.1780(1.1876) | Loss 3.026322(2.962216) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7316 | Time 1.1763(1.1868) | Loss 2.899700(2.957840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7317 | Time 1.1764(1.1861) | Loss 2.930596(2.955933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7318 | Time 1.1862(1.1861) | Loss 2.886244(2.951055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7319 | Time 1.1908(1.1864) | Loss 2.901734(2.947602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7320 | Time 1.1772(1.1858) | Loss 2.922647(2.945855) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7321 | Time 1.1749(1.1850) | Loss 2.945009(2.945796) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7322 | Time 1.1706(1.1840) | Loss 2.978865(2.948111) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7323 | Time 1.2138(1.1861) | Loss 2.979326(2.950296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7324 | Time 1.1873(1.1862) | Loss 2.935919(2.949289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7325 | Time 1.2034(1.1874) | Loss 2.954138(2.949629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7326 | Time 1.2184(1.1896) | Loss 2.947839(2.949504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7327 | Time 1.1911(1.1897) | Loss 2.968989(2.950868) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7328 | Time 1.1845(1.1893) | Loss 2.997392(2.954124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7329 | Time 1.1937(1.1896) | Loss 3.009849(2.958025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7330 | Time 1.2024(1.1905) | Loss 2.952470(2.957636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7331 | Time 1.1737(1.1893) | Loss 2.952025(2.957243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7332 | Time 1.1978(1.1899) | Loss 2.867855(2.950986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7333 | Time 1.1794(1.1892) | Loss 2.918380(2.948704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7334 | Time 1.1655(1.1875) | Loss 2.944684(2.948422) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7335 | Time 1.1756(1.1867) | Loss 2.859074(2.942168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7336 | Time 1.1747(1.1859) | Loss 2.918965(2.940544) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7337 | Time 1.1796(1.1854) | Loss 2.841896(2.933638) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7338 | Time 1.1699(1.1843) | Loss 2.939986(2.934083) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7339 | Time 1.1682(1.1832) | Loss 2.844835(2.927835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7340 | Time 1.1563(1.1813) | Loss 2.866105(2.923514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7341 | Time 1.1644(1.1801) | Loss 2.933469(2.924211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7342 | Time 1.1921(1.1810) | Loss 2.968568(2.927316) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7343 | Time 1.1712(1.1803) | Loss 2.921316(2.926896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7344 | Time 1.1878(1.1808) | Loss 2.883774(2.923878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7345 | Time 1.1662(1.1798) | Loss 2.968910(2.927030) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7346 | Time 1.1661(1.1788) | Loss 2.924974(2.926886) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7347 | Time 1.1678(1.1781) | Loss 2.954634(2.928828) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7348 | Time 1.1997(1.1796) | Loss 2.896989(2.926600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7349 | Time 1.1751(1.1793) | Loss 2.937283(2.927347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7350 | Time 1.1606(1.1780) | Loss 2.901650(2.925549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7351 | Time 1.2184(1.1808) | Loss 2.931875(2.925991) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7352 | Time 1.1871(1.1812) | Loss 2.917225(2.925378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7353 | Time 1.1901(1.1818) | Loss 2.928168(2.925573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7354 | Time 1.1885(1.1823) | Loss 2.895616(2.923476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7355 | Time 1.1876(1.1827) | Loss 2.923402(2.923471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7356 | Time 1.1988(1.1838) | Loss 3.020804(2.930284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7357 | Time 1.1887(1.1842) | Loss 2.975798(2.933470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7358 | Time 1.1985(1.1852) | Loss 2.917903(2.932381) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7359 | Time 1.1879(1.1854) | Loss 2.981886(2.935846) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7360 | Time 1.1804(1.1850) | Loss 2.952641(2.937022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7361 | Time 1.1794(1.1846) | Loss 3.055619(2.945323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7362 | Time 1.1778(1.1841) | Loss 3.021150(2.950631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7363 | Time 1.1824(1.1840) | Loss 3.063816(2.958554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7364 | Time 1.1793(1.1837) | Loss 2.995840(2.961164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7365 | Time 1.1700(1.1827) | Loss 2.996283(2.963623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7366 | Time 1.1831(1.1828) | Loss 2.891953(2.958606) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7367 | Time 1.1831(1.1828) | Loss 2.919693(2.955882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7368 | Time 1.1694(1.1819) | Loss 2.918275(2.953249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7369 | Time 1.1735(1.1813) | Loss 2.958940(2.953648) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7370 | Time 1.1822(1.1813) | Loss 2.992032(2.956335) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7371 | Time 1.1791(1.1812) | Loss 2.996911(2.959175) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7372 | Time 1.1726(1.1806) | Loss 2.972134(2.960082) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7373 | Time 1.1794(1.1805) | Loss 2.949653(2.959352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7374 | Time 1.1725(1.1799) | Loss 3.040621(2.965041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7375 | Time 1.1723(1.1794) | Loss 2.907317(2.961000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7376 | Time 1.1894(1.1801) | Loss 2.986340(2.962774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7377 | Time 1.1798(1.1801) | Loss 2.947852(2.961729) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7378 | Time 1.2018(1.1816) | Loss 2.946227(2.960644) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7379 | Time 1.2248(1.1846) | Loss 2.920373(2.957825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7380 | Time 1.1846(1.1846) | Loss 2.955693(2.957676) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7381 | Time 1.1768(1.1841) | Loss 2.976047(2.958962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7382 | Time 1.1725(1.1833) | Loss 3.009387(2.962492) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7383 | Time 1.1784(1.1829) | Loss 2.987008(2.964208) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7384 | Time 1.1816(1.1828) | Loss 2.898240(2.959590) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7385 | Time 1.1815(1.1827) | Loss 2.902279(2.955578) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7386 | Time 1.1814(1.1826) | Loss 2.973881(2.956860) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7387 | Time 1.1820(1.1826) | Loss 2.871667(2.950896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7388 | Time 1.1695(1.1817) | Loss 2.893396(2.946871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7389 | Time 1.1705(1.1809) | Loss 2.969393(2.948448) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7390 | Time 1.1749(1.1805) | Loss 2.905117(2.945414) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7391 | Time 1.1903(1.1812) | Loss 2.967092(2.946932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7392 | Time 1.1750(1.1807) | Loss 2.989385(2.949904) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7393 | Time 1.1794(1.1806) | Loss 2.928354(2.948395) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7394 | Time 1.1838(1.1809) | Loss 2.969116(2.949846) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7395 | Time 1.1904(1.1815) | Loss 2.944528(2.949473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7396 | Time 1.1758(1.1811) | Loss 2.822055(2.940554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7397 | Time 1.1658(1.1801) | Loss 2.883864(2.936586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7398 | Time 1.1943(1.1810) | Loss 2.920020(2.935426) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7399 | Time 1.1702(1.1803) | Loss 2.929346(2.935000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7400 | Time 1.2230(1.1833) | Loss 2.899125(2.932489) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7400 | Test Loss 2.839391 | NFE 20
Skipping vis as data dimension is >2
Iter 7401 | Time 1.1731(1.1826) | Loss 2.901638(2.930330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7402 | Time 1.1827(1.1826) | Loss 2.903732(2.928468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7403 | Time 1.1672(1.1815) | Loss 2.899581(2.926446) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7404 | Time 1.1764(1.1811) | Loss 2.900986(2.924664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7405 | Time 1.1774(1.1809) | Loss 2.946302(2.926178) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7406 | Time 1.2078(1.1828) | Loss 2.899354(2.924301) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7407 | Time 1.1897(1.1833) | Loss 2.893384(2.922136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7408 | Time 1.2053(1.1848) | Loss 2.933448(2.922928) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7409 | Time 1.1790(1.1844) | Loss 2.944306(2.924425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7410 | Time 1.2132(1.1864) | Loss 2.971952(2.927752) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7411 | Time 1.1929(1.1869) | Loss 2.998930(2.932734) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7412 | Time 1.1967(1.1876) | Loss 2.929353(2.932497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7413 | Time 1.2090(1.1891) | Loss 2.999483(2.937186) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7414 | Time 1.2206(1.1913) | Loss 3.017808(2.942830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7415 | Time 1.2050(1.1922) | Loss 2.968665(2.944638) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7416 | Time 1.1919(1.1922) | Loss 2.970481(2.946447) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7417 | Time 1.1766(1.1911) | Loss 2.946984(2.946485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7418 | Time 1.1790(1.1903) | Loss 2.992994(2.949740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7419 | Time 1.1754(1.1892) | Loss 2.960503(2.950494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7420 | Time 1.1874(1.1891) | Loss 2.941638(2.949874) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7421 | Time 1.1974(1.1897) | Loss 2.968319(2.951165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7422 | Time 1.1913(1.1898) | Loss 2.944606(2.950706) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7423 | Time 1.1890(1.1897) | Loss 2.945995(2.950376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7424 | Time 1.1978(1.1903) | Loss 3.015325(2.954923) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7425 | Time 1.2006(1.1910) | Loss 2.917048(2.952271) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7426 | Time 1.1954(1.1913) | Loss 3.005941(2.956028) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7427 | Time 1.1704(1.1899) | Loss 2.992031(2.958548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7428 | Time 1.1932(1.1901) | Loss 3.012429(2.962320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7429 | Time 1.2047(1.1911) | Loss 2.954041(2.961741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7430 | Time 1.2120(1.1926) | Loss 2.992738(2.963910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7431 | Time 1.2016(1.1932) | Loss 2.915822(2.960544) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7432 | Time 1.1832(1.1925) | Loss 3.001574(2.963416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7433 | Time 1.2166(1.1942) | Loss 2.930839(2.961136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7434 | Time 1.1847(1.1935) | Loss 2.983031(2.962669) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7435 | Time 1.1666(1.1917) | Loss 2.953447(2.962023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7436 | Time 1.1649(1.1898) | Loss 2.976082(2.963007) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7437 | Time 1.1746(1.1887) | Loss 2.890454(2.957929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7438 | Time 1.1768(1.1879) | Loss 2.902675(2.954061) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7439 | Time 1.1989(1.1887) | Loss 2.917713(2.951516) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7440 | Time 1.2258(1.1913) | Loss 2.906451(2.948362) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7441 | Time 1.2009(1.1919) | Loss 2.986500(2.951032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7442 | Time 1.1820(1.1912) | Loss 2.942501(2.950434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7443 | Time 1.1871(1.1909) | Loss 3.032155(2.956155) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7444 | Time 1.1720(1.1896) | Loss 2.938347(2.954908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7445 | Time 1.1915(1.1898) | Loss 2.974929(2.956310) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7446 | Time 1.1880(1.1896) | Loss 2.921762(2.953891) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7447 | Time 1.1778(1.1888) | Loss 2.977510(2.955545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7448 | Time 1.1623(1.1870) | Loss 2.951107(2.955234) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7449 | Time 1.1770(1.1863) | Loss 3.000965(2.958435) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7450 | Time 1.1877(1.1864) | Loss 3.044899(2.964488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7451 | Time 1.1789(1.1858) | Loss 3.025864(2.968784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7452 | Time 1.1685(1.1846) | Loss 3.054635(2.974794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7453 | Time 1.1850(1.1846) | Loss 3.008073(2.977123) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7454 | Time 1.1675(1.1834) | Loss 3.021629(2.980239) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7455 | Time 1.1610(1.1819) | Loss 3.139052(2.991355) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7456 | Time 1.1717(1.1812) | Loss 3.014115(2.992949) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7457 | Time 1.1825(1.1813) | Loss 2.979788(2.992027) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7458 | Time 1.1754(1.1809) | Loss 3.027514(2.994511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7459 | Time 1.1845(1.1811) | Loss 3.029695(2.996974) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7460 | Time 1.2214(1.1839) | Loss 2.985048(2.996139) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7461 | Time 1.2571(1.1890) | Loss 2.988328(2.995593) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7462 | Time 1.1954(1.1895) | Loss 3.058360(2.999986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7463 | Time 1.1927(1.1897) | Loss 3.008795(3.000603) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7464 | Time 1.2038(1.1907) | Loss 2.987127(2.999660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7465 | Time 1.2083(1.1919) | Loss 2.899563(2.992653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7466 | Time 1.2201(1.1939) | Loss 2.987747(2.992309) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7467 | Time 1.2134(1.1953) | Loss 2.946136(2.989077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7468 | Time 1.2168(1.1968) | Loss 2.910667(2.983589) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7469 | Time 1.2116(1.1978) | Loss 2.964518(2.982254) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7470 | Time 1.2138(1.1989) | Loss 2.932724(2.978787) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7471 | Time 1.2048(1.1994) | Loss 2.912659(2.974158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7472 | Time 1.2269(1.2013) | Loss 2.908269(2.969545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7473 | Time 1.2092(1.2018) | Loss 2.873084(2.962793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7474 | Time 1.1924(1.2012) | Loss 2.886442(2.957449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7475 | Time 1.2276(1.2030) | Loss 2.869706(2.951307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7476 | Time 1.2052(1.2032) | Loss 2.887907(2.946869) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7477 | Time 1.1876(1.2021) | Loss 2.887349(2.942702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7478 | Time 1.1738(1.2001) | Loss 2.876480(2.938067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7479 | Time 1.1778(1.1985) | Loss 2.896332(2.935145) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7480 | Time 1.1856(1.1976) | Loss 2.968169(2.937457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7481 | Time 1.1706(1.1958) | Loss 2.993836(2.941403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7482 | Time 1.1543(1.1928) | Loss 2.911528(2.939312) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7483 | Time 1.1769(1.1917) | Loss 2.967282(2.941270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7484 | Time 1.1638(1.1898) | Loss 2.939458(2.941143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7485 | Time 1.1698(1.1884) | Loss 2.946982(2.941552) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7486 | Time 1.1753(1.1875) | Loss 2.954921(2.942488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7487 | Time 1.1839(1.1872) | Loss 3.047743(2.949856) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7488 | Time 1.1822(1.1869) | Loss 3.003180(2.953588) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7489 | Time 1.2339(1.1902) | Loss 3.001581(2.956948) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7490 | Time 1.2163(1.1920) | Loss 2.972590(2.958043) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7491 | Time 1.1990(1.1925) | Loss 2.949907(2.957473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7492 | Time 1.1817(1.1917) | Loss 2.952058(2.957094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7493 | Time 1.1959(1.1920) | Loss 2.846001(2.949318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7494 | Time 1.1959(1.1923) | Loss 2.908684(2.946473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7495 | Time 1.2020(1.1930) | Loss 2.869992(2.941120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7496 | Time 1.2060(1.1939) | Loss 2.878090(2.936707) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7497 | Time 1.1988(1.1942) | Loss 2.908467(2.934731) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7498 | Time 1.1914(1.1940) | Loss 2.939628(2.935073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7499 | Time 1.1787(1.1930) | Loss 2.888741(2.931830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7500 | Time 1.1800(1.1920) | Loss 2.904086(2.929888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7500 | Test Loss 2.887491 | NFE 20
Skipping vis as data dimension is >2
Iter 7501 | Time 1.1759(1.1909) | Loss 2.927882(2.929748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7502 | Time 1.1835(1.1904) | Loss 2.818217(2.921941) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7503 | Time 1.1832(1.1899) | Loss 2.878705(2.918914) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7504 | Time 1.1820(1.1893) | Loss 2.864288(2.915090) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7505 | Time 1.2022(1.1902) | Loss 2.901058(2.914108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7506 | Time 1.1843(1.1898) | Loss 2.818173(2.907393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7507 | Time 1.1867(1.1896) | Loss 2.948590(2.910276) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7508 | Time 1.1882(1.1895) | Loss 2.911864(2.910387) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7509 | Time 1.2123(1.1911) | Loss 2.885363(2.908636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7510 | Time 1.1954(1.1914) | Loss 2.855702(2.904930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7511 | Time 1.2166(1.1932) | Loss 2.944873(2.907726) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7512 | Time 1.1816(1.1924) | Loss 2.895373(2.906862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7513 | Time 1.1967(1.1927) | Loss 2.938970(2.909109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7514 | Time 1.1892(1.1924) | Loss 2.979934(2.914067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7515 | Time 1.1921(1.1924) | Loss 2.883074(2.911897) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7516 | Time 1.2086(1.1935) | Loss 2.886478(2.910118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7517 | Time 1.1971(1.1938) | Loss 2.951309(2.913001) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7518 | Time 1.2231(1.1958) | Loss 2.974786(2.917326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7519 | Time 1.2124(1.1970) | Loss 3.005334(2.923487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7520 | Time 1.2108(1.1980) | Loss 2.942572(2.924823) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7521 | Time 1.2134(1.1990) | Loss 3.013921(2.931060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7522 | Time 1.2033(1.1993) | Loss 2.971704(2.933905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7523 | Time 1.2163(1.2005) | Loss 2.940308(2.934353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7524 | Time 1.2253(1.2023) | Loss 2.880490(2.930583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7525 | Time 1.2116(1.2029) | Loss 2.968330(2.933225) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7526 | Time 1.2137(1.2037) | Loss 2.950227(2.934415) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7527 | Time 1.1962(1.2031) | Loss 2.888870(2.931227) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7528 | Time 1.2144(1.2039) | Loss 2.960851(2.933301) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7529 | Time 1.1913(1.2030) | Loss 2.897266(2.930778) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7530 | Time 1.2083(1.2034) | Loss 2.991898(2.935057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7531 | Time 1.2060(1.2036) | Loss 2.931750(2.934825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7532 | Time 1.1868(1.2024) | Loss 2.966894(2.937070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7533 | Time 1.1792(1.2008) | Loss 2.947222(2.937781) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7534 | Time 1.1824(1.1995) | Loss 2.946692(2.938404) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7535 | Time 1.1843(1.1984) | Loss 2.912215(2.936571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7536 | Time 1.1842(1.1974) | Loss 2.950128(2.937520) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7537 | Time 1.2282(1.1996) | Loss 2.999075(2.941829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7538 | Time 1.1942(1.1992) | Loss 2.909528(2.939568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7539 | Time 1.1744(1.1975) | Loss 2.946495(2.940053) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7540 | Time 1.1669(1.1953) | Loss 2.959334(2.941402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7541 | Time 1.1731(1.1938) | Loss 2.908971(2.939132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7542 | Time 1.1843(1.1931) | Loss 3.011200(2.944177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7543 | Time 1.1887(1.1928) | Loss 2.926735(2.942956) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7544 | Time 1.2110(1.1941) | Loss 2.981441(2.945650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7545 | Time 1.1998(1.1945) | Loss 2.971427(2.947454) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7546 | Time 1.1824(1.1936) | Loss 2.934464(2.946545) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7547 | Time 1.1792(1.1926) | Loss 3.041042(2.953160) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7548 | Time 1.1853(1.1921) | Loss 2.921355(2.950933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7549 | Time 1.2046(1.1930) | Loss 2.993223(2.953894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7550 | Time 1.1793(1.1920) | Loss 3.020659(2.958567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7551 | Time 1.1771(1.1910) | Loss 3.031407(2.963666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7552 | Time 1.1737(1.1898) | Loss 2.925533(2.960997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7553 | Time 1.1887(1.1897) | Loss 2.979650(2.962302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7554 | Time 1.1771(1.1888) | Loss 2.988282(2.964121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7555 | Time 1.1753(1.1879) | Loss 3.032129(2.968882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7556 | Time 1.1786(1.1872) | Loss 2.952780(2.967754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7557 | Time 1.1670(1.1858) | Loss 2.933702(2.965371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7558 | Time 1.1875(1.1859) | Loss 2.972327(2.965858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7559 | Time 1.1806(1.1856) | Loss 2.965038(2.965800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7560 | Time 1.1826(1.1853) | Loss 3.031962(2.970432) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7561 | Time 1.1841(1.1853) | Loss 2.999850(2.972491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7562 | Time 1.1857(1.1853) | Loss 2.960495(2.971651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7563 | Time 1.1891(1.1856) | Loss 3.041532(2.976543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7564 | Time 1.1687(1.1844) | Loss 3.026165(2.980016) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7565 | Time 1.1780(1.1839) | Loss 3.012915(2.982319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7566 | Time 1.2395(1.1878) | Loss 3.042772(2.986551) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7567 | Time 1.2095(1.1893) | Loss 3.015021(2.988544) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7568 | Time 1.1779(1.1885) | Loss 2.945996(2.985566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7569 | Time 1.1710(1.1873) | Loss 2.968638(2.984381) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7570 | Time 1.1812(1.1869) | Loss 2.981786(2.984199) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7571 | Time 1.2102(1.1885) | Loss 2.941790(2.981230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7572 | Time 1.1834(1.1882) | Loss 2.983828(2.981412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7573 | Time 1.1844(1.1879) | Loss 2.961118(2.979992) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7574 | Time 1.1839(1.1876) | Loss 2.931505(2.976598) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7575 | Time 1.1940(1.1881) | Loss 2.995980(2.977954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7576 | Time 1.1975(1.1887) | Loss 2.930948(2.974664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7577 | Time 1.2138(1.1905) | Loss 2.929637(2.971512) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7578 | Time 1.1928(1.1906) | Loss 2.896071(2.966231) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7579 | Time 1.1844(1.1902) | Loss 2.988613(2.967798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7580 | Time 1.2091(1.1915) | Loss 2.989529(2.969319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7581 | Time 1.1924(1.1916) | Loss 2.955010(2.968317) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7582 | Time 1.1827(1.1910) | Loss 2.923262(2.965164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7583 | Time 1.1977(1.1914) | Loss 2.905922(2.961017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7584 | Time 1.2029(1.1922) | Loss 2.867101(2.954443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7585 | Time 1.1989(1.1927) | Loss 2.960859(2.954892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7586 | Time 1.1649(1.1908) | Loss 2.990830(2.957407) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7587 | Time 1.1709(1.1894) | Loss 2.899891(2.953381) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7588 | Time 1.2055(1.1905) | Loss 2.943760(2.952708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7589 | Time 1.2125(1.1920) | Loss 2.975623(2.954312) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7590 | Time 1.2228(1.1942) | Loss 3.006820(2.957987) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7591 | Time 1.1941(1.1942) | Loss 2.903502(2.954173) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7592 | Time 1.1972(1.1944) | Loss 2.980580(2.956022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7593 | Time 1.2170(1.1960) | Loss 2.886271(2.951139) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7594 | Time 1.1825(1.1950) | Loss 2.889370(2.946815) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7595 | Time 1.2290(1.1974) | Loss 2.864534(2.941056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7596 | Time 1.2261(1.1994) | Loss 2.849773(2.934666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7597 | Time 1.1901(1.1988) | Loss 2.856995(2.929229) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7598 | Time 1.1815(1.1976) | Loss 2.875620(2.925476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7599 | Time 1.2315(1.1999) | Loss 2.932906(2.925996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7600 | Time 1.2531(1.2037) | Loss 2.931146(2.926357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7600 | Test Loss 2.894123 | NFE 20
Skipping vis as data dimension is >2
Iter 7601 | Time 1.2192(1.2047) | Loss 2.928826(2.926530) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7602 | Time 1.2071(1.2049) | Loss 2.877789(2.923118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7603 | Time 1.2076(1.2051) | Loss 2.877240(2.919907) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7604 | Time 1.1969(1.2045) | Loss 2.859246(2.915660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7605 | Time 1.1984(1.2041) | Loss 2.856154(2.911495) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7606 | Time 1.1745(1.2020) | Loss 2.919048(2.912024) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7607 | Time 1.1830(1.2007) | Loss 2.846149(2.907412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7608 | Time 1.1885(1.1998) | Loss 2.883234(2.905720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7609 | Time 1.1778(1.1983) | Loss 2.949592(2.908791) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7610 | Time 1.1938(1.1980) | Loss 2.914888(2.909218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7611 | Time 1.1916(1.1975) | Loss 2.883314(2.907405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7612 | Time 1.1902(1.1970) | Loss 2.912873(2.907787) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7613 | Time 1.1720(1.1953) | Loss 2.860114(2.904450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7614 | Time 1.1847(1.1945) | Loss 2.954874(2.907980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7615 | Time 1.1749(1.1932) | Loss 2.965373(2.911997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7616 | Time 1.1714(1.1916) | Loss 2.963463(2.915600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7617 | Time 1.1894(1.1915) | Loss 2.902034(2.914650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7618 | Time 1.1887(1.1913) | Loss 2.916184(2.914758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7619 | Time 1.1848(1.1908) | Loss 2.970787(2.918680) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7620 | Time 1.1976(1.1913) | Loss 2.907109(2.917870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7621 | Time 1.1970(1.1917) | Loss 3.008786(2.924234) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7622 | Time 1.2060(1.1927) | Loss 2.939025(2.925269) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7623 | Time 1.1983(1.1931) | Loss 3.020739(2.931952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7624 | Time 1.1880(1.1927) | Loss 2.980638(2.935360) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7625 | Time 1.1990(1.1932) | Loss 3.029794(2.941971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7626 | Time 1.2537(1.1974) | Loss 3.016911(2.947216) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7627 | Time 1.2096(1.1983) | Loss 3.004763(2.951245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7628 | Time 1.2026(1.1986) | Loss 2.979852(2.953247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7629 | Time 1.1926(1.1982) | Loss 2.974689(2.954748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7630 | Time 1.2069(1.1988) | Loss 3.052272(2.961575) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7631 | Time 1.2007(1.1989) | Loss 3.000108(2.964272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7632 | Time 1.1980(1.1988) | Loss 2.978489(2.965267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7633 | Time 1.2074(1.1994) | Loss 2.932153(2.962949) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7634 | Time 1.1981(1.1993) | Loss 2.915772(2.959647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7635 | Time 1.2033(1.1996) | Loss 2.937601(2.958104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7636 | Time 1.1824(1.1984) | Loss 2.978219(2.959512) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7637 | Time 1.1944(1.1981) | Loss 2.987230(2.961452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7638 | Time 1.1812(1.1970) | Loss 2.984394(2.963058) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7639 | Time 1.1719(1.1952) | Loss 2.905446(2.959025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7640 | Time 1.1806(1.1942) | Loss 2.937050(2.957487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7641 | Time 1.1749(1.1928) | Loss 2.904253(2.953760) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7642 | Time 1.1830(1.1921) | Loss 2.911343(2.950791) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7643 | Time 1.1767(1.1911) | Loss 3.003278(2.954465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7644 | Time 1.1753(1.1900) | Loss 2.892313(2.950115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7645 | Time 1.2590(1.1948) | Loss 2.958554(2.950705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7646 | Time 1.1871(1.1943) | Loss 2.974244(2.952353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7647 | Time 1.1746(1.1929) | Loss 2.918450(2.949980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7648 | Time 1.1869(1.1925) | Loss 2.929208(2.948526) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7649 | Time 1.2240(1.1947) | Loss 2.916162(2.946260) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7650 | Time 1.1690(1.1929) | Loss 2.942758(2.946015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7651 | Time 1.1759(1.1917) | Loss 2.971949(2.947831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7652 | Time 1.1754(1.1905) | Loss 2.917626(2.945716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7653 | Time 1.2262(1.1930) | Loss 2.869348(2.940370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7654 | Time 1.1842(1.1924) | Loss 2.943122(2.940563) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7655 | Time 1.1820(1.1917) | Loss 2.935829(2.940232) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7656 | Time 1.1775(1.1907) | Loss 2.944725(2.940546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7657 | Time 1.1925(1.1908) | Loss 2.904955(2.938055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7658 | Time 1.1846(1.1904) | Loss 2.931536(2.937599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7659 | Time 1.1792(1.1896) | Loss 2.916608(2.936129) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7660 | Time 1.1843(1.1892) | Loss 2.936149(2.936131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7661 | Time 1.1813(1.1887) | Loss 2.913862(2.934572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7662 | Time 1.1708(1.1874) | Loss 2.905026(2.932504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7663 | Time 1.1646(1.1858) | Loss 2.878386(2.928715) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7664 | Time 1.1732(1.1849) | Loss 2.929124(2.928744) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7665 | Time 1.1667(1.1837) | Loss 2.960958(2.930999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7666 | Time 1.1617(1.1821) | Loss 2.911820(2.929656) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7667 | Time 1.1557(1.1803) | Loss 2.971730(2.932602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7668 | Time 1.1692(1.1795) | Loss 2.899199(2.930263) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7669 | Time 1.1686(1.1787) | Loss 2.982790(2.933940) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7670 | Time 1.1856(1.1792) | Loss 2.970374(2.936491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7671 | Time 1.1716(1.1787) | Loss 2.897622(2.933770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7672 | Time 1.1806(1.1788) | Loss 2.904005(2.931686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7673 | Time 1.1818(1.1790) | Loss 2.864193(2.926962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7674 | Time 1.1826(1.1793) | Loss 2.869148(2.922915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7675 | Time 1.1761(1.1790) | Loss 2.886452(2.920362) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7676 | Time 1.1812(1.1792) | Loss 2.869403(2.916795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7677 | Time 1.1908(1.1800) | Loss 2.836494(2.911174) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7678 | Time 1.1818(1.1801) | Loss 2.888008(2.909553) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7679 | Time 1.1952(1.1812) | Loss 2.894713(2.908514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7680 | Time 1.1716(1.1805) | Loss 2.859347(2.905072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7681 | Time 1.2094(1.1825) | Loss 2.939448(2.907478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7682 | Time 1.2203(1.1852) | Loss 2.930152(2.909066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7683 | Time 1.2274(1.1881) | Loss 2.904874(2.908772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7684 | Time 1.2039(1.1892) | Loss 2.862955(2.905565) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7685 | Time 1.1959(1.1897) | Loss 2.896286(2.904915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7686 | Time 1.2403(1.1932) | Loss 2.939456(2.907333) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7687 | Time 1.2390(1.1965) | Loss 2.956536(2.910777) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7688 | Time 1.1983(1.1966) | Loss 2.952029(2.913665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7689 | Time 1.1932(1.1963) | Loss 2.899377(2.912665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7690 | Time 1.1999(1.1966) | Loss 2.838301(2.907459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7691 | Time 1.1998(1.1968) | Loss 2.863087(2.904353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7692 | Time 1.2040(1.1973) | Loss 2.826789(2.898924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7693 | Time 1.1946(1.1971) | Loss 2.852067(2.895644) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7694 | Time 1.1591(1.1945) | Loss 2.871357(2.893944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7695 | Time 1.1960(1.1946) | Loss 2.889819(2.893655) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7696 | Time 1.1856(1.1939) | Loss 2.901332(2.894192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7697 | Time 1.1770(1.1928) | Loss 2.921335(2.896092) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7698 | Time 1.1754(1.1915) | Loss 2.820326(2.890789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7699 | Time 1.1839(1.1910) | Loss 2.833225(2.886759) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7700 | Time 1.1842(1.1905) | Loss 2.820908(2.882150) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7700 | Test Loss 2.864143 | NFE 20
Skipping vis as data dimension is >2
Iter 7701 | Time 1.1819(1.1899) | Loss 2.866741(2.881071) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7702 | Time 1.1973(1.1904) | Loss 2.877628(2.880830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7703 | Time 1.1876(1.1902) | Loss 2.834316(2.877574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7704 | Time 1.1596(1.1881) | Loss 2.837590(2.874775) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7705 | Time 1.2249(1.1907) | Loss 2.895602(2.876233) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7706 | Time 1.2219(1.1929) | Loss 2.835992(2.873416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7707 | Time 1.1827(1.1922) | Loss 2.856048(2.872200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7708 | Time 1.2362(1.1952) | Loss 2.855014(2.870997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7709 | Time 1.1806(1.1942) | Loss 2.873967(2.871205) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7710 | Time 1.2033(1.1948) | Loss 2.854305(2.870022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7711 | Time 1.2097(1.1959) | Loss 2.954813(2.875958) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7712 | Time 1.1941(1.1958) | Loss 2.884908(2.876584) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7713 | Time 1.1972(1.1959) | Loss 2.913671(2.879180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7714 | Time 1.1736(1.1943) | Loss 2.911190(2.881421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7715 | Time 1.1743(1.1929) | Loss 2.911559(2.883531) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7716 | Time 1.1869(1.1925) | Loss 2.948512(2.888079) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7717 | Time 1.1913(1.1924) | Loss 2.903288(2.889144) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7718 | Time 1.1652(1.1905) | Loss 2.857979(2.886962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7719 | Time 1.2413(1.1941) | Loss 2.858991(2.885004) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7720 | Time 1.1962(1.1942) | Loss 2.852047(2.882697) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7721 | Time 1.1789(1.1931) | Loss 2.828234(2.878885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7722 | Time 1.1936(1.1932) | Loss 2.861873(2.877694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7723 | Time 1.1927(1.1931) | Loss 2.869050(2.877089) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7724 | Time 1.1905(1.1929) | Loss 2.830234(2.873809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7725 | Time 1.1757(1.1917) | Loss 2.818784(2.869957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7726 | Time 1.2074(1.1928) | Loss 2.800351(2.865085) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7727 | Time 1.1974(1.1932) | Loss 2.899550(2.867497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7728 | Time 1.1968(1.1934) | Loss 2.849814(2.866260) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7729 | Time 1.1935(1.1934) | Loss 2.887928(2.867776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7730 | Time 1.1987(1.1938) | Loss 2.837082(2.865628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7731 | Time 1.1899(1.1935) | Loss 2.816541(2.862192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7732 | Time 1.1850(1.1929) | Loss 2.859114(2.861976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7733 | Time 1.1972(1.1932) | Loss 2.834052(2.860022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7734 | Time 1.1940(1.1933) | Loss 2.809770(2.856504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7735 | Time 1.1718(1.1918) | Loss 2.774596(2.850771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7736 | Time 1.2294(1.1944) | Loss 2.784543(2.846135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7737 | Time 1.1907(1.1941) | Loss 2.760050(2.840109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7738 | Time 1.1779(1.1930) | Loss 2.869018(2.842132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7739 | Time 1.1876(1.1926) | Loss 2.776078(2.837508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7740 | Time 1.1819(1.1919) | Loss 2.837627(2.837517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7741 | Time 1.1897(1.1917) | Loss 2.945405(2.845069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7742 | Time 1.1768(1.1907) | Loss 2.806309(2.842356) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7743 | Time 1.1715(1.1893) | Loss 2.860390(2.843618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7744 | Time 1.1816(1.1888) | Loss 2.897687(2.847403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7745 | Time 1.1720(1.1876) | Loss 2.882004(2.849825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7746 | Time 1.1801(1.1871) | Loss 2.858202(2.850411) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7747 | Time 1.1984(1.1879) | Loss 2.942185(2.856836) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7748 | Time 1.1744(1.1869) | Loss 2.985405(2.865836) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7749 | Time 1.1710(1.1858) | Loss 2.959715(2.872407) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7750 | Time 1.1722(1.1849) | Loss 2.969667(2.879215) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7751 | Time 1.1747(1.1842) | Loss 2.890723(2.880021) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7752 | Time 1.1629(1.1827) | Loss 3.043342(2.891453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7753 | Time 1.1843(1.1828) | Loss 2.946670(2.895318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7754 | Time 1.1725(1.1821) | Loss 2.997211(2.902451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7755 | Time 1.1605(1.1806) | Loss 2.977007(2.907670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7756 | Time 1.1792(1.1805) | Loss 3.014875(2.915174) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7757 | Time 1.1685(1.1796) | Loss 2.986840(2.920191) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7758 | Time 1.1746(1.1793) | Loss 2.963192(2.923201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7759 | Time 1.1737(1.1789) | Loss 2.957732(2.925618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7760 | Time 1.2321(1.1826) | Loss 3.003677(2.931082) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7761 | Time 1.1686(1.1816) | Loss 2.969583(2.933777) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7762 | Time 1.1657(1.1805) | Loss 2.905288(2.931783) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7763 | Time 1.1819(1.1806) | Loss 2.897594(2.929390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7764 | Time 1.2060(1.1824) | Loss 2.937708(2.929972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7765 | Time 1.1902(1.1829) | Loss 2.884922(2.926819) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7766 | Time 1.1818(1.1829) | Loss 2.930925(2.927106) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7767 | Time 1.1793(1.1826) | Loss 3.065311(2.936780) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7768 | Time 1.1933(1.1834) | Loss 2.966763(2.938879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7769 | Time 1.1862(1.1836) | Loss 2.875275(2.934427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7770 | Time 1.2014(1.1848) | Loss 2.856833(2.928995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7771 | Time 1.2011(1.1859) | Loss 2.875054(2.925219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7772 | Time 1.1760(1.1852) | Loss 2.886431(2.922504) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7773 | Time 1.1683(1.1841) | Loss 2.894150(2.920519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7774 | Time 1.1770(1.1836) | Loss 2.886665(2.918150) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7775 | Time 1.1902(1.1840) | Loss 2.887439(2.916000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7776 | Time 1.1676(1.1829) | Loss 2.869105(2.912717) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7777 | Time 1.1853(1.1830) | Loss 2.879673(2.910404) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7778 | Time 1.1842(1.1831) | Loss 2.820458(2.904108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7779 | Time 1.1743(1.1825) | Loss 2.849407(2.900279) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7780 | Time 1.1655(1.1813) | Loss 2.902290(2.900420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7781 | Time 1.1626(1.1800) | Loss 2.884360(2.899295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7782 | Time 1.1627(1.1788) | Loss 2.969107(2.904182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7783 | Time 1.1781(1.1788) | Loss 2.911056(2.904663) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7784 | Time 1.1673(1.1780) | Loss 2.899626(2.904311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7785 | Time 1.1684(1.1773) | Loss 2.850481(2.900543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7786 | Time 1.1630(1.1763) | Loss 2.837677(2.896142) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7787 | Time 1.1723(1.1760) | Loss 2.870621(2.894356) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7788 | Time 1.1751(1.1759) | Loss 2.845320(2.890923) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7789 | Time 1.1524(1.1743) | Loss 2.877088(2.889955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7790 | Time 1.1708(1.1740) | Loss 2.865577(2.888248) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7791 | Time 1.1771(1.1743) | Loss 2.910939(2.889837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7792 | Time 1.2135(1.1770) | Loss 2.837378(2.886164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7793 | Time 1.1805(1.1772) | Loss 2.873341(2.885267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7794 | Time 1.1772(1.1772) | Loss 2.857847(2.883347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7795 | Time 1.1826(1.1776) | Loss 2.843353(2.880548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7796 | Time 1.1831(1.1780) | Loss 2.825891(2.876722) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7797 | Time 1.1997(1.1795) | Loss 2.795271(2.871020) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7798 | Time 1.1694(1.1788) | Loss 2.817128(2.867248) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7799 | Time 1.2363(1.1828) | Loss 2.813126(2.863459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7800 | Time 1.1828(1.1828) | Loss 2.849258(2.862465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7800 | Test Loss 2.776061 | NFE 20
Skipping vis as data dimension is >2
Iter 7801 | Time 1.1625(1.1814) | Loss 2.812743(2.858985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7802 | Time 1.1705(1.1806) | Loss 2.815275(2.855925) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7803 | Time 1.1728(1.1801) | Loss 2.838460(2.854702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7804 | Time 1.1720(1.1795) | Loss 2.857817(2.854920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7805 | Time 1.1714(1.1790) | Loss 2.849411(2.854535) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7806 | Time 1.1664(1.1781) | Loss 2.766161(2.848349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7807 | Time 1.1733(1.1778) | Loss 2.787334(2.844078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7808 | Time 1.1792(1.1779) | Loss 2.794929(2.840637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7809 | Time 1.1676(1.1771) | Loss 2.899169(2.844734) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7810 | Time 1.1804(1.1774) | Loss 2.860165(2.845815) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7811 | Time 1.1782(1.1774) | Loss 2.872866(2.847708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7812 | Time 1.1892(1.1782) | Loss 2.863953(2.848845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7813 | Time 1.1758(1.1781) | Loss 2.937126(2.855025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7814 | Time 1.1759(1.1779) | Loss 2.873947(2.856350) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7815 | Time 1.1962(1.1792) | Loss 2.901278(2.859494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7816 | Time 1.1728(1.1787) | Loss 2.889932(2.861625) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7817 | Time 1.1641(1.1777) | Loss 2.874350(2.862516) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7818 | Time 1.1851(1.1782) | Loss 2.979045(2.870673) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7819 | Time 1.2079(1.1803) | Loss 2.871193(2.870709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7820 | Time 1.1614(1.1790) | Loss 2.883034(2.871572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7821 | Time 1.2101(1.1812) | Loss 2.923402(2.875200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7822 | Time 1.1799(1.1811) | Loss 2.908039(2.877499) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7823 | Time 1.1786(1.1809) | Loss 2.880085(2.877680) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7824 | Time 1.1713(1.1802) | Loss 2.895905(2.878956) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7825 | Time 1.1841(1.1805) | Loss 2.949721(2.883909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7826 | Time 1.1915(1.1813) | Loss 2.969835(2.889924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7827 | Time 1.1897(1.1819) | Loss 2.919061(2.891964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7828 | Time 1.1779(1.1816) | Loss 2.934590(2.894947) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7829 | Time 1.1693(1.1807) | Loss 2.987742(2.901443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7830 | Time 1.1682(1.1799) | Loss 3.006802(2.908818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7831 | Time 1.1765(1.1796) | Loss 2.918351(2.909486) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7832 | Time 1.1724(1.1791) | Loss 3.044099(2.918908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7833 | Time 1.1560(1.1775) | Loss 3.015354(2.925660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7834 | Time 1.1647(1.1766) | Loss 2.937150(2.926464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7835 | Time 1.2148(1.1793) | Loss 2.956933(2.928597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7836 | Time 1.1601(1.1779) | Loss 2.955766(2.930499) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7837 | Time 1.1638(1.1769) | Loss 2.959111(2.932502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7838 | Time 1.1612(1.1758) | Loss 2.916811(2.931403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7839 | Time 1.1551(1.1744) | Loss 2.888541(2.928403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7840 | Time 1.1772(1.1746) | Loss 2.937962(2.929072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7841 | Time 1.1623(1.1737) | Loss 2.852595(2.923719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7842 | Time 1.1654(1.1731) | Loss 2.872673(2.920145) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7843 | Time 1.1696(1.1729) | Loss 2.867431(2.916455) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7844 | Time 1.1681(1.1726) | Loss 2.834747(2.910736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7845 | Time 1.1681(1.1722) | Loss 2.849491(2.906449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7846 | Time 1.1523(1.1709) | Loss 2.933495(2.908342) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7847 | Time 1.2121(1.1737) | Loss 2.872232(2.905814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7848 | Time 1.1708(1.1735) | Loss 2.839284(2.901157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7849 | Time 1.1864(1.1744) | Loss 2.966431(2.905726) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7850 | Time 1.1724(1.1743) | Loss 2.822864(2.899926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7851 | Time 1.2295(1.1782) | Loss 2.901454(2.900033) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7852 | Time 1.1666(1.1773) | Loss 2.797122(2.892829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7853 | Time 1.1460(1.1751) | Loss 2.794271(2.885930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7854 | Time 1.1573(1.1739) | Loss 2.849290(2.883365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7855 | Time 1.1632(1.1732) | Loss 2.862724(2.881920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7856 | Time 1.1563(1.1720) | Loss 2.903282(2.883416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7857 | Time 1.1559(1.1709) | Loss 2.853148(2.881297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7858 | Time 1.1675(1.1706) | Loss 2.881669(2.881323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7859 | Time 1.1615(1.1700) | Loss 2.814088(2.876616) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7860 | Time 1.1479(1.1684) | Loss 2.866879(2.875935) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7861 | Time 1.1714(1.1686) | Loss 2.905417(2.877999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7862 | Time 1.1648(1.1684) | Loss 2.860039(2.876741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7863 | Time 1.1668(1.1683) | Loss 2.926968(2.880257) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7864 | Time 1.2076(1.1710) | Loss 2.927119(2.883538) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7865 | Time 1.1964(1.1728) | Loss 2.890473(2.884023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7866 | Time 1.1627(1.1721) | Loss 2.921529(2.886649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7867 | Time 1.1575(1.1711) | Loss 2.927490(2.889507) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7868 | Time 1.1693(1.1709) | Loss 2.871301(2.888233) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7869 | Time 1.1563(1.1699) | Loss 2.921047(2.890530) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7870 | Time 1.1588(1.1691) | Loss 2.839947(2.886989) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7871 | Time 1.1713(1.1693) | Loss 2.891229(2.887286) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7872 | Time 1.2169(1.1726) | Loss 2.834225(2.883572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7873 | Time 1.1779(1.1730) | Loss 2.859867(2.881912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7874 | Time 1.1666(1.1725) | Loss 2.913785(2.884143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7875 | Time 1.1853(1.1734) | Loss 2.915293(2.886324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7876 | Time 1.1744(1.1735) | Loss 2.910943(2.888047) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7877 | Time 1.1631(1.1728) | Loss 2.854898(2.885727) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7878 | Time 1.1788(1.1732) | Loss 2.925922(2.888540) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7879 | Time 1.1818(1.1738) | Loss 2.922742(2.890935) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7880 | Time 1.1804(1.1743) | Loss 2.888346(2.890753) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7881 | Time 1.1810(1.1747) | Loss 2.855677(2.888298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7882 | Time 1.1793(1.1750) | Loss 2.896606(2.888880) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7883 | Time 1.1689(1.1746) | Loss 2.769267(2.880507) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7884 | Time 1.1780(1.1748) | Loss 2.840275(2.877690) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7885 | Time 1.1776(1.1750) | Loss 2.847342(2.875566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7886 | Time 1.1653(1.1744) | Loss 2.809683(2.870954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7887 | Time 1.1595(1.1733) | Loss 2.853750(2.869750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7888 | Time 1.1746(1.1734) | Loss 2.872815(2.869964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7889 | Time 1.1643(1.1728) | Loss 2.821431(2.866567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7890 | Time 1.1647(1.1722) | Loss 2.818725(2.863218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7891 | Time 1.1640(1.1716) | Loss 2.826884(2.860675) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7892 | Time 1.2014(1.1737) | Loss 2.751189(2.853011) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7893 | Time 1.2429(1.1786) | Loss 2.842989(2.852309) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7894 | Time 1.1649(1.1776) | Loss 2.854450(2.852459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7895 | Time 1.1744(1.1774) | Loss 2.867232(2.853493) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7896 | Time 1.1654(1.1765) | Loss 2.819220(2.851094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7897 | Time 1.1601(1.1754) | Loss 2.955239(2.858384) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7898 | Time 1.2008(1.1772) | Loss 2.860583(2.858538) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7899 | Time 1.1761(1.1771) | Loss 2.900631(2.861485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7900 | Time 1.1530(1.1754) | Loss 2.892109(2.863628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 7900 | Test Loss 2.828700 | NFE 20
Skipping vis as data dimension is >2
Iter 7901 | Time 1.2035(1.1774) | Loss 2.822942(2.860780) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7902 | Time 1.1811(1.1776) | Loss 2.856433(2.860476) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7903 | Time 1.1970(1.1790) | Loss 2.852658(2.859929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7904 | Time 1.1765(1.1788) | Loss 2.903355(2.862969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7905 | Time 1.1981(1.1802) | Loss 2.855091(2.862417) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7906 | Time 1.2502(1.1851) | Loss 2.817117(2.859246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7907 | Time 1.2578(1.1902) | Loss 2.947355(2.865414) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7908 | Time 1.2334(1.1932) | Loss 2.794108(2.860422) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7909 | Time 1.2568(1.1976) | Loss 2.826529(2.858050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7910 | Time 1.2308(1.2000) | Loss 2.860436(2.858217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7911 | Time 1.2166(1.2011) | Loss 2.855869(2.858052) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7912 | Time 1.1929(1.2005) | Loss 2.860659(2.858235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7913 | Time 1.1624(1.1979) | Loss 2.916681(2.862326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7914 | Time 1.1625(1.1954) | Loss 2.779077(2.856499) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7915 | Time 1.1591(1.1929) | Loss 2.826551(2.854402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7916 | Time 1.1612(1.1906) | Loss 2.828679(2.852602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7917 | Time 1.1620(1.1886) | Loss 2.875595(2.854211) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7918 | Time 1.1530(1.1861) | Loss 2.735259(2.845885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7919 | Time 1.1574(1.1841) | Loss 2.783362(2.841508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7920 | Time 1.1540(1.1820) | Loss 2.845614(2.841795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7921 | Time 1.1588(1.1804) | Loss 2.750648(2.835415) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7922 | Time 1.1847(1.1807) | Loss 2.861960(2.837273) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7923 | Time 1.1824(1.1808) | Loss 2.766590(2.832325) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7924 | Time 1.1752(1.1804) | Loss 2.809952(2.830759) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7925 | Time 1.1821(1.1805) | Loss 2.884381(2.834513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7926 | Time 1.1887(1.1811) | Loss 2.894429(2.838707) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7927 | Time 1.1683(1.1802) | Loss 2.873013(2.841108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7928 | Time 1.2425(1.1846) | Loss 2.883476(2.844074) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7929 | Time 1.2422(1.1886) | Loss 2.868845(2.845808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7930 | Time 1.2386(1.1921) | Loss 2.865226(2.847167) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7931 | Time 1.1987(1.1926) | Loss 2.885530(2.849853) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7932 | Time 1.1703(1.1910) | Loss 2.928284(2.855343) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7933 | Time 1.2106(1.1924) | Loss 2.889514(2.857735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7934 | Time 1.2320(1.1952) | Loss 2.854788(2.857529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7935 | Time 1.1923(1.1950) | Loss 2.845205(2.856666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7936 | Time 1.1949(1.1950) | Loss 2.943216(2.862724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7937 | Time 1.2257(1.1971) | Loss 2.905359(2.865709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7938 | Time 1.2400(1.2001) | Loss 2.928547(2.870107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7939 | Time 1.2177(1.2013) | Loss 2.808975(2.865828) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7940 | Time 1.1798(1.1998) | Loss 2.872050(2.866264) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7941 | Time 1.1666(1.1975) | Loss 2.797621(2.861459) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7942 | Time 1.1821(1.1964) | Loss 2.877178(2.862559) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7943 | Time 1.1839(1.1956) | Loss 2.881881(2.863912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7944 | Time 1.2468(1.1991) | Loss 2.851739(2.863060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7945 | Time 1.2311(1.2014) | Loss 2.853815(2.862412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7946 | Time 1.6161(1.2304) | Loss 2.863759(2.862507) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7947 | Time 1.2434(1.2313) | Loss 2.809209(2.858776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7948 | Time 1.2254(1.2309) | Loss 2.798755(2.854574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7949 | Time 1.2841(1.2346) | Loss 2.852523(2.854431) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7950 | Time 1.2312(1.2344) | Loss 2.824317(2.852323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7951 | Time 1.1870(1.2311) | Loss 2.840855(2.851520) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7952 | Time 1.2166(1.2301) | Loss 2.882764(2.853707) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7953 | Time 1.2618(1.2323) | Loss 2.816716(2.851118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7954 | Time 1.2496(1.2335) | Loss 2.854784(2.851374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7955 | Time 1.2438(1.2342) | Loss 2.788851(2.846998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7956 | Time 1.2459(1.2350) | Loss 2.889083(2.849944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7957 | Time 1.2746(1.2378) | Loss 2.873411(2.851586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7958 | Time 1.2398(1.2379) | Loss 2.848822(2.851393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7959 | Time 1.2603(1.2395) | Loss 2.774729(2.846026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7960 | Time 1.2995(1.2437) | Loss 2.805789(2.843210) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7961 | Time 1.2808(1.2463) | Loss 2.763995(2.837665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7962 | Time 1.2676(1.2478) | Loss 2.815193(2.836092) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7963 | Time 1.2444(1.2476) | Loss 2.856763(2.837539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7964 | Time 1.2218(1.2458) | Loss 2.838897(2.837634) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7965 | Time 1.2844(1.2485) | Loss 2.862580(2.839380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7966 | Time 1.2710(1.2500) | Loss 2.827503(2.838549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7967 | Time 1.2212(1.2480) | Loss 2.859502(2.840015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7968 | Time 1.2332(1.2470) | Loss 2.888017(2.843375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7969 | Time 1.2428(1.2467) | Loss 2.854040(2.844122) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7970 | Time 1.2248(1.2452) | Loss 2.892505(2.847509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7971 | Time 1.2188(1.2433) | Loss 2.900252(2.851201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7972 | Time 1.2217(1.2418) | Loss 2.872427(2.852687) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7973 | Time 1.2296(1.2409) | Loss 2.890558(2.855338) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7974 | Time 1.2154(1.2392) | Loss 2.846800(2.854740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7975 | Time 1.1966(1.2362) | Loss 2.884325(2.856811) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7976 | Time 1.1942(1.2332) | Loss 2.827934(2.854790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7977 | Time 1.2520(1.2346) | Loss 2.871636(2.855969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7978 | Time 1.1950(1.2318) | Loss 2.872387(2.857118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7979 | Time 1.2030(1.2298) | Loss 2.958367(2.864206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7980 | Time 1.1830(1.2265) | Loss 2.878682(2.865219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7981 | Time 1.2033(1.2249) | Loss 2.902273(2.867813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7982 | Time 1.1860(1.2222) | Loss 2.968241(2.874843) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7983 | Time 1.1931(1.2201) | Loss 2.915463(2.877686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7984 | Time 1.2179(1.2200) | Loss 2.881855(2.877978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7985 | Time 1.2026(1.2188) | Loss 2.861057(2.876794) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7986 | Time 1.2055(1.2178) | Loss 2.860345(2.875642) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7987 | Time 1.1998(1.2166) | Loss 2.889605(2.876620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7988 | Time 1.2143(1.2164) | Loss 2.885783(2.877261) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7989 | Time 1.2477(1.2186) | Loss 2.823649(2.873508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7990 | Time 1.2029(1.2175) | Loss 2.862849(2.872762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7991 | Time 1.2308(1.2184) | Loss 2.952304(2.878330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7992 | Time 1.1945(1.2168) | Loss 2.878578(2.878347) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7993 | Time 1.1949(1.2152) | Loss 2.874024(2.878045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7994 | Time 1.2242(1.2159) | Loss 2.845973(2.875800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7995 | Time 1.1900(1.2140) | Loss 2.833887(2.872866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7996 | Time 1.1851(1.2120) | Loss 2.841186(2.870648) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7997 | Time 1.1705(1.2091) | Loss 2.878816(2.871220) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7998 | Time 1.1786(1.2070) | Loss 2.881104(2.871912) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 7999 | Time 1.1804(1.2051) | Loss 2.867866(2.871629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8000 | Time 1.1897(1.2040) | Loss 2.900109(2.873622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8000 | Test Loss 2.842272 | NFE 20
Skipping vis as data dimension is >2
Iter 8001 | Time 1.1842(1.2026) | Loss 2.909065(2.876103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8002 | Time 1.1905(1.2018) | Loss 2.905584(2.878167) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8003 | Time 1.1854(1.2006) | Loss 2.904851(2.880035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8004 | Time 1.1745(1.1988) | Loss 2.883909(2.880306) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8005 | Time 1.2032(1.1991) | Loss 2.903014(2.881895) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8006 | Time 1.1801(1.1978) | Loss 2.891713(2.882583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8007 | Time 1.1786(1.1964) | Loss 2.871780(2.881826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8008 | Time 1.1972(1.1965) | Loss 2.878253(2.881576) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8009 | Time 1.2027(1.1969) | Loss 2.933635(2.885221) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8010 | Time 1.2131(1.1981) | Loss 2.881734(2.884976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8011 | Time 1.1879(1.1974) | Loss 2.857323(2.883041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8012 | Time 1.2033(1.1978) | Loss 2.843668(2.880285) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8013 | Time 1.1923(1.1974) | Loss 2.885639(2.880659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8014 | Time 1.1839(1.1964) | Loss 2.862545(2.879391) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8015 | Time 1.1792(1.1952) | Loss 2.884297(2.879735) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8016 | Time 1.1865(1.1946) | Loss 2.864610(2.878676) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8017 | Time 1.1895(1.1943) | Loss 2.874936(2.878414) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8018 | Time 1.1954(1.1943) | Loss 2.851877(2.876557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8019 | Time 1.2081(1.1953) | Loss 2.980980(2.883866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8020 | Time 1.2141(1.1966) | Loss 2.852068(2.881640) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8021 | Time 1.1822(1.1956) | Loss 2.904131(2.883215) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8022 | Time 1.1898(1.1952) | Loss 2.805096(2.877746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8023 | Time 1.1916(1.1949) | Loss 2.817933(2.873559) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8024 | Time 1.1932(1.1948) | Loss 2.747698(2.864749) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8025 | Time 1.1938(1.1948) | Loss 2.799663(2.860193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8026 | Time 1.2147(1.1962) | Loss 2.840855(2.858839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8027 | Time 1.2055(1.1968) | Loss 2.849252(2.858168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8028 | Time 1.1933(1.1966) | Loss 2.787369(2.853212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8029 | Time 1.2012(1.1969) | Loss 2.816613(2.850650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8030 | Time 1.2332(1.1994) | Loss 2.824870(2.848846) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8031 | Time 1.1957(1.1992) | Loss 2.778260(2.843905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8032 | Time 1.2246(1.2009) | Loss 2.771275(2.838821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8033 | Time 1.1838(1.1997) | Loss 2.827036(2.837996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8034 | Time 1.1847(1.1987) | Loss 2.835907(2.837850) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8035 | Time 1.2050(1.1991) | Loss 2.846883(2.838482) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8036 | Time 1.2132(1.2001) | Loss 2.858433(2.839878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8037 | Time 1.1837(1.1990) | Loss 2.839558(2.839856) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8038 | Time 1.2210(1.2005) | Loss 2.845188(2.840229) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8039 | Time 1.2099(1.2012) | Loss 2.905975(2.844831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8040 | Time 1.1899(1.2004) | Loss 2.859171(2.845835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8041 | Time 1.1994(1.2003) | Loss 2.907749(2.850169) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8042 | Time 1.1949(1.1999) | Loss 2.893192(2.853181) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8043 | Time 1.1903(1.1993) | Loss 2.889712(2.855738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8044 | Time 1.2516(1.2029) | Loss 2.891519(2.858243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8045 | Time 1.2231(1.2043) | Loss 2.850934(2.857731) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8046 | Time 1.2170(1.2052) | Loss 2.863967(2.858168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8047 | Time 1.2024(1.2050) | Loss 2.897772(2.860940) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8048 | Time 1.1792(1.2032) | Loss 2.822533(2.858251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8049 | Time 1.1894(1.2023) | Loss 2.868590(2.858975) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8050 | Time 1.1828(1.2009) | Loss 2.809609(2.855519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8051 | Time 1.1869(1.1999) | Loss 2.792626(2.851117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8052 | Time 1.1845(1.1988) | Loss 2.878511(2.853034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8053 | Time 1.1680(1.1967) | Loss 2.832916(2.851626) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8054 | Time 1.1791(1.1954) | Loss 2.838463(2.850705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8055 | Time 1.1950(1.1954) | Loss 2.855610(2.851048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8056 | Time 1.1848(1.1947) | Loss 2.849487(2.850939) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8057 | Time 1.1790(1.1936) | Loss 2.829548(2.849441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8058 | Time 1.1791(1.1926) | Loss 2.879842(2.851570) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8059 | Time 1.1962(1.1928) | Loss 2.851570(2.851570) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8060 | Time 1.1998(1.1933) | Loss 2.819675(2.849337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8061 | Time 1.1885(1.1930) | Loss 2.795547(2.845572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8062 | Time 1.2285(1.1955) | Loss 2.766295(2.840022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8063 | Time 1.1846(1.1947) | Loss 2.848972(2.840649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8064 | Time 1.1815(1.1938) | Loss 2.736654(2.833369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8065 | Time 1.2017(1.1943) | Loss 2.786953(2.830120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8066 | Time 1.2263(1.1966) | Loss 2.782144(2.826762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8067 | Time 1.1877(1.1959) | Loss 2.804666(2.825215) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8068 | Time 1.1894(1.1955) | Loss 2.796604(2.823212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8069 | Time 1.1873(1.1949) | Loss 2.759917(2.818782) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8070 | Time 1.1890(1.1945) | Loss 2.782799(2.816263) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8071 | Time 1.1802(1.1935) | Loss 2.757085(2.812120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8072 | Time 1.1881(1.1931) | Loss 2.778169(2.809744) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8073 | Time 1.1813(1.1923) | Loss 2.788365(2.808247) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8074 | Time 1.1671(1.1905) | Loss 2.808512(2.808266) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8075 | Time 1.1780(1.1896) | Loss 2.737145(2.803287) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8076 | Time 1.2061(1.1908) | Loss 2.742029(2.798999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8077 | Time 1.1799(1.1900) | Loss 2.754807(2.795906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8078 | Time 1.1816(1.1895) | Loss 2.778481(2.794686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8079 | Time 1.1650(1.1877) | Loss 2.809079(2.795694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8080 | Time 1.1697(1.1865) | Loss 2.776963(2.794382) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8081 | Time 1.1746(1.1856) | Loss 2.808131(2.795345) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8082 | Time 1.1761(1.1850) | Loss 2.853555(2.799420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8083 | Time 1.1709(1.1840) | Loss 2.755875(2.796371) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8084 | Time 1.1694(1.1830) | Loss 2.795871(2.796336) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8085 | Time 1.1655(1.1818) | Loss 2.739630(2.792367) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8086 | Time 1.1604(1.1803) | Loss 2.796454(2.792653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8087 | Time 1.1751(1.1799) | Loss 2.827955(2.795124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8088 | Time 1.1656(1.1789) | Loss 2.846496(2.798720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8089 | Time 1.2094(1.1810) | Loss 2.808894(2.799432) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8090 | Time 1.1740(1.1805) | Loss 2.883300(2.805303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8091 | Time 1.1793(1.1805) | Loss 2.840142(2.807742) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8092 | Time 1.1826(1.1806) | Loss 2.841533(2.810107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8093 | Time 1.1805(1.1806) | Loss 2.828153(2.811370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8094 | Time 1.2120(1.1828) | Loss 2.875471(2.815857) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8095 | Time 1.1783(1.1825) | Loss 2.839009(2.817478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8096 | Time 1.1688(1.1815) | Loss 2.830723(2.818405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8097 | Time 1.1786(1.1813) | Loss 2.847910(2.820471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8098 | Time 1.1651(1.1802) | Loss 2.861316(2.823330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8099 | Time 1.1790(1.1801) | Loss 2.837339(2.824310) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8100 | Time 1.1772(1.1799) | Loss 2.861954(2.826945) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8100 | Test Loss 2.849977 | NFE 20
Skipping vis as data dimension is >2
Iter 8101 | Time 1.1724(1.1794) | Loss 2.874436(2.830270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8102 | Time 1.1776(1.1793) | Loss 2.909566(2.835821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8103 | Time 1.1858(1.1797) | Loss 2.863557(2.837762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8104 | Time 1.1728(1.1792) | Loss 2.904352(2.842423) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8105 | Time 1.1910(1.1800) | Loss 2.837330(2.842067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8106 | Time 1.1932(1.1810) | Loss 2.832882(2.841424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8107 | Time 1.1836(1.1812) | Loss 2.830865(2.840685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8108 | Time 1.1644(1.1800) | Loss 2.797571(2.837667) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8109 | Time 1.1664(1.1790) | Loss 2.839045(2.837763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8110 | Time 1.1761(1.1788) | Loss 2.866422(2.839769) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8111 | Time 1.1884(1.1795) | Loss 2.781703(2.835705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8112 | Time 1.1691(1.1788) | Loss 2.767502(2.830931) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8113 | Time 1.1669(1.1779) | Loss 2.828696(2.830774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8114 | Time 1.1647(1.1770) | Loss 2.806242(2.829057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8115 | Time 1.1826(1.1774) | Loss 2.813898(2.827996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8116 | Time 1.1592(1.1761) | Loss 2.844430(2.829146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8117 | Time 1.1554(1.1747) | Loss 2.841338(2.830000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8118 | Time 1.1625(1.1738) | Loss 2.774402(2.826108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8119 | Time 1.1666(1.1733) | Loss 2.791796(2.823706) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8120 | Time 1.1659(1.1728) | Loss 2.862927(2.826451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8121 | Time 1.2040(1.1750) | Loss 2.821774(2.826124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8122 | Time 1.1870(1.1758) | Loss 2.770324(2.822218) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8123 | Time 1.1754(1.1758) | Loss 2.788989(2.819892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8124 | Time 1.1612(1.1748) | Loss 2.885505(2.824485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8125 | Time 1.1786(1.1750) | Loss 2.927393(2.831689) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8126 | Time 1.1664(1.1744) | Loss 2.907126(2.836969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8127 | Time 1.1978(1.1761) | Loss 2.851596(2.837993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8128 | Time 1.1680(1.1755) | Loss 2.857258(2.839342) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8129 | Time 1.1615(1.1745) | Loss 2.886178(2.842620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8130 | Time 1.1667(1.1740) | Loss 2.904145(2.846927) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8131 | Time 1.1892(1.1750) | Loss 2.855408(2.847521) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8132 | Time 1.1606(1.1740) | Loss 2.937938(2.853850) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8133 | Time 1.1558(1.1728) | Loss 2.890150(2.856391) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8134 | Time 1.1494(1.1711) | Loss 2.883892(2.858316) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8135 | Time 1.1578(1.1702) | Loss 2.925615(2.863027) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8136 | Time 1.1726(1.1704) | Loss 3.007119(2.873113) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8137 | Time 1.1489(1.1689) | Loss 2.782913(2.866799) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8138 | Time 1.1521(1.1677) | Loss 2.821668(2.863640) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8139 | Time 1.1522(1.1666) | Loss 2.800595(2.859227) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8140 | Time 1.1479(1.1653) | Loss 2.813765(2.856045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8141 | Time 1.1547(1.1646) | Loss 2.846711(2.855391) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8142 | Time 1.1920(1.1665) | Loss 2.849905(2.855007) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8143 | Time 1.1706(1.1668) | Loss 2.800058(2.851161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8144 | Time 1.1718(1.1671) | Loss 2.822817(2.849177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8145 | Time 1.1572(1.1664) | Loss 2.788602(2.844936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8146 | Time 1.1720(1.1668) | Loss 2.800035(2.841793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8147 | Time 1.1539(1.1659) | Loss 2.826032(2.840690) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8148 | Time 1.1884(1.1675) | Loss 2.765735(2.835443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8149 | Time 1.2458(1.1730) | Loss 2.798573(2.832862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8150 | Time 1.2271(1.1768) | Loss 2.849240(2.834009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8151 | Time 1.2061(1.1788) | Loss 2.776296(2.829969) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8152 | Time 1.1841(1.1792) | Loss 2.751409(2.824470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8153 | Time 1.1851(1.1796) | Loss 2.728084(2.817723) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8154 | Time 1.1835(1.1799) | Loss 2.757958(2.813539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8155 | Time 1.1784(1.1798) | Loss 2.745678(2.808789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8156 | Time 1.1888(1.1804) | Loss 2.760718(2.805424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8157 | Time 1.2037(1.1820) | Loss 2.761434(2.802345) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8158 | Time 1.1734(1.1814) | Loss 2.796650(2.801946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8159 | Time 1.1671(1.1804) | Loss 2.779765(2.800393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8160 | Time 1.1875(1.1809) | Loss 2.783440(2.799207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8161 | Time 1.1637(1.1797) | Loss 2.730935(2.794427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8162 | Time 1.1580(1.1782) | Loss 2.811941(2.795653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8163 | Time 1.1667(1.1774) | Loss 2.824516(2.797674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8164 | Time 1.1562(1.1759) | Loss 2.867755(2.802579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8165 | Time 1.1694(1.1755) | Loss 2.809061(2.803033) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8166 | Time 1.1820(1.1759) | Loss 2.786931(2.801906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8167 | Time 1.1846(1.1765) | Loss 2.912703(2.809662) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8168 | Time 1.1656(1.1758) | Loss 2.880095(2.814592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8169 | Time 1.1569(1.1744) | Loss 2.944019(2.823652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8170 | Time 1.1630(1.1736) | Loss 2.890590(2.828338) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8171 | Time 1.1762(1.1738) | Loss 2.849989(2.829853) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8172 | Time 1.1952(1.1753) | Loss 2.836264(2.830302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8173 | Time 1.1800(1.1756) | Loss 2.953623(2.838935) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8174 | Time 1.1724(1.1754) | Loss 2.920396(2.844637) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8175 | Time 1.1794(1.1757) | Loss 2.924349(2.850217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8176 | Time 1.1797(1.1760) | Loss 2.921225(2.855187) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8177 | Time 1.2133(1.1786) | Loss 2.883833(2.857193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8178 | Time 1.1869(1.1792) | Loss 2.900661(2.860235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8179 | Time 1.1750(1.1789) | Loss 2.818246(2.857296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8180 | Time 1.1826(1.1791) | Loss 2.838535(2.855983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8181 | Time 1.1750(1.1788) | Loss 2.834142(2.854454) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8182 | Time 1.1839(1.1792) | Loss 2.842167(2.853594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8183 | Time 1.1710(1.1786) | Loss 2.850494(2.853377) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8184 | Time 1.1657(1.1777) | Loss 2.901760(2.856764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8185 | Time 1.2072(1.1798) | Loss 2.800522(2.852827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8186 | Time 1.1683(1.1790) | Loss 2.748195(2.845503) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8187 | Time 1.1693(1.1783) | Loss 2.860441(2.846548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8188 | Time 1.1722(1.1779) | Loss 2.797233(2.843096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8189 | Time 1.1538(1.1762) | Loss 2.854135(2.843869) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8190 | Time 1.2048(1.1782) | Loss 2.843426(2.843838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8191 | Time 1.1692(1.1776) | Loss 2.778236(2.839246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8192 | Time 1.1620(1.1765) | Loss 2.817173(2.837701) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8193 | Time 1.1792(1.1767) | Loss 2.850415(2.838591) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8194 | Time 1.1633(1.1757) | Loss 2.746543(2.832147) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8195 | Time 1.1635(1.1749) | Loss 2.813164(2.830818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8196 | Time 1.1727(1.1747) | Loss 2.767856(2.826411) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8197 | Time 1.1598(1.1737) | Loss 2.765200(2.822126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8198 | Time 1.1769(1.1739) | Loss 2.741516(2.816484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8199 | Time 1.1599(1.1729) | Loss 2.753575(2.812080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8200 | Time 1.1599(1.1720) | Loss 2.721601(2.805746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8200 | Test Loss 2.686918 | NFE 20
Skipping vis as data dimension is >2
Iter 8201 | Time 1.1691(1.1718) | Loss 2.794652(2.804970) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8202 | Time 1.1612(1.1711) | Loss 2.802611(2.804805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8203 | Time 1.1684(1.1709) | Loss 2.762234(2.801825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8204 | Time 1.2003(1.1729) | Loss 2.749543(2.798165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8205 | Time 1.1860(1.1739) | Loss 2.772959(2.796401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8206 | Time 1.1773(1.1741) | Loss 2.835224(2.799118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8207 | Time 1.1803(1.1745) | Loss 2.811597(2.799992) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8208 | Time 1.1700(1.1742) | Loss 2.804321(2.800295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8209 | Time 1.1876(1.1752) | Loss 2.821427(2.801774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8210 | Time 1.1681(1.1747) | Loss 2.851671(2.805267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8211 | Time 1.2032(1.1767) | Loss 2.801033(2.804971) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8212 | Time 1.1761(1.1766) | Loss 2.819845(2.806012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8213 | Time 1.1781(1.1767) | Loss 2.778408(2.804080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8214 | Time 1.1917(1.1778) | Loss 2.861909(2.808128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8215 | Time 1.1688(1.1771) | Loss 2.876144(2.812889) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8216 | Time 1.1656(1.1763) | Loss 2.856126(2.815915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8217 | Time 1.1698(1.1759) | Loss 2.847701(2.818140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8218 | Time 1.1684(1.1754) | Loss 2.906769(2.824344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8219 | Time 1.1671(1.1748) | Loss 2.888720(2.828851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8220 | Time 1.1786(1.1750) | Loss 2.849907(2.830325) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8221 | Time 1.1777(1.1752) | Loss 2.852303(2.831863) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8222 | Time 1.1784(1.1755) | Loss 2.875355(2.834908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8223 | Time 1.1692(1.1750) | Loss 2.827369(2.834380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8224 | Time 1.1763(1.1751) | Loss 2.801998(2.832113) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8225 | Time 1.1691(1.1747) | Loss 2.869983(2.834764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8226 | Time 1.1630(1.1739) | Loss 2.842891(2.835333) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8227 | Time 1.1612(1.1730) | Loss 2.833701(2.835219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8228 | Time 1.1751(1.1731) | Loss 2.795809(2.832460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8229 | Time 1.1788(1.1735) | Loss 2.862057(2.834532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8230 | Time 1.2111(1.1762) | Loss 2.808799(2.832730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8231 | Time 1.2026(1.1780) | Loss 2.802305(2.830601) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8232 | Time 1.2112(1.1803) | Loss 2.831931(2.830694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8233 | Time 1.1866(1.1808) | Loss 2.789883(2.827837) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8234 | Time 1.1746(1.1803) | Loss 2.772302(2.823950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8235 | Time 1.1832(1.1805) | Loss 2.775463(2.820556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8236 | Time 1.1885(1.1811) | Loss 2.806190(2.819550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8237 | Time 1.1883(1.1816) | Loss 2.791489(2.817586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8238 | Time 1.1783(1.1814) | Loss 2.764354(2.813860) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8239 | Time 1.1752(1.1809) | Loss 2.812186(2.813742) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8240 | Time 1.1813(1.1810) | Loss 2.892465(2.819253) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8241 | Time 1.2040(1.1826) | Loss 2.795680(2.817603) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8242 | Time 1.2001(1.1838) | Loss 2.870846(2.821330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8243 | Time 1.1888(1.1841) | Loss 2.827064(2.821731) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8244 | Time 1.1887(1.1845) | Loss 2.844192(2.823303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8245 | Time 1.1808(1.1842) | Loss 2.881044(2.827345) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8246 | Time 1.1804(1.1839) | Loss 2.851112(2.829009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8247 | Time 1.1906(1.1844) | Loss 2.886390(2.833026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8248 | Time 1.1927(1.1850) | Loss 2.831573(2.832924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8249 | Time 1.2135(1.1870) | Loss 2.895099(2.837276) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8250 | Time 1.1759(1.1862) | Loss 2.860693(2.838915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8251 | Time 1.1807(1.1858) | Loss 2.828864(2.838212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8252 | Time 1.1748(1.1851) | Loss 2.786497(2.834592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8253 | Time 1.1783(1.1846) | Loss 2.831952(2.834407) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8254 | Time 1.1793(1.1842) | Loss 2.835298(2.834469) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8255 | Time 1.1722(1.1834) | Loss 2.849078(2.835492) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8256 | Time 1.1749(1.1828) | Loss 2.868031(2.837770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8257 | Time 1.1758(1.1823) | Loss 2.823663(2.836782) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8258 | Time 1.1748(1.1818) | Loss 2.826250(2.836045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8259 | Time 1.1717(1.1811) | Loss 2.812182(2.834375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8260 | Time 1.2005(1.1824) | Loss 2.900214(2.838983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8261 | Time 1.2044(1.1840) | Loss 2.795755(2.835957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8262 | Time 1.1781(1.1836) | Loss 2.725424(2.828220) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8263 | Time 1.1730(1.1828) | Loss 2.830099(2.828351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8264 | Time 1.1665(1.1817) | Loss 2.799938(2.826363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8265 | Time 1.1610(1.1802) | Loss 2.788550(2.823716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8266 | Time 1.1784(1.1801) | Loss 2.792053(2.821499) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8267 | Time 1.2002(1.1815) | Loss 2.825754(2.821797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8268 | Time 1.1996(1.1828) | Loss 2.844146(2.823362) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8269 | Time 1.1686(1.1818) | Loss 2.826639(2.823591) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8270 | Time 1.1563(1.1800) | Loss 2.767421(2.819659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8271 | Time 1.1691(1.1792) | Loss 2.873918(2.823457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8272 | Time 1.1694(1.1785) | Loss 2.811913(2.822649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8273 | Time 1.2292(1.1821) | Loss 2.822464(2.822636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8274 | Time 1.1726(1.1814) | Loss 2.817711(2.822291) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8275 | Time 1.1895(1.1820) | Loss 2.842711(2.823721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8276 | Time 1.1789(1.1818) | Loss 2.759347(2.819215) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8277 | Time 1.1855(1.1820) | Loss 2.779052(2.816403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8278 | Time 1.1651(1.1809) | Loss 2.736778(2.810829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8279 | Time 1.1808(1.1808) | Loss 2.815539(2.811159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8280 | Time 1.1910(1.1816) | Loss 2.787786(2.809523) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8281 | Time 1.1873(1.1820) | Loss 2.854828(2.812694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8282 | Time 1.1704(1.1811) | Loss 2.818164(2.813077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8283 | Time 1.1740(1.1806) | Loss 2.805774(2.812566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8284 | Time 1.1656(1.1796) | Loss 2.833874(2.814057) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8285 | Time 1.1644(1.1785) | Loss 2.821482(2.814577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8286 | Time 1.1579(1.1771) | Loss 2.814534(2.814574) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8287 | Time 1.1726(1.1768) | Loss 2.786127(2.812583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8288 | Time 1.2009(1.1785) | Loss 2.845062(2.814856) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8289 | Time 1.2022(1.1801) | Loss 2.768026(2.811578) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8290 | Time 1.1837(1.1804) | Loss 2.781655(2.809484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8291 | Time 1.1860(1.1808) | Loss 2.760851(2.806079) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8292 | Time 1.1855(1.1811) | Loss 2.809788(2.806339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8293 | Time 1.1897(1.1817) | Loss 2.851457(2.809497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8294 | Time 1.1779(1.1814) | Loss 2.786085(2.807858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8295 | Time 1.1699(1.1806) | Loss 2.765998(2.804928) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8296 | Time 1.1973(1.1818) | Loss 2.806164(2.805015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8297 | Time 1.1842(1.1820) | Loss 2.796581(2.804424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8298 | Time 1.1795(1.1818) | Loss 2.771741(2.802136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8299 | Time 1.1674(1.1808) | Loss 2.854676(2.805814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8300 | Time 1.1560(1.1790) | Loss 2.842949(2.808414) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8300 | Test Loss 2.793292 | NFE 20
Skipping vis as data dimension is >2
Iter 8301 | Time 1.1780(1.1790) | Loss 2.808719(2.808435) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8302 | Time 1.1550(1.1773) | Loss 2.725396(2.802622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8303 | Time 1.1781(1.1773) | Loss 2.808154(2.803009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8304 | Time 1.1802(1.1775) | Loss 2.758166(2.799870) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8305 | Time 1.1853(1.1781) | Loss 2.806591(2.800341) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8306 | Time 1.1805(1.1783) | Loss 2.753537(2.797065) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8307 | Time 1.1717(1.1778) | Loss 2.832024(2.799512) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8308 | Time 1.1803(1.1780) | Loss 2.759549(2.796714) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8309 | Time 1.1907(1.1789) | Loss 2.824580(2.798665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8310 | Time 1.1760(1.1787) | Loss 2.810503(2.799494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8311 | Time 1.1740(1.1783) | Loss 2.798231(2.799405) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8312 | Time 1.1906(1.1792) | Loss 2.809443(2.800108) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8313 | Time 1.1817(1.1794) | Loss 2.830789(2.802256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8314 | Time 1.1740(1.1790) | Loss 2.830039(2.804200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8315 | Time 1.2249(1.1822) | Loss 2.876892(2.809289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8316 | Time 1.2079(1.1840) | Loss 2.929737(2.817720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8317 | Time 1.1806(1.1838) | Loss 2.857852(2.820529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8318 | Time 1.1794(1.1835) | Loss 2.831211(2.821277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8319 | Time 1.1805(1.1832) | Loss 2.902351(2.826952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8320 | Time 1.1838(1.1833) | Loss 2.813643(2.826021) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8321 | Time 1.1823(1.1832) | Loss 2.847827(2.827547) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8322 | Time 1.2150(1.1854) | Loss 2.919537(2.833986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8323 | Time 1.1882(1.1856) | Loss 2.830218(2.833723) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8324 | Time 1.1730(1.1848) | Loss 2.808091(2.831928) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8325 | Time 1.1884(1.1850) | Loss 2.896560(2.836453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8326 | Time 1.1664(1.1837) | Loss 2.890285(2.840221) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8327 | Time 1.1766(1.1832) | Loss 2.856869(2.841386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8328 | Time 1.1744(1.1826) | Loss 2.821372(2.839985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8329 | Time 1.1916(1.1832) | Loss 2.914539(2.845204) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8330 | Time 1.1765(1.1828) | Loss 2.826169(2.843872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8331 | Time 1.1891(1.1832) | Loss 2.878526(2.846297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8332 | Time 1.2000(1.1844) | Loss 2.940091(2.852863) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8333 | Time 1.1747(1.1837) | Loss 2.881555(2.854871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8334 | Time 1.2091(1.1855) | Loss 2.849773(2.854515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8335 | Time 1.2211(1.1880) | Loss 2.881085(2.856374) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8336 | Time 1.1746(1.1870) | Loss 2.837055(2.855022) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8337 | Time 1.1674(1.1857) | Loss 2.853869(2.854941) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8338 | Time 1.1670(1.1843) | Loss 2.875509(2.856381) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8339 | Time 1.1740(1.1836) | Loss 2.825060(2.854189) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8340 | Time 1.1832(1.1836) | Loss 2.853753(2.854158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8341 | Time 1.1700(1.1826) | Loss 2.791032(2.849739) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8342 | Time 1.1956(1.1835) | Loss 2.879279(2.851807) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8343 | Time 1.2040(1.1850) | Loss 2.782194(2.846934) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8344 | Time 1.1986(1.1859) | Loss 2.828544(2.845647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8345 | Time 1.2095(1.1876) | Loss 2.824642(2.844177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8346 | Time 1.1784(1.1869) | Loss 2.852805(2.844781) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8347 | Time 1.2191(1.1892) | Loss 2.854490(2.845460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8348 | Time 1.1774(1.1884) | Loss 2.850693(2.845826) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8349 | Time 1.1841(1.1881) | Loss 2.887004(2.848709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8350 | Time 1.1696(1.1868) | Loss 2.812539(2.846177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8351 | Time 1.1935(1.1872) | Loss 2.843984(2.846023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8352 | Time 1.1770(1.1865) | Loss 2.828177(2.844774) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8353 | Time 1.1846(1.1864) | Loss 2.842597(2.844622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8354 | Time 1.1772(1.1858) | Loss 2.789320(2.840751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8355 | Time 1.1732(1.1849) | Loss 2.770280(2.835818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8356 | Time 1.1775(1.1844) | Loss 2.765882(2.830922) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8357 | Time 1.1752(1.1837) | Loss 2.804806(2.829094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8358 | Time 1.1666(1.1825) | Loss 2.853078(2.830773) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8359 | Time 1.1917(1.1832) | Loss 2.831349(2.830813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8360 | Time 1.1810(1.1830) | Loss 2.706627(2.822120) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8361 | Time 1.1809(1.1829) | Loss 2.738245(2.816249) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8362 | Time 1.1898(1.1833) | Loss 2.759782(2.812296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8363 | Time 1.1878(1.1837) | Loss 2.847175(2.814738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8364 | Time 1.2072(1.1853) | Loss 2.753594(2.810458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8365 | Time 1.1715(1.1843) | Loss 2.785815(2.808733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8366 | Time 1.1753(1.1837) | Loss 2.729590(2.803193) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8367 | Time 1.1700(1.1828) | Loss 2.736932(2.798555) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8368 | Time 1.2379(1.1866) | Loss 2.690340(2.790980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8369 | Time 1.2504(1.1911) | Loss 2.806146(2.792041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8370 | Time 1.1904(1.1910) | Loss 2.778403(2.791087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8371 | Time 1.2085(1.1923) | Loss 2.746694(2.787979) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8372 | Time 1.1945(1.1924) | Loss 2.793552(2.788369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8373 | Time 1.1784(1.1914) | Loss 2.749658(2.785659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8374 | Time 1.1930(1.1915) | Loss 2.757461(2.783686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8375 | Time 1.1850(1.1911) | Loss 2.827780(2.786772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8376 | Time 1.1975(1.1915) | Loss 2.767607(2.785431) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8377 | Time 1.1956(1.1918) | Loss 2.758812(2.783567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8378 | Time 1.2003(1.1924) | Loss 2.780680(2.783365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8379 | Time 1.1747(1.1912) | Loss 2.808950(2.785156) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8380 | Time 1.1886(1.1910) | Loss 2.728589(2.781196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8381 | Time 1.1800(1.1902) | Loss 2.766114(2.780141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8382 | Time 1.1665(1.1886) | Loss 2.803881(2.781803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8383 | Time 1.1781(1.1878) | Loss 2.714195(2.777070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8384 | Time 1.1606(1.1859) | Loss 2.719038(2.773008) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8385 | Time 1.1748(1.1852) | Loss 2.768255(2.772675) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8386 | Time 1.1559(1.1831) | Loss 2.803466(2.774830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8387 | Time 1.1704(1.1822) | Loss 2.822236(2.778149) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8388 | Time 1.1714(1.1815) | Loss 2.758399(2.776766) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8389 | Time 1.1751(1.1810) | Loss 2.758230(2.775469) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8390 | Time 1.1722(1.1804) | Loss 2.817481(2.778410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8391 | Time 1.1753(1.1800) | Loss 2.736910(2.775505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8392 | Time 1.1555(1.1783) | Loss 2.829254(2.779267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8393 | Time 1.1783(1.1783) | Loss 2.712691(2.774607) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8394 | Time 1.1685(1.1776) | Loss 2.747926(2.772739) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8395 | Time 1.1820(1.1779) | Loss 2.752807(2.771344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8396 | Time 1.1843(1.1784) | Loss 2.750919(2.769914) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8397 | Time 1.1584(1.1770) | Loss 2.795021(2.771672) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8398 | Time 1.1652(1.1761) | Loss 2.730487(2.768789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8399 | Time 1.1987(1.1777) | Loss 2.865581(2.775564) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8400 | Time 1.1806(1.1779) | Loss 2.800410(2.777303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8400 | Test Loss 2.737830 | NFE 20
Skipping vis as data dimension is >2
Iter 8401 | Time 1.2261(1.1813) | Loss 2.687618(2.771025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8402 | Time 1.2168(1.1838) | Loss 2.810836(2.773812) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8403 | Time 1.2500(1.1884) | Loss 2.819808(2.777032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8404 | Time 1.1932(1.1888) | Loss 2.792837(2.778138) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8405 | Time 1.1781(1.1880) | Loss 2.766464(2.777321) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8406 | Time 1.1863(1.1879) | Loss 2.751847(2.775538) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8407 | Time 1.1854(1.1877) | Loss 2.817922(2.778505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8408 | Time 1.1824(1.1873) | Loss 2.794025(2.779591) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8409 | Time 1.1981(1.1881) | Loss 2.808363(2.781605) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8410 | Time 1.1689(1.1868) | Loss 2.810373(2.783619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8411 | Time 1.1779(1.1861) | Loss 2.820626(2.786209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8412 | Time 1.1822(1.1859) | Loss 2.816165(2.788306) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8413 | Time 1.1922(1.1863) | Loss 2.839576(2.791895) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8414 | Time 1.1807(1.1859) | Loss 2.829567(2.794532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8415 | Time 1.1844(1.1858) | Loss 2.822833(2.796513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8416 | Time 1.1818(1.1855) | Loss 2.816440(2.797908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8417 | Time 1.1691(1.1844) | Loss 2.833901(2.800428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8418 | Time 1.1916(1.1849) | Loss 2.856818(2.804375) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8419 | Time 1.1743(1.1841) | Loss 2.899258(2.811017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8420 | Time 1.1738(1.1834) | Loss 2.850439(2.813776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8421 | Time 1.1726(1.1827) | Loss 2.830340(2.814936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8422 | Time 1.1855(1.1829) | Loss 2.844701(2.817019) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8423 | Time 1.1792(1.1826) | Loss 2.896677(2.822595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8424 | Time 1.1933(1.1833) | Loss 2.850339(2.824537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8425 | Time 1.1871(1.1836) | Loss 2.882951(2.828626) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8426 | Time 1.2169(1.1859) | Loss 2.831069(2.828797) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8427 | Time 1.1845(1.1858) | Loss 2.883032(2.832594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8428 | Time 1.2002(1.1868) | Loss 2.884936(2.836258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8429 | Time 1.2052(1.1881) | Loss 2.828074(2.835685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8430 | Time 1.1896(1.1882) | Loss 2.878478(2.838680) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8431 | Time 1.1755(1.1873) | Loss 2.862942(2.840379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8432 | Time 1.2466(1.1915) | Loss 2.904197(2.844846) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8433 | Time 1.1952(1.1918) | Loss 2.879337(2.847260) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8434 | Time 1.1851(1.1913) | Loss 2.811370(2.844748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8435 | Time 1.1903(1.1912) | Loss 2.833050(2.843929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8436 | Time 1.1814(1.1905) | Loss 2.802461(2.841026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8437 | Time 1.1929(1.1907) | Loss 2.922296(2.846715) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8438 | Time 1.2133(1.1923) | Loss 2.851654(2.847061) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8439 | Time 1.1775(1.1912) | Loss 2.869742(2.848649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8440 | Time 1.1774(1.1903) | Loss 2.841381(2.848140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8441 | Time 1.1766(1.1893) | Loss 2.812361(2.845635) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8442 | Time 1.1699(1.1880) | Loss 2.826950(2.844327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8443 | Time 1.1720(1.1868) | Loss 2.796276(2.840964) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8444 | Time 1.1958(1.1875) | Loss 2.814034(2.839079) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8445 | Time 1.1886(1.1876) | Loss 2.843464(2.839386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8446 | Time 1.1666(1.1861) | Loss 2.824668(2.838355) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8447 | Time 1.2368(1.1896) | Loss 2.820633(2.837115) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8448 | Time 1.1860(1.1894) | Loss 2.858919(2.838641) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8449 | Time 1.1861(1.1891) | Loss 2.866473(2.840589) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8450 | Time 1.2170(1.1911) | Loss 2.850177(2.841260) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8451 | Time 1.1745(1.1899) | Loss 2.841608(2.841285) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8452 | Time 1.1798(1.1892) | Loss 2.840499(2.841230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8453 | Time 1.1921(1.1894) | Loss 2.825913(2.840158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8454 | Time 1.2457(1.1934) | Loss 2.840203(2.840161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8455 | Time 1.2540(1.1976) | Loss 2.772631(2.835434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8456 | Time 1.2136(1.1987) | Loss 2.780723(2.831604) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8457 | Time 1.1984(1.1987) | Loss 2.816683(2.830559) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8458 | Time 1.1827(1.1976) | Loss 2.775068(2.826675) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8459 | Time 1.1985(1.1976) | Loss 2.821139(2.826288) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8460 | Time 1.1935(1.1974) | Loss 2.791161(2.823829) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8461 | Time 1.1935(1.1971) | Loss 2.785964(2.821178) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8462 | Time 1.2078(1.1978) | Loss 2.773660(2.817852) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8463 | Time 1.1823(1.1968) | Loss 2.789695(2.815881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8464 | Time 1.2189(1.1983) | Loss 2.775980(2.813088) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8465 | Time 1.1899(1.1977) | Loss 2.762789(2.809567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8466 | Time 1.1780(1.1963) | Loss 2.764950(2.806444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8467 | Time 1.1871(1.1957) | Loss 2.842489(2.808967) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8468 | Time 1.1834(1.1948) | Loss 2.807243(2.808846) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8469 | Time 1.1739(1.1934) | Loss 2.822376(2.809793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8470 | Time 1.1667(1.1915) | Loss 2.772869(2.807209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8471 | Time 1.1793(1.1906) | Loss 2.819247(2.808051) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8472 | Time 1.1899(1.1906) | Loss 2.783791(2.806353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8473 | Time 1.1978(1.1911) | Loss 2.816361(2.807054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8474 | Time 1.1732(1.1898) | Loss 2.799867(2.806551) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8475 | Time 1.1747(1.1888) | Loss 2.822467(2.807665) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8476 | Time 1.2113(1.1904) | Loss 2.811994(2.807968) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8477 | Time 1.1923(1.1905) | Loss 2.802385(2.807577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8478 | Time 1.1787(1.1897) | Loss 2.827112(2.808944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8479 | Time 1.1756(1.1887) | Loss 2.866417(2.812968) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8480 | Time 1.1941(1.1891) | Loss 2.788515(2.811256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8481 | Time 1.2062(1.1903) | Loss 2.838283(2.813148) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8482 | Time 1.2034(1.1912) | Loss 2.821273(2.813716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8483 | Time 1.1833(1.1906) | Loss 2.819363(2.814112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8484 | Time 1.1586(1.1884) | Loss 2.861658(2.817440) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8485 | Time 1.1941(1.1888) | Loss 2.779723(2.814800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8486 | Time 1.1779(1.1880) | Loss 2.802633(2.813948) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8487 | Time 1.1687(1.1867) | Loss 2.860875(2.817233) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8488 | Time 1.1717(1.1856) | Loss 2.775911(2.814340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8489 | Time 1.2060(1.1871) | Loss 2.825723(2.815137) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8490 | Time 1.1702(1.1859) | Loss 2.844105(2.817165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8491 | Time 1.1575(1.1839) | Loss 2.824852(2.817703) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8492 | Time 1.1661(1.1826) | Loss 2.790589(2.815805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8493 | Time 1.1663(1.1815) | Loss 2.807107(2.815196) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8494 | Time 1.1648(1.1803) | Loss 2.792641(2.813617) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8495 | Time 1.1736(1.1799) | Loss 2.847625(2.815998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8496 | Time 1.1679(1.1790) | Loss 2.831099(2.817055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8497 | Time 1.1715(1.1785) | Loss 2.832149(2.818112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8498 | Time 1.1605(1.1772) | Loss 2.796105(2.816571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8499 | Time 1.1578(1.1759) | Loss 2.826859(2.817291) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8500 | Time 1.1559(1.1745) | Loss 2.794848(2.815720) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8500 | Test Loss 2.800417 | NFE 20
Skipping vis as data dimension is >2
Iter 8501 | Time 1.1746(1.1745) | Loss 2.750222(2.811135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8502 | Time 1.1900(1.1756) | Loss 2.862674(2.814743) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8503 | Time 1.1857(1.1763) | Loss 2.801156(2.813792) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8504 | Time 1.1724(1.1760) | Loss 2.806485(2.813281) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8505 | Time 1.1860(1.1767) | Loss 2.827329(2.814264) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8506 | Time 1.1936(1.1779) | Loss 2.807311(2.813777) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8507 | Time 1.1946(1.1791) | Loss 2.824688(2.814541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8508 | Time 1.2229(1.1821) | Loss 2.796415(2.813272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8509 | Time 1.2255(1.1852) | Loss 2.806065(2.812768) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8510 | Time 1.2062(1.1866) | Loss 2.787565(2.811004) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8511 | Time 1.2003(1.1876) | Loss 2.805051(2.810587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8512 | Time 1.2033(1.1887) | Loss 2.766438(2.807496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8513 | Time 1.1860(1.1885) | Loss 2.795776(2.806676) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8514 | Time 1.1691(1.1872) | Loss 2.827101(2.808106) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8515 | Time 1.1733(1.1862) | Loss 2.782268(2.806297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8516 | Time 1.1688(1.1850) | Loss 2.814953(2.806903) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8517 | Time 1.1617(1.1833) | Loss 2.700479(2.799453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8518 | Time 1.1807(1.1831) | Loss 2.785942(2.798508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8519 | Time 1.1989(1.1843) | Loss 2.771831(2.796640) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8520 | Time 1.1577(1.1824) | Loss 2.765769(2.794479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8521 | Time 1.1542(1.1804) | Loss 2.771095(2.792842) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8522 | Time 1.1590(1.1789) | Loss 2.745671(2.789540) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8523 | Time 1.1639(1.1779) | Loss 2.761776(2.787597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8524 | Time 1.1832(1.1782) | Loss 2.757547(2.785493) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8525 | Time 1.1671(1.1775) | Loss 2.804988(2.786858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8526 | Time 1.1601(1.1762) | Loss 2.827172(2.789680) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8527 | Time 1.1961(1.1776) | Loss 2.824712(2.792132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8528 | Time 1.1806(1.1778) | Loss 2.767724(2.790424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8529 | Time 1.1884(1.1786) | Loss 2.795182(2.790757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8530 | Time 1.2088(1.1807) | Loss 2.857513(2.795430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8531 | Time 1.1691(1.1799) | Loss 2.803288(2.795980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8532 | Time 1.1716(1.1793) | Loss 2.818521(2.797558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8533 | Time 1.1764(1.1791) | Loss 2.853224(2.801454) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8534 | Time 1.1805(1.1792) | Loss 2.861257(2.805640) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8535 | Time 1.1819(1.1794) | Loss 2.821680(2.806763) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8536 | Time 1.1857(1.1798) | Loss 2.897549(2.813118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8537 | Time 1.2208(1.1827) | Loss 2.820854(2.813660) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8538 | Time 1.1868(1.1830) | Loss 2.850662(2.816250) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8539 | Time 1.1921(1.1836) | Loss 2.838828(2.817830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8540 | Time 1.2264(1.1866) | Loss 2.837326(2.819195) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8541 | Time 1.1715(1.1856) | Loss 2.857678(2.821889) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8542 | Time 1.1697(1.1845) | Loss 2.831081(2.822532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8543 | Time 1.2062(1.1860) | Loss 2.834185(2.823348) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8544 | Time 1.1882(1.1861) | Loss 2.845244(2.824881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8545 | Time 1.1896(1.1864) | Loss 2.847245(2.826446) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8546 | Time 1.1774(1.1857) | Loss 2.787122(2.823693) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8547 | Time 1.1960(1.1865) | Loss 2.779750(2.820617) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8548 | Time 1.1875(1.1865) | Loss 2.700557(2.812213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8549 | Time 1.1772(1.1859) | Loss 2.791311(2.810750) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8550 | Time 1.1836(1.1857) | Loss 2.853714(2.813757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8551 | Time 1.1774(1.1851) | Loss 2.772956(2.810901) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8552 | Time 1.1815(1.1849) | Loss 2.731109(2.805316) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8553 | Time 1.1818(1.1847) | Loss 2.751041(2.801517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8554 | Time 1.2007(1.1858) | Loss 2.757027(2.798402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8555 | Time 1.1726(1.1849) | Loss 2.698797(2.791430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8556 | Time 1.1734(1.1841) | Loss 2.777713(2.790470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8557 | Time 1.1887(1.1844) | Loss 2.756783(2.788112) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8558 | Time 1.2431(1.1885) | Loss 2.750714(2.785494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8559 | Time 1.1894(1.1886) | Loss 2.683977(2.778388) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8560 | Time 1.2054(1.1897) | Loss 2.750997(2.776470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8561 | Time 1.2310(1.1926) | Loss 2.752737(2.774809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8562 | Time 1.1962(1.1929) | Loss 2.763092(2.773989) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8563 | Time 1.2086(1.1940) | Loss 2.700855(2.768869) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8564 | Time 1.2236(1.1960) | Loss 2.715163(2.765110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8565 | Time 1.1950(1.1960) | Loss 2.751619(2.764166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8566 | Time 1.1854(1.1952) | Loss 2.744461(2.762786) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8567 | Time 1.2042(1.1959) | Loss 2.754268(2.762190) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8568 | Time 1.1983(1.1960) | Loss 2.675879(2.756148) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8569 | Time 1.2051(1.1967) | Loss 2.665529(2.749805) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8570 | Time 1.1813(1.1956) | Loss 2.756456(2.750270) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8571 | Time 1.1838(1.1948) | Loss 2.772165(2.751803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8572 | Time 1.2126(1.1960) | Loss 2.777174(2.753579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8573 | Time 1.2119(1.1971) | Loss 2.725468(2.751611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8574 | Time 1.2018(1.1975) | Loss 2.847903(2.758352) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8575 | Time 1.2000(1.1976) | Loss 2.723805(2.755933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8576 | Time 1.2110(1.1986) | Loss 2.720948(2.753484) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8577 | Time 1.2034(1.1989) | Loss 2.733605(2.752093) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8578 | Time 1.2146(1.2000) | Loss 2.792417(2.754916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8579 | Time 1.1952(1.1997) | Loss 2.749870(2.754562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8580 | Time 1.2185(1.2010) | Loss 2.725240(2.752510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8581 | Time 1.1924(1.2004) | Loss 2.751378(2.752431) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8582 | Time 1.2333(1.2027) | Loss 2.687104(2.747858) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8583 | Time 1.1933(1.2020) | Loss 2.739344(2.747262) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8584 | Time 1.1844(1.2008) | Loss 2.755519(2.747840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8585 | Time 1.1761(1.1991) | Loss 2.738189(2.747164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8586 | Time 1.1890(1.1984) | Loss 2.751667(2.747479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8587 | Time 1.1893(1.1977) | Loss 2.706627(2.744620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8588 | Time 1.2007(1.1979) | Loss 2.706501(2.741951) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8589 | Time 1.1919(1.1975) | Loss 2.735920(2.741529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8590 | Time 1.1999(1.1977) | Loss 2.724901(2.740365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8591 | Time 1.1791(1.1964) | Loss 2.753447(2.741281) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8592 | Time 1.2301(1.1987) | Loss 2.753120(2.742110) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8593 | Time 1.2123(1.1997) | Loss 2.779907(2.744756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8594 | Time 1.2531(1.2034) | Loss 2.836807(2.751199) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8595 | Time 1.2365(1.2057) | Loss 2.746145(2.750845) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8596 | Time 1.1974(1.2052) | Loss 2.765418(2.751865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8597 | Time 1.2004(1.2048) | Loss 2.736245(2.750772) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8598 | Time 1.2122(1.2053) | Loss 2.802748(2.754410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8599 | Time 1.1996(1.2049) | Loss 2.767886(2.755354) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8600 | Time 1.2039(1.2049) | Loss 2.841479(2.761382) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8600 | Test Loss 2.797668 | NFE 20
Skipping vis as data dimension is >2
Iter 8601 | Time 1.1985(1.2044) | Loss 2.842370(2.767052) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8602 | Time 1.1958(1.2038) | Loss 2.872658(2.774444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8603 | Time 1.1889(1.2028) | Loss 2.860785(2.780488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8604 | Time 1.1969(1.2024) | Loss 2.794215(2.781449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8605 | Time 1.1895(1.2015) | Loss 2.893275(2.789277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8606 | Time 1.2021(1.2015) | Loss 2.808948(2.790654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8607 | Time 1.1939(1.2010) | Loss 2.931128(2.800487) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8608 | Time 1.1819(1.1996) | Loss 2.928101(2.809420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8609 | Time 1.1897(1.1990) | Loss 2.996014(2.822481) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8610 | Time 1.1961(1.1987) | Loss 2.968084(2.832674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8611 | Time 1.2081(1.1994) | Loss 2.946779(2.840661) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8612 | Time 1.2034(1.1997) | Loss 2.958586(2.848916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8613 | Time 1.1850(1.1987) | Loss 2.967108(2.857189) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8614 | Time 1.1923(1.1982) | Loss 2.839033(2.855918) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8615 | Time 1.1846(1.1973) | Loss 2.874973(2.857252) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8616 | Time 1.1986(1.1974) | Loss 2.879219(2.858790) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8617 | Time 1.2009(1.1976) | Loss 2.864916(2.859219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8618 | Time 1.2104(1.1985) | Loss 2.848892(2.858496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8619 | Time 1.2180(1.1999) | Loss 2.846189(2.857634) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8620 | Time 1.1967(1.1996) | Loss 2.873633(2.858754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8621 | Time 1.2210(1.2011) | Loss 2.826077(2.856467) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8622 | Time 1.1896(1.2003) | Loss 2.781337(2.851208) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8623 | Time 1.1901(1.1996) | Loss 2.844253(2.850721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8624 | Time 1.2041(1.1999) | Loss 2.834649(2.849596) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8625 | Time 1.1877(1.1991) | Loss 2.838229(2.848800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8626 | Time 1.1952(1.1988) | Loss 2.807842(2.845933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8627 | Time 1.1911(1.1983) | Loss 2.825808(2.844524) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8628 | Time 1.2008(1.1984) | Loss 2.806445(2.841859) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8629 | Time 1.1988(1.1985) | Loss 2.855543(2.842817) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8630 | Time 1.2223(1.2001) | Loss 2.767095(2.837516) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8631 | Time 1.2054(1.2005) | Loss 2.827220(2.836795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8632 | Time 1.2055(1.2008) | Loss 2.866520(2.838876) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8633 | Time 1.2019(1.2009) | Loss 2.865112(2.840713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8634 | Time 1.2179(1.2021) | Loss 2.833066(2.840177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8635 | Time 1.1869(1.2010) | Loss 2.788176(2.836537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8636 | Time 1.1926(1.2005) | Loss 2.879428(2.839540) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8637 | Time 1.1961(1.2001) | Loss 2.919343(2.845126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8638 | Time 1.2268(1.2020) | Loss 2.885791(2.847972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8639 | Time 1.2518(1.2055) | Loss 2.828835(2.846633) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8640 | Time 1.2175(1.2063) | Loss 2.804006(2.843649) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8641 | Time 1.2161(1.2070) | Loss 2.821719(2.842114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8642 | Time 1.2113(1.2073) | Loss 2.833245(2.841493) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8643 | Time 1.2142(1.2078) | Loss 2.800735(2.838640) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8644 | Time 1.2222(1.2088) | Loss 2.852485(2.839609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8645 | Time 1.2183(1.2095) | Loss 2.835389(2.839314) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8646 | Time 1.2560(1.2127) | Loss 2.811494(2.837366) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8647 | Time 1.2356(1.2143) | Loss 2.812022(2.835592) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8648 | Time 1.2223(1.2149) | Loss 2.782604(2.831883) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8649 | Time 1.2180(1.2151) | Loss 2.788617(2.828855) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8650 | Time 1.2127(1.2149) | Loss 2.816092(2.827961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8651 | Time 1.2310(1.2161) | Loss 2.783564(2.824853) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8652 | Time 1.2258(1.2167) | Loss 2.785371(2.822090) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8653 | Time 1.2058(1.2160) | Loss 2.819920(2.821938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8654 | Time 1.2319(1.2171) | Loss 2.682563(2.812182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8655 | Time 1.2092(1.2165) | Loss 2.793916(2.810903) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8656 | Time 1.2034(1.2156) | Loss 2.811124(2.810918) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8657 | Time 1.2112(1.2153) | Loss 2.827841(2.812103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8658 | Time 1.1964(1.2140) | Loss 2.783988(2.810135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8659 | Time 1.2493(1.2164) | Loss 2.744841(2.805564) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8660 | Time 1.2686(1.2201) | Loss 2.738784(2.800890) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8661 | Time 1.2635(1.2231) | Loss 2.759286(2.797977) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8662 | Time 1.2534(1.2253) | Loss 2.756052(2.795043) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8663 | Time 1.2276(1.2254) | Loss 2.807575(2.795920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8664 | Time 1.2384(1.2263) | Loss 2.849889(2.799698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8665 | Time 1.2119(1.2253) | Loss 2.821365(2.801214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8666 | Time 1.2292(1.2256) | Loss 2.795885(2.800841) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8667 | Time 1.2258(1.2256) | Loss 2.845313(2.803954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8668 | Time 1.2226(1.2254) | Loss 2.819793(2.805063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8669 | Time 1.2042(1.2239) | Loss 2.773174(2.802831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8670 | Time 1.2053(1.2226) | Loss 2.788035(2.801795) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8671 | Time 1.2086(1.2216) | Loss 2.748724(2.798080) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8672 | Time 1.2056(1.2205) | Loss 2.813102(2.799132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8673 | Time 1.2555(1.2230) | Loss 2.825202(2.800957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8674 | Time 1.2807(1.2270) | Loss 2.809710(2.801569) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8675 | Time 1.2976(1.2319) | Loss 2.850378(2.804986) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8676 | Time 1.2583(1.2338) | Loss 2.845685(2.807835) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8677 | Time 1.2226(1.2330) | Loss 2.806594(2.807748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8678 | Time 1.2207(1.2321) | Loss 2.856610(2.811168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8679 | Time 1.2511(1.2335) | Loss 2.813420(2.811326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8680 | Time 1.2295(1.2332) | Loss 2.805473(2.810916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8681 | Time 1.2231(1.2325) | Loss 2.806239(2.810589) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8682 | Time 1.2280(1.2322) | Loss 2.790733(2.809199) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8683 | Time 1.2301(1.2320) | Loss 2.733412(2.803894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8684 | Time 1.2378(1.2324) | Loss 2.819158(2.804962) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8685 | Time 1.2239(1.2318) | Loss 2.772790(2.802710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8686 | Time 1.2088(1.2302) | Loss 2.833727(2.804882) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8687 | Time 1.2179(1.2294) | Loss 2.692264(2.796998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8688 | Time 1.2199(1.2287) | Loss 2.762650(2.794594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8689 | Time 1.2111(1.2275) | Loss 2.749051(2.791406) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8690 | Time 1.2257(1.2273) | Loss 2.755064(2.788862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8691 | Time 1.2058(1.2258) | Loss 2.783128(2.788461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8692 | Time 1.2023(1.2242) | Loss 2.790722(2.788619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8693 | Time 1.2279(1.2244) | Loss 2.841769(2.792339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8694 | Time 1.2057(1.2231) | Loss 2.781265(2.791564) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8695 | Time 1.2114(1.2223) | Loss 2.864280(2.796654) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8696 | Time 1.2088(1.2214) | Loss 2.858182(2.800961) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8697 | Time 1.2149(1.2209) | Loss 2.762842(2.798293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8698 | Time 1.2087(1.2201) | Loss 2.810960(2.799180) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8699 | Time 1.2031(1.2189) | Loss 2.768718(2.797047) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8700 | Time 1.2445(1.2207) | Loss 2.818569(2.798554) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8700 | Test Loss 2.805532 | NFE 20
Skipping vis as data dimension is >2
Iter 8701 | Time 1.2264(1.2211) | Loss 2.852036(2.802298) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8702 | Time 1.2214(1.2211) | Loss 2.823984(2.803816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8703 | Time 1.2247(1.2213) | Loss 2.791237(2.802935) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8704 | Time 1.2234(1.2215) | Loss 2.815261(2.803798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8705 | Time 1.2201(1.2214) | Loss 2.742618(2.799515) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8706 | Time 1.2297(1.2220) | Loss 2.795199(2.799213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8707 | Time 1.2241(1.2221) | Loss 2.759671(2.796445) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8708 | Time 1.2128(1.2215) | Loss 2.829861(2.798784) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8709 | Time 1.2375(1.2226) | Loss 2.819830(2.800258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8710 | Time 1.2253(1.2228) | Loss 2.815711(2.801339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8711 | Time 1.2005(1.2212) | Loss 2.796250(2.800983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8712 | Time 1.2110(1.2205) | Loss 2.818567(2.802214) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8713 | Time 1.2175(1.2203) | Loss 2.779064(2.800593) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8714 | Time 1.2274(1.2208) | Loss 2.786571(2.799612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8715 | Time 1.2451(1.2225) | Loss 2.874175(2.804831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8716 | Time 1.2019(1.2211) | Loss 2.775705(2.802792) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8717 | Time 1.2256(1.2214) | Loss 2.844881(2.805739) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8718 | Time 1.2278(1.2218) | Loss 2.881935(2.811072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8719 | Time 1.2258(1.2221) | Loss 2.875788(2.815602) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8720 | Time 1.2195(1.2219) | Loss 2.809325(2.815163) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8721 | Time 1.2174(1.2216) | Loss 2.742519(2.810078) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8722 | Time 1.2033(1.2203) | Loss 2.804090(2.809659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8723 | Time 1.2099(1.2196) | Loss 2.855481(2.812866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8724 | Time 1.1899(1.2175) | Loss 2.772485(2.810040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8725 | Time 1.2328(1.2186) | Loss 2.762887(2.806739) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8726 | Time 1.2408(1.2201) | Loss 2.795301(2.805938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8727 | Time 1.2360(1.2212) | Loss 2.766562(2.803182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8728 | Time 1.2214(1.2213) | Loss 2.739609(2.798732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8729 | Time 1.2149(1.2208) | Loss 2.746686(2.795089) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8730 | Time 1.2186(1.2207) | Loss 2.766882(2.793114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8731 | Time 1.2227(1.2208) | Loss 2.771031(2.791568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8732 | Time 1.2307(1.2215) | Loss 2.707613(2.785692) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8733 | Time 1.2251(1.2217) | Loss 2.753782(2.783458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8734 | Time 1.2152(1.2213) | Loss 2.715407(2.778694) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8735 | Time 1.2092(1.2204) | Loss 2.716366(2.774331) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8736 | Time 1.2003(1.2190) | Loss 2.746187(2.772361) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8737 | Time 1.2100(1.2184) | Loss 2.786942(2.773382) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8738 | Time 1.2313(1.2193) | Loss 2.741724(2.771166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8739 | Time 1.2679(1.2227) | Loss 2.741300(2.769075) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8740 | Time 1.2582(1.2252) | Loss 2.788179(2.770412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8741 | Time 1.2375(1.2260) | Loss 2.772487(2.770558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8742 | Time 1.2205(1.2257) | Loss 2.767596(2.770350) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8743 | Time 1.2141(1.2249) | Loss 2.790823(2.771783) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8744 | Time 1.1952(1.2228) | Loss 2.742078(2.769704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8745 | Time 1.2018(1.2213) | Loss 2.805010(2.772176) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8746 | Time 1.2078(1.2204) | Loss 2.809123(2.774762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8747 | Time 1.2062(1.2194) | Loss 2.843426(2.779568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8748 | Time 1.2128(1.2189) | Loss 2.798064(2.780863) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8749 | Time 1.2164(1.2187) | Loss 2.864299(2.786704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8750 | Time 1.2101(1.2181) | Loss 2.799838(2.787623) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8751 | Time 1.2206(1.2183) | Loss 2.727846(2.783439) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8752 | Time 1.2291(1.2191) | Loss 2.815140(2.785658) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8753 | Time 1.2465(1.2210) | Loss 2.769560(2.784531) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8754 | Time 1.2159(1.2206) | Loss 2.783405(2.784452) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8755 | Time 1.2397(1.2220) | Loss 2.874193(2.790734) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8756 | Time 1.2452(1.2236) | Loss 2.841033(2.794255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8757 | Time 1.2307(1.2241) | Loss 2.705070(2.788012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8758 | Time 1.2539(1.2262) | Loss 2.767066(2.786546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8759 | Time 1.2420(1.2273) | Loss 2.727811(2.782434) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8760 | Time 1.2276(1.2273) | Loss 2.731474(2.778867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8761 | Time 1.2539(1.2292) | Loss 2.771650(2.778362) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8762 | Time 1.2417(1.2300) | Loss 2.720329(2.774300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8763 | Time 1.2471(1.2312) | Loss 2.731879(2.771330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8764 | Time 1.2234(1.2307) | Loss 2.691352(2.765732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8765 | Time 1.2154(1.2296) | Loss 2.693207(2.760655) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8766 | Time 1.2434(1.2306) | Loss 2.716109(2.757537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8767 | Time 1.2191(1.2298) | Loss 2.785837(2.759518) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8768 | Time 1.2202(1.2291) | Loss 2.738072(2.758016) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8769 | Time 1.2353(1.2295) | Loss 2.679339(2.752509) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8770 | Time 1.2704(1.2324) | Loss 2.739325(2.751586) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8771 | Time 1.2448(1.2333) | Loss 2.775633(2.753269) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8772 | Time 1.2089(1.2316) | Loss 2.708154(2.750111) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8773 | Time 1.2087(1.2300) | Loss 2.716241(2.747740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8774 | Time 1.2062(1.2283) | Loss 2.700160(2.744410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8775 | Time 1.2029(1.2265) | Loss 2.675997(2.739621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8776 | Time 1.2186(1.2260) | Loss 2.680038(2.735450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8777 | Time 1.2441(1.2272) | Loss 2.718923(2.734293) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8778 | Time 1.1934(1.2249) | Loss 2.764790(2.736428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8779 | Time 1.2014(1.2232) | Loss 2.736076(2.736403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8780 | Time 1.2520(1.2252) | Loss 2.770168(2.738767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8781 | Time 1.2137(1.2244) | Loss 2.740898(2.738916) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8782 | Time 1.2118(1.2235) | Loss 2.803672(2.743449) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8783 | Time 1.1945(1.2215) | Loss 2.755895(2.744320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8784 | Time 1.2000(1.2200) | Loss 2.773576(2.746368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8785 | Time 1.2057(1.2190) | Loss 2.770705(2.748072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8786 | Time 1.2055(1.2181) | Loss 2.802023(2.751848) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8787 | Time 1.2152(1.2179) | Loss 2.706942(2.748705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8788 | Time 1.2006(1.2167) | Loss 2.776433(2.750646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8789 | Time 1.1822(1.2142) | Loss 2.756742(2.751073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8790 | Time 1.2014(1.2133) | Loss 2.752597(2.751179) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8791 | Time 1.1979(1.2123) | Loss 2.806708(2.755066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8792 | Time 1.2082(1.2120) | Loss 2.784382(2.757118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8793 | Time 1.1942(1.2107) | Loss 2.799278(2.760070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8794 | Time 1.1921(1.2094) | Loss 2.796316(2.762607) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8795 | Time 1.1869(1.2079) | Loss 2.754702(2.762054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8796 | Time 1.1959(1.2070) | Loss 2.772768(2.762804) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8797 | Time 1.1960(1.2062) | Loss 2.828623(2.767411) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8798 | Time 1.2058(1.2062) | Loss 2.814260(2.770690) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8799 | Time 1.2001(1.2058) | Loss 2.836299(2.775283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8800 | Time 1.1912(1.2048) | Loss 2.770422(2.774943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8800 | Test Loss 2.795270 | NFE 20
Skipping vis as data dimension is >2
Iter 8801 | Time 1.1987(1.2043) | Loss 2.774544(2.774915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8802 | Time 1.2157(1.2051) | Loss 2.821155(2.778152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8803 | Time 1.1916(1.2042) | Loss 2.768718(2.777491) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8804 | Time 1.2121(1.2047) | Loss 2.823629(2.780721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8805 | Time 1.2080(1.2050) | Loss 2.742068(2.778015) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8806 | Time 1.2288(1.2066) | Loss 2.795107(2.779212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8807 | Time 1.2281(1.2081) | Loss 2.708153(2.774238) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8808 | Time 1.2112(1.2084) | Loss 2.747673(2.772378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8809 | Time 1.2066(1.2082) | Loss 2.777957(2.772769) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8810 | Time 1.2109(1.2084) | Loss 2.743444(2.770716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8811 | Time 1.2189(1.2092) | Loss 2.784770(2.771700) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8812 | Time 1.2046(1.2088) | Loss 2.786522(2.772737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8813 | Time 1.2089(1.2088) | Loss 2.767365(2.772361) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8814 | Time 1.2150(1.2093) | Loss 2.736698(2.769865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8815 | Time 1.2057(1.2090) | Loss 2.748437(2.768365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8816 | Time 1.2135(1.2093) | Loss 2.766046(2.768202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8817 | Time 1.1918(1.2081) | Loss 2.726670(2.765295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8818 | Time 1.1895(1.2068) | Loss 2.762277(2.765084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8819 | Time 1.2079(1.2069) | Loss 2.746006(2.763748) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8820 | Time 1.1945(1.2060) | Loss 2.708836(2.759905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8821 | Time 1.2535(1.2093) | Loss 2.752389(2.759378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8822 | Time 1.2121(1.2095) | Loss 2.724434(2.756932) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8823 | Time 1.2072(1.2094) | Loss 2.839963(2.762745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8824 | Time 1.2043(1.2090) | Loss 2.711161(2.759134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8825 | Time 1.2186(1.2097) | Loss 2.721059(2.756468) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8826 | Time 1.2042(1.2093) | Loss 2.716715(2.753686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8827 | Time 1.2060(1.2091) | Loss 2.748179(2.753300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8828 | Time 1.2037(1.2087) | Loss 2.723378(2.751206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8829 | Time 1.2008(1.2081) | Loss 2.785145(2.753581) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8830 | Time 1.1977(1.2074) | Loss 2.755352(2.753705) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8831 | Time 1.2096(1.2076) | Loss 2.729795(2.752032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8832 | Time 1.2096(1.2077) | Loss 2.733251(2.750717) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8833 | Time 1.2065(1.2076) | Loss 2.705797(2.747573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8834 | Time 1.2531(1.2108) | Loss 2.745760(2.747446) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8835 | Time 1.2078(1.2106) | Loss 2.762186(2.748478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8836 | Time 1.2271(1.2117) | Loss 2.768280(2.749864) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8837 | Time 1.2185(1.2122) | Loss 2.788483(2.752567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8838 | Time 1.2110(1.2121) | Loss 2.727880(2.750839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8839 | Time 1.2082(1.2119) | Loss 2.774209(2.752475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8840 | Time 1.2015(1.2111) | Loss 2.881561(2.761511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8841 | Time 1.2039(1.2106) | Loss 2.787629(2.763339) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8842 | Time 1.2014(1.2100) | Loss 2.817964(2.767163) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8843 | Time 1.2155(1.2104) | Loss 2.789628(2.768736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8844 | Time 1.2230(1.2113) | Loss 2.787085(2.770020) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8845 | Time 1.2464(1.2137) | Loss 2.826632(2.773983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8846 | Time 1.2144(1.2138) | Loss 2.814980(2.776853) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8847 | Time 1.1983(1.2127) | Loss 2.778220(2.776948) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8848 | Time 1.2043(1.2121) | Loss 2.878423(2.784052) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8849 | Time 1.2029(1.2114) | Loss 2.846778(2.788442) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8850 | Time 1.2180(1.2119) | Loss 2.824264(2.790950) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8851 | Time 1.2041(1.2114) | Loss 2.813393(2.792521) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8852 | Time 1.2125(1.2114) | Loss 2.768255(2.790822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8853 | Time 1.2047(1.2110) | Loss 2.827240(2.793372) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8854 | Time 1.2079(1.2108) | Loss 2.808095(2.794402) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8855 | Time 1.1953(1.2097) | Loss 2.758726(2.791905) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8856 | Time 1.2020(1.2091) | Loss 2.738206(2.788146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8857 | Time 1.2149(1.2095) | Loss 2.752159(2.785627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8858 | Time 1.2078(1.2094) | Loss 2.768030(2.784395) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8859 | Time 1.2017(1.2089) | Loss 2.752073(2.782132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8860 | Time 1.1948(1.2079) | Loss 2.755579(2.780274) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8861 | Time 1.2517(1.2110) | Loss 2.766694(2.779323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8862 | Time 1.2210(1.2117) | Loss 2.812914(2.781674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8863 | Time 1.2393(1.2136) | Loss 2.822653(2.784543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8864 | Time 1.2064(1.2131) | Loss 2.721875(2.780156) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8865 | Time 1.2024(1.2123) | Loss 2.764042(2.779028) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8866 | Time 1.2130(1.2124) | Loss 2.814586(2.781517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8867 | Time 1.2057(1.2119) | Loss 2.836460(2.785363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8868 | Time 1.2162(1.2122) | Loss 2.770753(2.784341) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8869 | Time 1.2102(1.2121) | Loss 2.751305(2.782028) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8870 | Time 1.2130(1.2121) | Loss 2.714079(2.777272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8871 | Time 1.2058(1.2117) | Loss 2.696231(2.771599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8872 | Time 1.2038(1.2111) | Loss 2.708080(2.767152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8873 | Time 1.2090(1.2110) | Loss 2.731749(2.764674) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8874 | Time 1.2074(1.2107) | Loss 2.693545(2.759695) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8875 | Time 1.2318(1.2122) | Loss 2.679267(2.754065) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8876 | Time 1.2008(1.2114) | Loss 2.776265(2.755619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8877 | Time 1.2059(1.2110) | Loss 2.662108(2.749073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8878 | Time 1.2080(1.2108) | Loss 2.715738(2.746740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8879 | Time 1.1995(1.2100) | Loss 2.733616(2.745821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8880 | Time 1.1994(1.2093) | Loss 2.716785(2.743789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8881 | Time 1.2133(1.2096) | Loss 2.719238(2.742070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8882 | Time 1.2082(1.2095) | Loss 2.735796(2.741631) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8883 | Time 1.2088(1.2094) | Loss 2.729439(2.740778) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8884 | Time 1.1994(1.2087) | Loss 2.801020(2.744995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8885 | Time 1.2171(1.2093) | Loss 2.772960(2.746952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8886 | Time 1.2114(1.2095) | Loss 2.784320(2.749568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8887 | Time 1.2120(1.2096) | Loss 2.764120(2.750587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8888 | Time 1.2800(1.2146) | Loss 2.742605(2.750028) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8889 | Time 1.2290(1.2156) | Loss 2.767122(2.751224) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8890 | Time 1.2137(1.2154) | Loss 2.767962(2.752396) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8891 | Time 1.2176(1.2156) | Loss 2.781201(2.754412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8892 | Time 1.1979(1.2144) | Loss 2.773742(2.755765) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8893 | Time 1.2037(1.2136) | Loss 2.727722(2.753802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8894 | Time 1.2140(1.2136) | Loss 2.721892(2.751569) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8895 | Time 1.2258(1.2145) | Loss 2.799977(2.754957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8896 | Time 1.2392(1.2162) | Loss 2.679418(2.749670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8897 | Time 1.2746(1.2203) | Loss 2.771805(2.751219) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8898 | Time 1.2108(1.2196) | Loss 2.760837(2.751892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8899 | Time 1.2053(1.2186) | Loss 2.795946(2.754976) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8900 | Time 1.2062(1.2178) | Loss 2.721228(2.752614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 8900 | Test Loss 2.769641 | NFE 20
Skipping vis as data dimension is >2
Iter 8901 | Time 1.2194(1.2179) | Loss 2.767426(2.753651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8902 | Time 1.2039(1.2169) | Loss 2.806163(2.757326) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8903 | Time 1.2218(1.2172) | Loss 2.780054(2.758917) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8904 | Time 1.2172(1.2172) | Loss 2.747926(2.758148) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8905 | Time 1.2268(1.2179) | Loss 2.755713(2.757978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8906 | Time 1.2449(1.2198) | Loss 2.755019(2.757770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8907 | Time 1.2520(1.2220) | Loss 2.824732(2.762458) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8908 | Time 1.1947(1.2201) | Loss 2.799428(2.765046) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8909 | Time 1.2134(1.2197) | Loss 2.712651(2.761378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8910 | Time 1.2002(1.2183) | Loss 2.711977(2.757920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8911 | Time 1.1923(1.2165) | Loss 2.737967(2.756523) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8912 | Time 1.1964(1.2151) | Loss 2.722416(2.754136) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8913 | Time 1.1993(1.2140) | Loss 2.698841(2.750265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8914 | Time 1.2249(1.2147) | Loss 2.760797(2.751002) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8915 | Time 1.2271(1.2156) | Loss 2.684149(2.746323) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8916 | Time 1.2198(1.2159) | Loss 2.749954(2.746577) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8917 | Time 1.2134(1.2157) | Loss 2.777546(2.748745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8918 | Time 1.2205(1.2161) | Loss 2.708453(2.745924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8919 | Time 1.2119(1.2158) | Loss 2.786407(2.748758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8920 | Time 1.2044(1.2150) | Loss 2.740997(2.748215) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8921 | Time 1.2110(1.2147) | Loss 2.685425(2.743819) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8922 | Time 1.2054(1.2140) | Loss 2.766585(2.745413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8923 | Time 1.2252(1.2148) | Loss 2.648147(2.738604) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8924 | Time 1.2108(1.2145) | Loss 2.671883(2.733934) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8925 | Time 1.2793(1.2191) | Loss 2.745456(2.734740) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8926 | Time 1.2276(1.2197) | Loss 2.702646(2.732494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8927 | Time 1.2103(1.2190) | Loss 2.751556(2.733828) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8928 | Time 1.2258(1.2195) | Loss 2.754811(2.735297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8929 | Time 1.2090(1.2187) | Loss 2.716739(2.733998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8930 | Time 1.2238(1.2191) | Loss 2.696894(2.731401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8931 | Time 1.2651(1.2223) | Loss 2.808372(2.736789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8932 | Time 1.2109(1.2215) | Loss 2.764393(2.738721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8933 | Time 1.2108(1.2208) | Loss 2.776988(2.741400) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8934 | Time 1.1885(1.2185) | Loss 2.660393(2.735729) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8935 | Time 1.2130(1.2181) | Loss 2.813366(2.741164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8936 | Time 1.1861(1.2159) | Loss 2.799970(2.745280) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8937 | Time 1.1927(1.2143) | Loss 2.780670(2.747757) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8938 | Time 1.2115(1.2141) | Loss 2.773821(2.749582) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8939 | Time 1.2200(1.2145) | Loss 2.800295(2.753132) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8940 | Time 1.2054(1.2138) | Loss 2.804629(2.756737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8941 | Time 1.1954(1.2126) | Loss 2.729116(2.754803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8942 | Time 1.2357(1.2142) | Loss 2.839382(2.760724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8943 | Time 1.2186(1.2145) | Loss 2.731400(2.758671) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8944 | Time 1.2142(1.2145) | Loss 2.821964(2.763102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8945 | Time 1.2166(1.2146) | Loss 2.771008(2.763655) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8946 | Time 1.2213(1.2151) | Loss 2.778749(2.764712) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8947 | Time 1.2258(1.2158) | Loss 2.741501(2.763087) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8948 | Time 1.2190(1.2161) | Loss 2.765900(2.763284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8949 | Time 1.2064(1.2154) | Loss 2.734593(2.761275) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8950 | Time 1.1984(1.2142) | Loss 2.737714(2.759626) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8951 | Time 1.2065(1.2136) | Loss 2.794799(2.762088) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8952 | Time 1.2125(1.2136) | Loss 2.716038(2.758865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8953 | Time 1.1981(1.2125) | Loss 2.806070(2.762169) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8954 | Time 1.1956(1.2113) | Loss 2.749756(2.761300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8955 | Time 1.2362(1.2130) | Loss 2.685785(2.756014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8956 | Time 1.2010(1.2122) | Loss 2.709778(2.752778) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8957 | Time 1.2035(1.2116) | Loss 2.725843(2.750892) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8958 | Time 1.2054(1.2112) | Loss 2.747482(2.750653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8959 | Time 1.2257(1.2122) | Loss 2.794830(2.753746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8960 | Time 1.2104(1.2121) | Loss 2.697281(2.749793) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8961 | Time 1.1900(1.2105) | Loss 2.744670(2.749435) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8962 | Time 1.2021(1.2099) | Loss 2.730039(2.748077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8963 | Time 1.1959(1.2089) | Loss 2.717785(2.745957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8964 | Time 1.2080(1.2089) | Loss 2.656482(2.739693) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8965 | Time 1.2710(1.2132) | Loss 2.702193(2.737068) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8966 | Time 1.2301(1.2144) | Loss 2.743793(2.737539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8967 | Time 1.2040(1.2137) | Loss 2.722975(2.736519) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8968 | Time 1.1993(1.2127) | Loss 2.774769(2.739197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8969 | Time 1.2514(1.2154) | Loss 2.756070(2.740378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8970 | Time 1.2118(1.2151) | Loss 2.737069(2.740146) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8971 | Time 1.2147(1.2151) | Loss 2.756156(2.741267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8972 | Time 1.2190(1.2154) | Loss 2.684196(2.737272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8973 | Time 1.2129(1.2152) | Loss 2.774738(2.739895) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8974 | Time 1.2034(1.2144) | Loss 2.739657(2.739878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8975 | Time 1.1970(1.2132) | Loss 2.733554(2.739435) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8976 | Time 1.2123(1.2131) | Loss 2.712446(2.737546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8977 | Time 1.2471(1.2155) | Loss 2.692441(2.734389) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8978 | Time 1.2222(1.2159) | Loss 2.705120(2.732340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8979 | Time 1.2089(1.2155) | Loss 2.731409(2.732275) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8980 | Time 1.2048(1.2147) | Loss 2.773041(2.735128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8981 | Time 1.2050(1.2140) | Loss 2.711170(2.733451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8982 | Time 1.2063(1.2135) | Loss 2.813393(2.739047) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8983 | Time 1.2002(1.2126) | Loss 2.741858(2.739244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8984 | Time 1.2200(1.2131) | Loss 2.671836(2.734525) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8985 | Time 1.1916(1.2116) | Loss 2.702125(2.732257) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8986 | Time 1.2128(1.2117) | Loss 2.698690(2.729908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8987 | Time 1.2050(1.2112) | Loss 2.755919(2.731728) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8988 | Time 1.2171(1.2116) | Loss 2.729275(2.731557) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8989 | Time 1.1977(1.2106) | Loss 2.675237(2.727614) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8990 | Time 1.2051(1.2102) | Loss 2.687076(2.724777) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8991 | Time 1.1979(1.2094) | Loss 2.717449(2.724264) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8992 | Time 1.2055(1.2091) | Loss 2.642863(2.718566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8993 | Time 1.1920(1.2079) | Loss 2.709014(2.717897) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8994 | Time 1.2011(1.2074) | Loss 2.718835(2.717963) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8995 | Time 1.2083(1.2075) | Loss 2.691726(2.716126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8996 | Time 1.2379(1.2096) | Loss 2.674383(2.713204) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8997 | Time 1.2138(1.2099) | Loss 2.667131(2.709979) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8998 | Time 1.2148(1.2102) | Loss 2.670489(2.707215) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 8999 | Time 1.2041(1.2098) | Loss 2.690498(2.706045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9000 | Time 1.2095(1.2098) | Loss 2.741652(2.708537) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9000 | Test Loss 2.693537 | NFE 20
Skipping vis as data dimension is >2
Iter 9001 | Time 1.2178(1.2103) | Loss 2.701548(2.708048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9002 | Time 1.2442(1.2127) | Loss 2.764902(2.712028) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9003 | Time 1.2186(1.2131) | Loss 2.724112(2.712874) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9004 | Time 1.2597(1.2164) | Loss 2.717620(2.713206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9005 | Time 1.2325(1.2175) | Loss 2.773402(2.717420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9006 | Time 1.2207(1.2177) | Loss 2.754808(2.720037) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9007 | Time 1.2013(1.2166) | Loss 2.704591(2.718956) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9008 | Time 1.2154(1.2165) | Loss 2.703190(2.717852) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9009 | Time 1.2076(1.2159) | Loss 2.766182(2.721235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9010 | Time 1.2167(1.2159) | Loss 2.759771(2.723933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9011 | Time 1.2201(1.2162) | Loss 2.738493(2.724952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9012 | Time 1.2062(1.2155) | Loss 2.740096(2.726012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9013 | Time 1.2235(1.2161) | Loss 2.777434(2.729611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9014 | Time 1.2275(1.2169) | Loss 2.720054(2.728942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9015 | Time 1.2227(1.2173) | Loss 2.728808(2.728933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9016 | Time 1.2180(1.2173) | Loss 2.730580(2.729048) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9017 | Time 1.2228(1.2177) | Loss 2.784847(2.732954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9018 | Time 1.2047(1.2168) | Loss 2.776550(2.736006) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9019 | Time 1.2057(1.2160) | Loss 2.780519(2.739122) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9020 | Time 1.2226(1.2165) | Loss 2.770938(2.741349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9021 | Time 1.2498(1.2188) | Loss 2.747840(2.741803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9022 | Time 1.2380(1.2202) | Loss 2.758901(2.743000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9023 | Time 1.2253(1.2205) | Loss 2.682301(2.738751) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9024 | Time 1.2158(1.2202) | Loss 2.779418(2.741598) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9025 | Time 1.2156(1.2199) | Loss 2.734934(2.741131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9026 | Time 1.2318(1.2207) | Loss 2.735884(2.740764) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9027 | Time 1.2477(1.2226) | Loss 2.757646(2.741946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9028 | Time 1.2354(1.2235) | Loss 2.650159(2.735521) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9029 | Time 1.2087(1.2225) | Loss 2.755142(2.736894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9030 | Time 1.2239(1.2226) | Loss 2.716201(2.735446) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9031 | Time 1.2169(1.2222) | Loss 2.720170(2.734376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9032 | Time 1.2560(1.2245) | Loss 2.734665(2.734397) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9033 | Time 1.2201(1.2242) | Loss 2.682479(2.730762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9034 | Time 1.2117(1.2233) | Loss 2.754786(2.732444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9035 | Time 1.2167(1.2229) | Loss 2.831661(2.739389) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9036 | Time 1.2197(1.2227) | Loss 2.751803(2.740258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9037 | Time 1.2021(1.2212) | Loss 2.735016(2.739891) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9038 | Time 1.2130(1.2206) | Loss 2.772421(2.742168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9039 | Time 1.2149(1.2202) | Loss 2.684938(2.738162) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9040 | Time 1.2139(1.2198) | Loss 2.785878(2.741502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9041 | Time 1.2082(1.2190) | Loss 2.886419(2.751647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9042 | Time 1.1972(1.2175) | Loss 2.885786(2.761036) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9043 | Time 1.2069(1.2167) | Loss 2.794020(2.763345) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9044 | Time 1.2062(1.2160) | Loss 2.813445(2.766852) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9045 | Time 1.2119(1.2157) | Loss 2.835823(2.771680) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9046 | Time 1.2070(1.2151) | Loss 2.911645(2.781478) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9047 | Time 1.2070(1.2145) | Loss 2.855850(2.786684) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9048 | Time 1.2037(1.2138) | Loss 2.768873(2.785437) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9049 | Time 1.2219(1.2143) | Loss 2.862624(2.790840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9050 | Time 1.2400(1.2161) | Loss 2.837608(2.794114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9051 | Time 1.2048(1.2153) | Loss 2.873243(2.799653) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9052 | Time 1.2279(1.2162) | Loss 2.864837(2.804216) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9053 | Time 1.2307(1.2172) | Loss 2.808883(2.804543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9054 | Time 1.2141(1.2170) | Loss 2.769616(2.802098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9055 | Time 1.2255(1.2176) | Loss 2.775614(2.800244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9056 | Time 1.2206(1.2178) | Loss 2.830774(2.802381) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9057 | Time 1.2315(1.2188) | Loss 2.751585(2.798825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9058 | Time 1.2346(1.2199) | Loss 2.776036(2.797230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9059 | Time 1.2401(1.2213) | Loss 2.744942(2.793570) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9060 | Time 1.2206(1.2212) | Loss 2.723614(2.788673) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9061 | Time 1.2213(1.2212) | Loss 2.724207(2.784160) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9062 | Time 1.2223(1.2213) | Loss 2.755577(2.782159) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9063 | Time 1.2182(1.2211) | Loss 2.735888(2.778920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9064 | Time 1.2179(1.2209) | Loss 2.678394(2.771884) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9065 | Time 1.2070(1.2199) | Loss 2.714501(2.767867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9066 | Time 1.2114(1.2193) | Loss 2.715259(2.764184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9067 | Time 1.2011(1.2180) | Loss 2.703089(2.759908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9068 | Time 1.2428(1.2198) | Loss 2.729499(2.757779) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9069 | Time 1.2172(1.2196) | Loss 2.761537(2.758042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9070 | Time 1.2263(1.2201) | Loss 2.709615(2.754652) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9071 | Time 1.2186(1.2200) | Loss 2.701072(2.750902) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9072 | Time 1.2098(1.2192) | Loss 2.762514(2.751714) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9073 | Time 1.2100(1.2186) | Loss 2.712313(2.748956) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9074 | Time 1.2415(1.2202) | Loss 2.698864(2.745450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9075 | Time 1.2448(1.2219) | Loss 2.681488(2.740972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9076 | Time 1.2309(1.2226) | Loss 2.714718(2.739135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9077 | Time 1.2373(1.2236) | Loss 2.689621(2.735669) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9078 | Time 1.2414(1.2248) | Loss 2.655734(2.730073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9079 | Time 1.2264(1.2250) | Loss 2.686411(2.727017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9080 | Time 1.2101(1.2239) | Loss 2.765509(2.729711) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9081 | Time 1.2207(1.2237) | Loss 2.668979(2.725460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9082 | Time 1.2326(1.2243) | Loss 2.683367(2.722514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9083 | Time 1.2217(1.2241) | Loss 2.738173(2.723610) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9084 | Time 1.2260(1.2243) | Loss 2.682360(2.720722) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9085 | Time 1.2192(1.2239) | Loss 2.732449(2.721543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9086 | Time 1.2181(1.2235) | Loss 2.684178(2.718928) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9087 | Time 1.2051(1.2222) | Loss 2.728797(2.719618) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9088 | Time 1.2220(1.2222) | Loss 2.685605(2.717238) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9089 | Time 1.2024(1.2208) | Loss 2.736016(2.718552) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9090 | Time 1.2025(1.2195) | Loss 2.691128(2.716632) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9091 | Time 1.1852(1.2171) | Loss 2.693511(2.715014) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9092 | Time 1.1839(1.2148) | Loss 2.776170(2.719295) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9093 | Time 1.1876(1.2129) | Loss 2.762423(2.722314) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9094 | Time 1.2139(1.2130) | Loss 2.729246(2.722799) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9095 | Time 1.1986(1.2120) | Loss 2.747114(2.724501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9096 | Time 1.1992(1.2111) | Loss 2.642228(2.718742) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9097 | Time 1.2430(1.2133) | Loss 2.691337(2.716824) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9098 | Time 1.1998(1.2124) | Loss 2.689102(2.714883) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9099 | Time 1.2066(1.2120) | Loss 2.722723(2.715432) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9100 | Time 1.2060(1.2115) | Loss 2.732311(2.716613) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9100 | Test Loss 2.748827 | NFE 20
Skipping vis as data dimension is >2
Iter 9101 | Time 1.2205(1.2122) | Loss 2.769004(2.720281) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9102 | Time 1.2228(1.2129) | Loss 2.726018(2.720682) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9103 | Time 1.2532(1.2157) | Loss 2.732374(2.721501) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9104 | Time 1.2015(1.2147) | Loss 2.673173(2.718118) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9105 | Time 1.2075(1.2142) | Loss 2.743949(2.719926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9106 | Time 1.2078(1.2138) | Loss 2.726254(2.720369) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9107 | Time 1.2226(1.2144) | Loss 2.775202(2.724207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9108 | Time 1.1932(1.2129) | Loss 2.774268(2.727712) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9109 | Time 1.2475(1.2153) | Loss 2.724418(2.727481) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9110 | Time 1.2454(1.2174) | Loss 2.754510(2.729373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9111 | Time 1.2237(1.2179) | Loss 2.759408(2.731475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9112 | Time 1.2217(1.2181) | Loss 2.781175(2.734954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9113 | Time 1.2175(1.2181) | Loss 2.896096(2.746234) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9114 | Time 1.2119(1.2177) | Loss 2.779548(2.748566) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9115 | Time 1.2157(1.2175) | Loss 2.768032(2.749929) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9116 | Time 1.2029(1.2165) | Loss 2.813188(2.754357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9117 | Time 1.2172(1.2166) | Loss 2.843673(2.760609) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9118 | Time 1.2125(1.2163) | Loss 2.811064(2.764141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9119 | Time 1.2486(1.2185) | Loss 2.793985(2.766230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9120 | Time 1.2178(1.2185) | Loss 2.775934(2.766909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9121 | Time 1.2277(1.2191) | Loss 2.763168(2.766647) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9122 | Time 1.2102(1.2185) | Loss 2.769443(2.766843) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9123 | Time 1.2146(1.2182) | Loss 2.742238(2.765121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9124 | Time 1.2223(1.2185) | Loss 2.734021(2.762944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9125 | Time 1.2030(1.2174) | Loss 2.697237(2.758344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9126 | Time 1.2159(1.2173) | Loss 2.709428(2.754920) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9127 | Time 1.2057(1.2165) | Loss 2.688864(2.750296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9128 | Time 1.2082(1.2159) | Loss 2.755296(2.750646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9129 | Time 1.2061(1.2152) | Loss 2.687668(2.746238) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9130 | Time 1.2520(1.2178) | Loss 2.603687(2.736259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9131 | Time 1.2137(1.2175) | Loss 2.740984(2.736590) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9132 | Time 1.2140(1.2173) | Loss 2.720725(2.735479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9133 | Time 1.2174(1.2173) | Loss 2.702075(2.733141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9134 | Time 1.2153(1.2172) | Loss 2.647283(2.727131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9135 | Time 1.2383(1.2186) | Loss 2.739518(2.727998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9136 | Time 1.2341(1.2197) | Loss 2.701449(2.726140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9137 | Time 1.2224(1.2199) | Loss 2.665519(2.721896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9138 | Time 1.2092(1.2192) | Loss 2.655304(2.717235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9139 | Time 1.2320(1.2201) | Loss 2.649908(2.712522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9140 | Time 1.2669(1.2233) | Loss 2.685628(2.710639) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9141 | Time 1.2259(1.2235) | Loss 2.645898(2.706107) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9142 | Time 1.2105(1.2226) | Loss 2.625332(2.700453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9143 | Time 1.2248(1.2228) | Loss 2.663293(2.697852) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9144 | Time 1.2157(1.2223) | Loss 2.766058(2.702626) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9145 | Time 1.2186(1.2220) | Loss 2.768581(2.707243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9146 | Time 1.2136(1.2214) | Loss 2.779092(2.712273) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9147 | Time 1.2110(1.2207) | Loss 2.689595(2.710685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9148 | Time 1.2100(1.2199) | Loss 2.758523(2.714034) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9149 | Time 1.2182(1.2198) | Loss 2.701444(2.713153) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9150 | Time 1.2210(1.2199) | Loss 2.776053(2.717556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9151 | Time 1.2222(1.2201) | Loss 2.758297(2.720407) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9152 | Time 1.2140(1.2196) | Loss 2.715586(2.720070) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9153 | Time 1.2098(1.2189) | Loss 2.658076(2.715730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9154 | Time 1.2079(1.2182) | Loss 2.720281(2.716049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9155 | Time 1.2195(1.2183) | Loss 2.690473(2.714259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9156 | Time 1.2144(1.2180) | Loss 2.722179(2.714813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9157 | Time 1.2448(1.2199) | Loss 2.701635(2.713891) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9158 | Time 1.2266(1.2203) | Loss 2.679096(2.711455) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9159 | Time 1.2327(1.2212) | Loss 2.616834(2.704831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9160 | Time 1.2316(1.2219) | Loss 2.667129(2.702192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9161 | Time 1.2313(1.2226) | Loss 2.681128(2.700718) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9162 | Time 1.2288(1.2230) | Loss 2.664845(2.698207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9163 | Time 1.2237(1.2231) | Loss 2.701412(2.698431) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9164 | Time 1.2222(1.2230) | Loss 2.677833(2.696989) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9165 | Time 1.2130(1.2223) | Loss 2.693204(2.696724) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9166 | Time 1.2744(1.2260) | Loss 2.720569(2.698393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9167 | Time 1.2240(1.2258) | Loss 2.686754(2.697579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9168 | Time 1.2552(1.2279) | Loss 2.684592(2.696670) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9169 | Time 1.2379(1.2286) | Loss 2.734083(2.699289) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9170 | Time 1.2356(1.2291) | Loss 2.729154(2.701379) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9171 | Time 1.2173(1.2282) | Loss 2.716576(2.702443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9172 | Time 1.2601(1.2305) | Loss 2.717047(2.703465) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9173 | Time 1.2867(1.2344) | Loss 2.722845(2.704822) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9174 | Time 1.2684(1.2368) | Loss 2.689492(2.703749) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9175 | Time 1.2334(1.2365) | Loss 2.773218(2.708612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9176 | Time 1.2176(1.2352) | Loss 2.685552(2.706997) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9177 | Time 1.2118(1.2336) | Loss 2.814864(2.714548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9178 | Time 1.2035(1.2315) | Loss 2.836914(2.723114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9179 | Time 1.2088(1.2299) | Loss 2.822836(2.730094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9180 | Time 1.2198(1.2292) | Loss 2.775902(2.733301) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9181 | Time 1.2366(1.2297) | Loss 2.830699(2.740119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9182 | Time 1.2111(1.2284) | Loss 2.761133(2.741590) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9183 | Time 1.2060(1.2268) | Loss 2.745486(2.741862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9184 | Time 1.2649(1.2295) | Loss 2.767837(2.743681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9185 | Time 1.2381(1.2301) | Loss 2.704116(2.740911) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9186 | Time 1.2215(1.2295) | Loss 2.820328(2.746470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9187 | Time 1.2226(1.2290) | Loss 2.709589(2.743889) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9188 | Time 1.2249(1.2287) | Loss 2.676944(2.739203) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9189 | Time 1.2226(1.2283) | Loss 2.763259(2.740887) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9190 | Time 1.2025(1.2265) | Loss 2.785951(2.744041) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9191 | Time 1.1965(1.2244) | Loss 2.686600(2.740020) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9192 | Time 1.2322(1.2249) | Loss 2.746071(2.740444) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9193 | Time 1.2240(1.2249) | Loss 2.675019(2.735864) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9194 | Time 1.1896(1.2224) | Loss 2.745320(2.736526) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9195 | Time 1.2500(1.2243) | Loss 2.701788(2.734094) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9196 | Time 1.2264(1.2245) | Loss 2.779854(2.737297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9197 | Time 1.2199(1.2242) | Loss 2.802157(2.741838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9198 | Time 1.2114(1.2233) | Loss 2.712551(2.739788) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9199 | Time 1.2846(1.2276) | Loss 2.669759(2.734885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9200 | Time 1.2351(1.2281) | Loss 2.683307(2.731275) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9200 | Test Loss 2.721968 | NFE 20
Skipping vis as data dimension is >2
Iter 9201 | Time 1.2079(1.2267) | Loss 2.803477(2.736329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9202 | Time 1.2193(1.2262) | Loss 2.700590(2.733827) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9203 | Time 1.2315(1.2265) | Loss 2.713480(2.732403) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9204 | Time 1.2260(1.2265) | Loss 2.769557(2.735004) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9205 | Time 1.2180(1.2259) | Loss 2.722664(2.734140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9206 | Time 1.2122(1.2249) | Loss 2.671534(2.729758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9207 | Time 1.2127(1.2241) | Loss 2.728726(2.729685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9208 | Time 1.2268(1.2243) | Loss 2.688901(2.726831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9209 | Time 1.2206(1.2240) | Loss 2.639476(2.720716) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9210 | Time 1.2483(1.2257) | Loss 2.711260(2.720054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9211 | Time 1.2381(1.2266) | Loss 2.699244(2.718597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9212 | Time 1.2317(1.2269) | Loss 2.684676(2.716223) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9213 | Time 1.2738(1.2302) | Loss 2.687838(2.714236) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9214 | Time 1.2322(1.2304) | Loss 2.686797(2.712315) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9215 | Time 1.2416(1.2312) | Loss 2.700207(2.711467) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9216 | Time 1.2361(1.2315) | Loss 2.671417(2.708664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9217 | Time 1.2252(1.2311) | Loss 2.707230(2.708564) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9218 | Time 1.2217(1.2304) | Loss 2.743889(2.711036) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9219 | Time 1.2136(1.2292) | Loss 2.729049(2.712297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9220 | Time 1.2201(1.2286) | Loss 2.711808(2.712263) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9221 | Time 1.2304(1.2287) | Loss 2.775404(2.716683) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9222 | Time 1.2133(1.2276) | Loss 2.763299(2.719946) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9223 | Time 1.2029(1.2259) | Loss 2.783466(2.724392) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9224 | Time 1.1986(1.2240) | Loss 2.695136(2.722344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9225 | Time 1.2058(1.2227) | Loss 2.751205(2.724365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9226 | Time 1.1959(1.2208) | Loss 2.655117(2.719517) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9227 | Time 1.2040(1.2197) | Loss 2.708637(2.718756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9228 | Time 1.2036(1.2185) | Loss 2.789574(2.723713) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9229 | Time 1.2131(1.2182) | Loss 2.815778(2.730158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9230 | Time 1.2121(1.2177) | Loss 2.759713(2.732226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9231 | Time 1.2573(1.2205) | Loss 2.826997(2.738860) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9232 | Time 1.2176(1.2203) | Loss 2.803146(2.743360) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9233 | Time 1.2054(1.2193) | Loss 2.710586(2.741066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9234 | Time 1.2209(1.2194) | Loss 2.731018(2.740363) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9235 | Time 1.1954(1.2177) | Loss 2.745420(2.740717) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9236 | Time 1.2100(1.2172) | Loss 2.744135(2.740956) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9237 | Time 1.2547(1.2198) | Loss 2.676228(2.736425) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9238 | Time 1.2194(1.2198) | Loss 2.791137(2.740255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9239 | Time 1.2125(1.2193) | Loss 2.742201(2.740391) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9240 | Time 1.2174(1.2191) | Loss 2.699236(2.737510) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9241 | Time 1.2131(1.2187) | Loss 2.720091(2.736291) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9242 | Time 1.2100(1.2181) | Loss 2.721358(2.735246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9243 | Time 1.2101(1.2175) | Loss 2.702525(2.732955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9244 | Time 1.2065(1.2168) | Loss 2.733197(2.732972) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9245 | Time 1.1943(1.2152) | Loss 2.720347(2.732088) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9246 | Time 1.2129(1.2150) | Loss 2.685345(2.728816) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9247 | Time 1.2289(1.2160) | Loss 2.702211(2.726954) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9248 | Time 1.1894(1.2141) | Loss 2.766157(2.729698) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9249 | Time 1.2813(1.2188) | Loss 2.739217(2.730364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9250 | Time 1.2048(1.2179) | Loss 2.712918(2.729143) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9251 | Time 1.1988(1.2165) | Loss 2.770350(2.732028) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9252 | Time 1.1878(1.2145) | Loss 2.768529(2.734583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9253 | Time 1.1998(1.2135) | Loss 2.710358(2.732887) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9254 | Time 1.1997(1.2125) | Loss 2.711101(2.731362) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9255 | Time 1.2032(1.2119) | Loss 2.704046(2.729450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9256 | Time 1.2033(1.2113) | Loss 2.722996(2.728998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9257 | Time 1.1968(1.2103) | Loss 2.641193(2.722852) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9258 | Time 1.1995(1.2095) | Loss 2.617432(2.715472) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9259 | Time 1.2047(1.2092) | Loss 2.716287(2.715529) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9260 | Time 1.2013(1.2086) | Loss 2.698884(2.714364) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9261 | Time 1.2209(1.2095) | Loss 2.622396(2.707926) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9262 | Time 1.2235(1.2105) | Loss 2.681455(2.706073) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9263 | Time 1.2023(1.2099) | Loss 2.669378(2.703505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9264 | Time 1.2479(1.2126) | Loss 2.706876(2.703741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9265 | Time 1.2690(1.2165) | Loss 2.710417(2.704208) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9266 | Time 1.2410(1.2182) | Loss 2.688328(2.703096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9267 | Time 1.2411(1.2198) | Loss 2.740938(2.705745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9268 | Time 1.2442(1.2215) | Loss 2.792090(2.711789) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9269 | Time 1.2463(1.2233) | Loss 2.753804(2.714730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9270 | Time 1.2231(1.2233) | Loss 2.691568(2.713109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9271 | Time 1.2942(1.2282) | Loss 2.666808(2.709868) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9272 | Time 1.2199(1.2276) | Loss 2.675505(2.707463) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9273 | Time 1.2284(1.2277) | Loss 2.774185(2.712133) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9274 | Time 1.2225(1.2273) | Loss 2.793523(2.717830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9275 | Time 1.2127(1.2263) | Loss 2.685926(2.715597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9276 | Time 1.2004(1.2245) | Loss 2.736592(2.717067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9277 | Time 1.2240(1.2244) | Loss 2.713868(2.716843) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9278 | Time 1.2050(1.2231) | Loss 2.706427(2.716114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9279 | Time 1.2133(1.2224) | Loss 2.753893(2.718758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9280 | Time 1.2191(1.2222) | Loss 2.740013(2.720246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9281 | Time 1.2186(1.2219) | Loss 2.705328(2.719202) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9282 | Time 1.2393(1.2231) | Loss 2.700123(2.717866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9283 | Time 1.2135(1.2225) | Loss 2.684624(2.715539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9284 | Time 1.2374(1.2235) | Loss 2.698713(2.714361) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9285 | Time 1.2082(1.2224) | Loss 2.662273(2.710715) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9286 | Time 1.2095(1.2215) | Loss 2.657638(2.707000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9287 | Time 1.2302(1.2221) | Loss 2.654482(2.703324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9288 | Time 1.2050(1.2209) | Loss 2.715128(2.704150) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9289 | Time 1.2091(1.2201) | Loss 2.641540(2.699767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9290 | Time 1.2279(1.2206) | Loss 2.668718(2.697594) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9291 | Time 1.2436(1.2223) | Loss 2.711097(2.698539) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9292 | Time 1.2344(1.2231) | Loss 2.640194(2.694455) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9293 | Time 1.2294(1.2235) | Loss 2.659316(2.691995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9294 | Time 1.2150(1.2229) | Loss 2.583624(2.684409) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9295 | Time 1.2146(1.2224) | Loss 2.609506(2.679166) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9296 | Time 1.2237(1.2225) | Loss 2.687687(2.679762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9297 | Time 1.2344(1.2233) | Loss 2.645217(2.677344) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9298 | Time 1.2263(1.2235) | Loss 2.691305(2.678322) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9299 | Time 1.2277(1.2238) | Loss 2.657824(2.676887) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9300 | Time 1.2712(1.2271) | Loss 2.702665(2.678691) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9300 | Test Loss 2.656829 | NFE 20
Skipping vis as data dimension is >2
Iter 9301 | Time 1.2115(1.2260) | Loss 2.659752(2.677365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9302 | Time 1.2067(1.2247) | Loss 2.669509(2.676815) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9303 | Time 1.2137(1.2239) | Loss 2.741885(2.681370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9304 | Time 1.2004(1.2223) | Loss 2.658507(2.679770) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9305 | Time 1.2238(1.2224) | Loss 2.617236(2.675393) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9306 | Time 1.2136(1.2218) | Loss 2.668977(2.674943) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9307 | Time 1.2236(1.2219) | Loss 2.721472(2.678201) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9308 | Time 1.2184(1.2216) | Loss 2.744107(2.682814) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9309 | Time 1.2135(1.2211) | Loss 2.735671(2.686514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9310 | Time 1.2225(1.2212) | Loss 2.746864(2.690738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9311 | Time 1.2200(1.2211) | Loss 2.698980(2.691315) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9312 | Time 1.2407(1.2225) | Loss 2.702972(2.692131) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9313 | Time 1.2228(1.2225) | Loss 2.744354(2.695787) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9314 | Time 1.2296(1.2230) | Loss 2.721850(2.697611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9315 | Time 1.2162(1.2225) | Loss 2.750357(2.701303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9316 | Time 1.2177(1.2222) | Loss 2.731487(2.703416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9317 | Time 1.2328(1.2229) | Loss 2.691031(2.702549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9318 | Time 1.2232(1.2229) | Loss 2.655825(2.699279) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9319 | Time 1.2241(1.2230) | Loss 2.679410(2.697888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9320 | Time 1.2098(1.2221) | Loss 2.698592(2.697937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9321 | Time 1.2136(1.2215) | Loss 2.666745(2.695754) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9322 | Time 1.2354(1.2225) | Loss 2.724209(2.697746) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9323 | Time 1.2299(1.2230) | Loss 2.703226(2.698129) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9324 | Time 1.2241(1.2231) | Loss 2.745034(2.701413) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9325 | Time 1.2154(1.2225) | Loss 2.772525(2.706390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9326 | Time 1.2233(1.2226) | Loss 2.735186(2.708406) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9327 | Time 1.2343(1.2234) | Loss 2.777529(2.713245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9328 | Time 1.2194(1.2231) | Loss 2.735891(2.714830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9329 | Time 1.2353(1.2240) | Loss 2.745026(2.716944) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9330 | Time 1.2102(1.2230) | Loss 2.697506(2.715583) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9331 | Time 1.2329(1.2237) | Loss 2.748747(2.717904) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9332 | Time 1.2079(1.2226) | Loss 2.755138(2.720511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9333 | Time 1.2123(1.2219) | Loss 2.760524(2.723312) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9334 | Time 1.2270(1.2222) | Loss 2.750132(2.725189) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9335 | Time 1.2128(1.2216) | Loss 2.756783(2.727401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9336 | Time 1.2297(1.2221) | Loss 2.714154(2.726473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9337 | Time 1.2125(1.2215) | Loss 2.733988(2.726999) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9338 | Time 1.2244(1.2217) | Loss 2.677173(2.723512) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9339 | Time 1.2296(1.2222) | Loss 2.695643(2.721561) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9340 | Time 1.2132(1.2216) | Loss 2.675740(2.718353) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9341 | Time 1.2155(1.2212) | Loss 2.728396(2.719056) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9342 | Time 1.2242(1.2214) | Loss 2.804319(2.725025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9343 | Time 1.2181(1.2211) | Loss 2.754010(2.727054) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9344 | Time 1.2341(1.2221) | Loss 2.714068(2.726145) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9345 | Time 1.2298(1.2226) | Loss 2.665260(2.721883) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9346 | Time 1.2325(1.2233) | Loss 2.720233(2.721767) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9347 | Time 1.2375(1.2243) | Loss 2.723645(2.721899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9348 | Time 1.2511(1.2262) | Loss 2.752777(2.724060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9349 | Time 1.2359(1.2268) | Loss 2.762408(2.726745) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9350 | Time 1.2203(1.2264) | Loss 2.734843(2.727311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9351 | Time 1.2125(1.2254) | Loss 2.718433(2.726690) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9352 | Time 1.2293(1.2257) | Loss 2.704714(2.725152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9353 | Time 1.2226(1.2255) | Loss 2.730292(2.725511) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9354 | Time 1.2159(1.2248) | Loss 2.769992(2.728625) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9355 | Time 1.2282(1.2250) | Loss 2.742722(2.729612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9356 | Time 1.2358(1.2258) | Loss 2.685181(2.726502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9357 | Time 1.2183(1.2253) | Loss 2.746458(2.727899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9358 | Time 1.2116(1.2243) | Loss 2.707370(2.726462) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9359 | Time 1.2019(1.2227) | Loss 2.738779(2.727324) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9360 | Time 1.2112(1.2219) | Loss 2.692403(2.724879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9361 | Time 1.2175(1.2216) | Loss 2.699858(2.723128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9362 | Time 1.2148(1.2211) | Loss 2.770794(2.726464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9363 | Time 1.2357(1.2222) | Loss 2.757886(2.728664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9364 | Time 1.2251(1.2224) | Loss 2.741060(2.729532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9365 | Time 1.2251(1.2226) | Loss 2.741325(2.730357) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9366 | Time 1.2166(1.2221) | Loss 2.738246(2.730909) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9367 | Time 1.2049(1.2209) | Loss 2.697206(2.728550) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9368 | Time 1.2025(1.2196) | Loss 2.724195(2.728245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9369 | Time 1.2061(1.2187) | Loss 2.741261(2.729156) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9370 | Time 1.2010(1.2175) | Loss 2.845743(2.737318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9371 | Time 1.2190(1.2176) | Loss 2.813013(2.742616) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9372 | Time 1.2467(1.2196) | Loss 2.759961(2.743830) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9373 | Time 1.2288(1.2203) | Loss 2.762016(2.745103) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9374 | Time 1.2327(1.2211) | Loss 2.679703(2.740525) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9375 | Time 1.2387(1.2224) | Loss 2.768121(2.742457) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9376 | Time 1.2253(1.2226) | Loss 2.764538(2.744003) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9377 | Time 1.2176(1.2222) | Loss 2.726836(2.742801) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9378 | Time 1.2406(1.2235) | Loss 2.693075(2.739320) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9379 | Time 1.2158(1.2230) | Loss 2.681622(2.735281) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9380 | Time 1.2184(1.2226) | Loss 2.664084(2.730297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9381 | Time 1.2206(1.2225) | Loss 2.724343(2.729881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9382 | Time 1.2070(1.2214) | Loss 2.653718(2.724549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9383 | Time 1.2570(1.2239) | Loss 2.697048(2.722624) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9384 | Time 1.2247(1.2240) | Loss 2.660217(2.718256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9385 | Time 1.2145(1.2233) | Loss 2.636995(2.712567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9386 | Time 1.2082(1.2222) | Loss 2.643336(2.707721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9387 | Time 1.2156(1.2218) | Loss 2.598270(2.700060) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9388 | Time 1.2321(1.2225) | Loss 2.683175(2.698878) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9389 | Time 1.2202(1.2223) | Loss 2.733929(2.701331) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9390 | Time 1.2169(1.2220) | Loss 2.675560(2.699527) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9391 | Time 1.2056(1.2208) | Loss 2.761457(2.703862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9392 | Time 1.2139(1.2203) | Loss 2.695927(2.703307) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9393 | Time 1.2035(1.2191) | Loss 2.737227(2.705681) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9394 | Time 1.2209(1.2193) | Loss 2.650498(2.701818) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9395 | Time 1.2393(1.2207) | Loss 2.697925(2.701546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9396 | Time 1.2224(1.2208) | Loss 2.704177(2.701730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9397 | Time 1.2111(1.2201) | Loss 2.752861(2.705309) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9398 | Time 1.2373(1.2213) | Loss 2.694917(2.704582) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9399 | Time 1.2214(1.2213) | Loss 2.713864(2.705232) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9400 | Time 1.2459(1.2230) | Loss 2.721833(2.706394) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9400 | Test Loss 2.715395 | NFE 20
Skipping vis as data dimension is >2
Iter 9401 | Time 1.2698(1.2263) | Loss 2.736480(2.708500) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9402 | Time 1.2740(1.2297) | Loss 2.684335(2.706808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9403 | Time 1.2766(1.2329) | Loss 2.750062(2.709836) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9404 | Time 1.2374(1.2333) | Loss 2.683855(2.708017) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9405 | Time 1.2484(1.2343) | Loss 2.737904(2.710109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9406 | Time 1.2279(1.2339) | Loss 2.710103(2.710109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9407 | Time 1.2304(1.2336) | Loss 2.723008(2.711012) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9408 | Time 1.2175(1.2325) | Loss 2.640844(2.706100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9409 | Time 1.2380(1.2329) | Loss 2.673539(2.703821) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9410 | Time 1.2295(1.2326) | Loss 2.676636(2.701918) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9411 | Time 1.2438(1.2334) | Loss 2.651107(2.698361) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9412 | Time 1.2489(1.2345) | Loss 2.657428(2.695496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9413 | Time 1.2192(1.2334) | Loss 2.725037(2.697564) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9414 | Time 1.2028(1.2313) | Loss 2.667840(2.695483) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9415 | Time 1.2181(1.2304) | Loss 2.721137(2.697279) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9416 | Time 1.2079(1.2288) | Loss 2.728853(2.699489) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9417 | Time 1.2117(1.2276) | Loss 2.699942(2.699521) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9418 | Time 1.2031(1.2259) | Loss 2.679075(2.698090) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9419 | Time 1.2013(1.2242) | Loss 2.760558(2.702462) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9420 | Time 1.2009(1.2225) | Loss 2.727868(2.704241) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9421 | Time 1.1881(1.2201) | Loss 2.739423(2.706704) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9422 | Time 1.2211(1.2202) | Loss 2.833346(2.715568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9423 | Time 1.2067(1.2192) | Loss 2.733840(2.716847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9424 | Time 1.2257(1.2197) | Loss 2.725175(2.717430) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9425 | Time 1.2501(1.2218) | Loss 2.763628(2.720664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9426 | Time 1.2303(1.2224) | Loss 2.724458(2.720930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9427 | Time 1.2218(1.2224) | Loss 2.745039(2.722617) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9428 | Time 1.2161(1.2219) | Loss 2.717975(2.722292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9429 | Time 1.2223(1.2220) | Loss 2.800133(2.727741) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9430 | Time 1.2090(1.2211) | Loss 2.747090(2.729096) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9431 | Time 1.2211(1.2211) | Loss 2.678181(2.725532) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9432 | Time 1.2392(1.2223) | Loss 2.723359(2.725380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9433 | Time 1.2384(1.2235) | Loss 2.750706(2.727152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9434 | Time 1.2743(1.2270) | Loss 2.683519(2.724098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9435 | Time 1.2862(1.2312) | Loss 2.730703(2.724560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9436 | Time 1.2124(1.2298) | Loss 2.642275(2.718800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9437 | Time 1.2159(1.2289) | Loss 2.696078(2.717210) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9438 | Time 1.2158(1.2280) | Loss 2.765851(2.720615) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9439 | Time 1.2188(1.2273) | Loss 2.688090(2.718338) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9440 | Time 1.1997(1.2254) | Loss 2.674421(2.715264) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9441 | Time 1.1992(1.2235) | Loss 2.688417(2.713385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9442 | Time 1.2132(1.2228) | Loss 2.679147(2.710988) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9443 | Time 1.1950(1.2209) | Loss 2.694456(2.709831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9444 | Time 1.2054(1.2198) | Loss 2.688391(2.708330) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9445 | Time 1.2039(1.2187) | Loss 2.635777(2.703251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9446 | Time 1.1969(1.2172) | Loss 2.699939(2.703019) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9447 | Time 1.2121(1.2168) | Loss 2.702113(2.702956) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9448 | Time 1.2078(1.2162) | Loss 2.668995(2.700579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9449 | Time 1.1933(1.2146) | Loss 2.670930(2.698503) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9450 | Time 1.1861(1.2126) | Loss 2.651208(2.695192) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9451 | Time 1.2510(1.2153) | Loss 2.646421(2.691779) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9452 | Time 1.2553(1.2181) | Loss 2.670027(2.690256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9453 | Time 1.2331(1.2191) | Loss 2.695153(2.690599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9454 | Time 1.2179(1.2190) | Loss 2.648343(2.687641) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9455 | Time 1.2156(1.2188) | Loss 2.729528(2.690573) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9456 | Time 1.2232(1.2191) | Loss 2.687598(2.690365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9457 | Time 1.2260(1.2196) | Loss 2.746626(2.694303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9458 | Time 1.2191(1.2196) | Loss 2.685726(2.693703) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9459 | Time 1.2270(1.2201) | Loss 2.667465(2.691866) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9460 | Time 1.2218(1.2202) | Loss 2.635426(2.687915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9461 | Time 1.2186(1.2201) | Loss 2.667224(2.686467) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9462 | Time 1.2068(1.2191) | Loss 2.683298(2.686245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9463 | Time 1.1965(1.2176) | Loss 2.653561(2.683957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9464 | Time 1.1925(1.2158) | Loss 2.659103(2.682217) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9465 | Time 1.2025(1.2149) | Loss 2.672604(2.681544) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9466 | Time 1.1904(1.2132) | Loss 2.657061(2.679831) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9467 | Time 1.1983(1.2121) | Loss 2.674545(2.679461) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9468 | Time 1.2075(1.2118) | Loss 2.716691(2.682067) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9469 | Time 1.2352(1.2134) | Loss 2.711622(2.684135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9470 | Time 1.2029(1.2127) | Loss 2.661569(2.682556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9471 | Time 1.1988(1.2117) | Loss 2.663612(2.681230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9472 | Time 1.1892(1.2101) | Loss 2.676636(2.680908) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9473 | Time 1.1978(1.2093) | Loss 2.683614(2.681098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9474 | Time 1.2269(1.2105) | Loss 2.636014(2.677942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9475 | Time 1.2389(1.2125) | Loss 2.669502(2.677351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9476 | Time 1.2490(1.2151) | Loss 2.701941(2.679072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9477 | Time 1.2572(1.2180) | Loss 2.681190(2.679221) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9478 | Time 1.2361(1.2193) | Loss 2.670667(2.678622) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9479 | Time 1.2939(1.2245) | Loss 2.684469(2.679031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9480 | Time 1.2730(1.2279) | Loss 2.653516(2.677245) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9481 | Time 1.2884(1.2321) | Loss 2.736283(2.681378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9482 | Time 1.2323(1.2321) | Loss 2.718220(2.683957) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9483 | Time 1.2180(1.2311) | Loss 2.716417(2.686229) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9484 | Time 1.2171(1.2302) | Loss 2.702097(2.687340) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9485 | Time 1.2028(1.2282) | Loss 2.718270(2.689505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9486 | Time 1.2313(1.2285) | Loss 2.663230(2.687666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9487 | Time 1.2211(1.2279) | Loss 2.680156(2.687140) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9488 | Time 1.2115(1.2268) | Loss 2.675618(2.686333) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9489 | Time 1.1979(1.2248) | Loss 2.729854(2.689380) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9490 | Time 1.2113(1.2238) | Loss 2.690185(2.689436) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9491 | Time 1.2031(1.2224) | Loss 2.686721(2.689246) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9492 | Time 1.1890(1.2200) | Loss 2.712701(2.690888) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9493 | Time 1.2381(1.2213) | Loss 2.646579(2.687786) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9494 | Time 1.2138(1.2208) | Loss 2.696334(2.688385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9495 | Time 1.1949(1.2190) | Loss 2.670358(2.687123) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9496 | Time 1.1838(1.2165) | Loss 2.733515(2.690370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9497 | Time 1.1891(1.2146) | Loss 2.674804(2.689281) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9498 | Time 1.1984(1.2135) | Loss 2.664988(2.687580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9499 | Time 1.1967(1.2123) | Loss 2.642327(2.684412) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9500 | Time 1.2076(1.2120) | Loss 2.620779(2.679958) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9500 | Test Loss 2.598334 | NFE 20
Skipping vis as data dimension is >2
Iter 9501 | Time 1.2174(1.2123) | Loss 2.622559(2.675940) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9502 | Time 1.2012(1.2116) | Loss 2.666487(2.675278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9503 | Time 1.2068(1.2112) | Loss 2.659869(2.674200) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9504 | Time 1.1851(1.2094) | Loss 2.600243(2.669023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9505 | Time 1.2371(1.2113) | Loss 2.611102(2.664968) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9506 | Time 1.2178(1.2118) | Loss 2.581341(2.659114) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9507 | Time 1.2127(1.2119) | Loss 2.618572(2.656276) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9508 | Time 1.2116(1.2118) | Loss 2.621034(2.653809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9509 | Time 1.2303(1.2131) | Loss 2.590820(2.649400) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9510 | Time 1.2231(1.2138) | Loss 2.644243(2.649039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9511 | Time 1.2383(1.2155) | Loss 2.608120(2.646175) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9512 | Time 1.2165(1.2156) | Loss 2.631755(2.645165) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9513 | Time 1.2207(1.2160) | Loss 2.689709(2.648284) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9514 | Time 1.2057(1.2153) | Loss 2.606769(2.645378) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9515 | Time 1.2048(1.2145) | Loss 2.640093(2.645008) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9516 | Time 1.2112(1.2143) | Loss 2.633757(2.644220) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9517 | Time 1.2085(1.2139) | Loss 2.642895(2.644127) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9518 | Time 1.2328(1.2152) | Loss 2.618204(2.642313) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9519 | Time 1.2464(1.2174) | Loss 2.670296(2.644272) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9520 | Time 1.2234(1.2178) | Loss 2.651914(2.644807) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9521 | Time 1.2253(1.2183) | Loss 2.712547(2.649548) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9522 | Time 1.2139(1.2180) | Loss 2.654655(2.649906) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9523 | Time 1.1956(1.2165) | Loss 2.684640(2.652337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9524 | Time 1.2037(1.2156) | Loss 2.711874(2.656505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9525 | Time 1.2105(1.2152) | Loss 2.670293(2.657470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9526 | Time 1.2063(1.2146) | Loss 2.768984(2.665276) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9527 | Time 1.2096(1.2142) | Loss 2.687662(2.666843) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9528 | Time 1.1923(1.2127) | Loss 2.799643(2.676139) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9529 | Time 1.1863(1.2108) | Loss 2.693408(2.677348) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9530 | Time 1.1815(1.2088) | Loss 2.699181(2.678876) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9531 | Time 1.1831(1.2070) | Loss 2.660232(2.677571) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9532 | Time 1.2186(1.2078) | Loss 2.718379(2.680428) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9533 | Time 1.1979(1.2071) | Loss 2.707055(2.682292) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9534 | Time 1.2125(1.2075) | Loss 2.756314(2.687473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9535 | Time 1.2068(1.2074) | Loss 2.710894(2.689113) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9536 | Time 1.2645(1.2114) | Loss 2.675928(2.688190) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9537 | Time 1.2002(1.2107) | Loss 2.688519(2.688213) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9538 | Time 1.1983(1.2098) | Loss 2.662705(2.686427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9539 | Time 1.2594(1.2133) | Loss 2.668848(2.685197) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9540 | Time 1.2086(1.2129) | Loss 2.635683(2.681731) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9541 | Time 1.2064(1.2125) | Loss 2.619483(2.677373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9542 | Time 1.1941(1.2112) | Loss 2.690669(2.678304) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9543 | Time 1.2129(1.2113) | Loss 2.662632(2.677207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9544 | Time 1.2337(1.2129) | Loss 2.711072(2.679578) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9545 | Time 1.1984(1.2119) | Loss 2.740021(2.683809) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9546 | Time 1.1886(1.2102) | Loss 2.716534(2.686099) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9547 | Time 1.1909(1.2089) | Loss 2.678411(2.685561) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9548 | Time 1.1804(1.2069) | Loss 2.775178(2.691834) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9549 | Time 1.1813(1.2051) | Loss 2.738508(2.695102) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9550 | Time 1.2087(1.2053) | Loss 2.802944(2.702651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9551 | Time 1.1939(1.2045) | Loss 2.749499(2.705930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9552 | Time 1.1928(1.2037) | Loss 2.713202(2.706439) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9553 | Time 1.1900(1.2028) | Loss 2.687361(2.705104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9554 | Time 1.1855(1.2015) | Loss 2.833620(2.714100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9555 | Time 1.2006(1.2015) | Loss 2.719076(2.714448) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9556 | Time 1.2319(1.2036) | Loss 2.781146(2.719117) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9557 | Time 1.2008(1.2034) | Loss 2.788983(2.724007) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9558 | Time 1.1991(1.2031) | Loss 2.813389(2.730264) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9559 | Time 1.2021(1.2030) | Loss 2.723693(2.729804) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9560 | Time 1.2341(1.2052) | Loss 2.788535(2.733915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9561 | Time 1.1829(1.2037) | Loss 2.748412(2.734930) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9562 | Time 1.1954(1.2031) | Loss 2.740679(2.735333) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9563 | Time 1.2045(1.2032) | Loss 2.749314(2.736311) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9564 | Time 1.1909(1.2023) | Loss 2.720490(2.735204) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9565 | Time 1.1968(1.2019) | Loss 2.701500(2.732844) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9566 | Time 1.2059(1.2022) | Loss 2.760893(2.734808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9567 | Time 1.2087(1.2027) | Loss 2.767901(2.737124) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9568 | Time 1.2018(1.2026) | Loss 2.701181(2.734608) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9569 | Time 1.2063(1.2029) | Loss 2.746681(2.735453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9570 | Time 1.1983(1.2025) | Loss 2.810071(2.740677) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9571 | Time 1.1979(1.2022) | Loss 2.776465(2.743182) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9572 | Time 1.2235(1.2037) | Loss 2.775455(2.745441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9573 | Time 1.2261(1.2053) | Loss 2.740088(2.745066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9574 | Time 1.2288(1.2069) | Loss 2.799709(2.748891) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9575 | Time 1.2446(1.2096) | Loss 2.715882(2.746581) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9576 | Time 1.2152(1.2099) | Loss 2.708884(2.743942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9577 | Time 1.2191(1.2106) | Loss 2.787962(2.747023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9578 | Time 1.2289(1.2119) | Loss 2.695411(2.743410) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9579 | Time 1.2246(1.2128) | Loss 2.722617(2.741955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9580 | Time 1.2542(1.2157) | Loss 2.778348(2.744502) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9581 | Time 1.2266(1.2164) | Loss 2.733449(2.743729) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9582 | Time 1.2054(1.2157) | Loss 2.752932(2.744373) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9583 | Time 1.2049(1.2149) | Loss 2.689332(2.740520) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9584 | Time 1.2035(1.2141) | Loss 2.741109(2.740561) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9585 | Time 1.2020(1.2133) | Loss 2.645132(2.733881) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9586 | Time 1.2509(1.2159) | Loss 2.677520(2.729936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9587 | Time 1.3209(1.2232) | Loss 2.659747(2.725023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9588 | Time 1.2506(1.2252) | Loss 2.689301(2.722522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9589 | Time 1.2239(1.2251) | Loss 2.630788(2.716101) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9590 | Time 1.2366(1.2259) | Loss 2.689354(2.714229) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9591 | Time 1.2290(1.2261) | Loss 2.697140(2.713032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9592 | Time 1.2161(1.2254) | Loss 2.702332(2.712283) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9593 | Time 1.2435(1.2267) | Loss 2.690884(2.710785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9594 | Time 1.2904(1.2311) | Loss 2.734735(2.712462) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9595 | Time 1.2319(1.2312) | Loss 2.719861(2.712980) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9596 | Time 1.2623(1.2334) | Loss 2.728490(2.714066) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9597 | Time 1.2404(1.2338) | Loss 2.737195(2.715685) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9598 | Time 1.2182(1.2327) | Loss 2.761682(2.718904) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9599 | Time 1.2327(1.2327) | Loss 2.711498(2.718386) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9600 | Time 1.2203(1.2319) | Loss 2.688611(2.716302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9600 | Test Loss 2.766755 | NFE 20
Skipping vis as data dimension is >2
Iter 9601 | Time 1.2218(1.2312) | Loss 2.701540(2.715268) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9602 | Time 1.2360(1.2315) | Loss 2.674552(2.712418) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9603 | Time 1.2187(1.2306) | Loss 2.730663(2.713695) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9604 | Time 1.2209(1.2299) | Loss 2.760143(2.716947) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9605 | Time 1.2244(1.2295) | Loss 2.714833(2.716799) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9606 | Time 1.2187(1.2288) | Loss 2.679073(2.714158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9607 | Time 1.2329(1.2291) | Loss 2.785595(2.719158) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9608 | Time 1.2378(1.2297) | Loss 2.737052(2.720411) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9609 | Time 1.2366(1.2302) | Loss 2.745024(2.722134) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9610 | Time 1.2208(1.2295) | Loss 2.765063(2.725139) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9611 | Time 1.2336(1.2298) | Loss 2.698179(2.723252) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9612 | Time 1.2227(1.2293) | Loss 2.732425(2.723894) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9613 | Time 1.2431(1.2303) | Loss 2.732038(2.724464) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9614 | Time 1.2161(1.2293) | Loss 2.676877(2.721133) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9615 | Time 1.2242(1.2289) | Loss 2.668197(2.717427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9616 | Time 1.2361(1.2294) | Loss 2.716535(2.717365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9617 | Time 1.2245(1.2291) | Loss 2.640050(2.711953) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9618 | Time 1.2314(1.2292) | Loss 2.698128(2.710985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9619 | Time 1.2414(1.2301) | Loss 2.733693(2.712575) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9620 | Time 1.2370(1.2306) | Loss 2.651654(2.708310) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9621 | Time 1.3540(1.2392) | Loss 2.682280(2.706488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9622 | Time 1.2460(1.2397) | Loss 2.708019(2.706595) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9623 | Time 1.2233(1.2385) | Loss 2.644038(2.702216) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9624 | Time 1.2321(1.2381) | Loss 2.758191(2.706135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9625 | Time 1.2330(1.2377) | Loss 2.691414(2.705104) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9626 | Time 1.2368(1.2377) | Loss 2.718542(2.706045) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9627 | Time 1.2553(1.2389) | Loss 2.781155(2.711302) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9628 | Time 1.2238(1.2378) | Loss 2.733418(2.712851) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9629 | Time 1.2449(1.2383) | Loss 2.761636(2.716266) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9630 | Time 1.2198(1.2370) | Loss 2.703678(2.715384) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9631 | Time 1.2322(1.2367) | Loss 2.706769(2.714781) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9632 | Time 1.2045(1.2344) | Loss 2.797090(2.720543) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9633 | Time 1.2133(1.2330) | Loss 2.787650(2.725240) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9634 | Time 1.2123(1.2315) | Loss 2.763501(2.727919) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9635 | Time 1.1971(1.2291) | Loss 2.723777(2.727629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9636 | Time 1.2132(1.2280) | Loss 2.681357(2.724390) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9637 | Time 1.2121(1.2269) | Loss 2.693034(2.722195) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9638 | Time 1.1955(1.2247) | Loss 2.737188(2.723244) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9639 | Time 1.2266(1.2248) | Loss 2.712199(2.722471) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9640 | Time 1.2463(1.2263) | Loss 2.685156(2.719859) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9641 | Time 1.2407(1.2273) | Loss 2.702289(2.718629) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9642 | Time 1.2098(1.2261) | Loss 2.727222(2.719231) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9643 | Time 1.2298(1.2264) | Loss 2.697714(2.717725) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9644 | Time 1.2298(1.2266) | Loss 2.673296(2.714615) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9645 | Time 1.2116(1.2255) | Loss 2.728870(2.715612) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9646 | Time 1.2259(1.2256) | Loss 2.632449(2.709791) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9647 | Time 1.2819(1.2295) | Loss 2.610496(2.702840) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9648 | Time 1.2516(1.2311) | Loss 2.598933(2.695567) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9649 | Time 1.2176(1.2301) | Loss 2.670137(2.693787) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9650 | Time 1.2719(1.2330) | Loss 2.581604(2.685934) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9651 | Time 1.2373(1.2333) | Loss 2.592163(2.679370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9652 | Time 1.2516(1.2346) | Loss 2.610080(2.674520) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9653 | Time 1.2211(1.2337) | Loss 2.625087(2.671059) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9654 | Time 1.2176(1.2326) | Loss 2.634134(2.668475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9655 | Time 1.2083(1.2309) | Loss 2.588984(2.662910) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9656 | Time 1.2208(1.2302) | Loss 2.627059(2.660401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9657 | Time 1.2442(1.2311) | Loss 2.663652(2.660628) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9658 | Time 1.2257(1.2307) | Loss 2.633027(2.658696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9659 | Time 1.2198(1.2300) | Loss 2.724509(2.663303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9660 | Time 1.2121(1.2287) | Loss 2.696703(2.665641) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9661 | Time 1.2236(1.2284) | Loss 2.606560(2.661505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9662 | Time 1.2091(1.2270) | Loss 2.715118(2.665258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9663 | Time 1.2048(1.2255) | Loss 2.672652(2.665776) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9664 | Time 1.2109(1.2244) | Loss 2.649141(2.664611) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9665 | Time 1.2213(1.2242) | Loss 2.655883(2.664000) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9666 | Time 1.2604(1.2268) | Loss 2.666231(2.664157) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9667 | Time 1.2729(1.2300) | Loss 2.684175(2.665558) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9668 | Time 1.2441(1.2310) | Loss 2.666839(2.665648) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9669 | Time 1.2364(1.2313) | Loss 2.675674(2.666349) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9670 | Time 1.2476(1.2325) | Loss 2.711054(2.669479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9671 | Time 1.2346(1.2326) | Loss 2.662581(2.668996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9672 | Time 1.2378(1.2330) | Loss 2.734764(2.673600) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9673 | Time 1.2349(1.2331) | Loss 2.681844(2.674177) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9674 | Time 1.2308(1.2330) | Loss 2.740775(2.678839) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9675 | Time 1.2818(1.2364) | Loss 2.694542(2.679938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9676 | Time 1.2297(1.2359) | Loss 2.739529(2.684109) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9677 | Time 1.2104(1.2341) | Loss 2.786006(2.691242) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9678 | Time 1.2112(1.2325) | Loss 2.714534(2.692872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9679 | Time 1.2059(1.2307) | Loss 2.776640(2.698736) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9680 | Time 1.2111(1.2293) | Loss 2.768522(2.703621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9681 | Time 1.2188(1.2286) | Loss 2.770674(2.708315) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9682 | Time 1.2132(1.2275) | Loss 2.769832(2.712621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9683 | Time 1.2337(1.2279) | Loss 2.739158(2.714479) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9684 | Time 1.2134(1.2269) | Loss 2.804894(2.720808) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9685 | Time 1.2241(1.2267) | Loss 2.747582(2.722682) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9686 | Time 1.2463(1.2281) | Loss 2.716521(2.722251) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9687 | Time 1.2377(1.2288) | Loss 2.826350(2.729538) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9688 | Time 1.2338(1.2291) | Loss 2.755242(2.731337) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9689 | Time 1.2263(1.2289) | Loss 2.741373(2.732040) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9690 | Time 1.2389(1.2296) | Loss 2.756434(2.733747) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9691 | Time 1.2601(1.2317) | Loss 2.743784(2.734450) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9692 | Time 1.2785(1.2350) | Loss 2.741432(2.734938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9693 | Time 1.2342(1.2350) | Loss 2.758268(2.736572) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9694 | Time 1.2107(1.2333) | Loss 2.731413(2.736210) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9695 | Time 1.2320(1.2332) | Loss 2.826946(2.742562) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9696 | Time 1.2206(1.2323) | Loss 2.742852(2.742582) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9697 | Time 1.2356(1.2325) | Loss 2.722328(2.741164) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9698 | Time 1.2198(1.2316) | Loss 2.770121(2.743191) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9699 | Time 1.2353(1.2319) | Loss 2.696642(2.739933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9700 | Time 1.2247(1.2314) | Loss 2.757387(2.741155) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9700 | Test Loss 2.732447 | NFE 20
Skipping vis as data dimension is >2
Iter 9701 | Time 1.2282(1.2312) | Loss 2.706194(2.738708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9702 | Time 1.2352(1.2315) | Loss 2.745832(2.739206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9703 | Time 1.1998(1.2292) | Loss 2.700328(2.736485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9704 | Time 1.2190(1.2285) | Loss 2.658054(2.730995) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9705 | Time 1.2126(1.2274) | Loss 2.656341(2.725769) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9706 | Time 1.2024(1.2257) | Loss 2.659815(2.721152) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9707 | Time 1.2007(1.2239) | Loss 2.677947(2.718128) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9708 | Time 1.2517(1.2259) | Loss 2.654796(2.713695) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9709 | Time 1.2209(1.2255) | Loss 2.770211(2.717651) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9710 | Time 1.2218(1.2253) | Loss 2.679575(2.714985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9711 | Time 1.2358(1.2260) | Loss 2.748195(2.717310) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9712 | Time 1.2300(1.2263) | Loss 2.765844(2.720707) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9713 | Time 1.2186(1.2257) | Loss 2.673004(2.717368) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9714 | Time 1.2239(1.2256) | Loss 2.772427(2.721222) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9715 | Time 1.2078(1.2243) | Loss 2.753189(2.723460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9716 | Time 1.2218(1.2242) | Loss 2.781483(2.727522) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9717 | Time 1.2137(1.2234) | Loss 2.750564(2.729135) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9718 | Time 1.2176(1.2230) | Loss 2.744051(2.730179) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9719 | Time 1.2313(1.2236) | Loss 2.809877(2.735758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9720 | Time 1.2623(1.2263) | Loss 2.799829(2.740243) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9721 | Time 1.2210(1.2259) | Loss 2.743180(2.740448) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9722 | Time 1.2550(1.2280) | Loss 2.741096(2.740494) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9723 | Time 1.2227(1.2276) | Loss 2.749533(2.741126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9724 | Time 1.2345(1.2281) | Loss 2.669073(2.736083) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9725 | Time 1.2124(1.2270) | Loss 2.681110(2.732235) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9726 | Time 1.2228(1.2267) | Loss 2.622615(2.724561) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9727 | Time 1.2265(1.2267) | Loss 2.697823(2.722689) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9728 | Time 1.2278(1.2268) | Loss 2.687878(2.720253) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9729 | Time 1.2069(1.2254) | Loss 2.610362(2.712560) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9730 | Time 1.2066(1.2241) | Loss 2.642210(2.707636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9731 | Time 1.2133(1.2233) | Loss 2.667812(2.704848) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9732 | Time 1.2094(1.2223) | Loss 2.708043(2.705072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9733 | Time 1.2062(1.2212) | Loss 2.659388(2.701874) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9734 | Time 1.2159(1.2208) | Loss 2.592065(2.694187) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9735 | Time 1.2048(1.2197) | Loss 2.587363(2.686710) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9736 | Time 1.2023(1.2185) | Loss 2.627233(2.682546) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9737 | Time 1.2073(1.2177) | Loss 2.625328(2.678541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9738 | Time 1.2235(1.2181) | Loss 2.586531(2.672100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9739 | Time 1.2121(1.2177) | Loss 2.594444(2.666664) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9740 | Time 1.2073(1.2170) | Loss 2.649647(2.665473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9741 | Time 1.2266(1.2176) | Loss 2.684017(2.666771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9742 | Time 1.2211(1.2179) | Loss 2.570123(2.660006) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9743 | Time 1.2171(1.2178) | Loss 2.597777(2.655650) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9744 | Time 1.2330(1.2189) | Loss 2.664983(2.656303) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9745 | Time 1.2117(1.2184) | Loss 2.711131(2.660141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9746 | Time 1.2432(1.2201) | Loss 2.611251(2.656719) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9747 | Time 1.2065(1.2192) | Loss 2.659095(2.656885) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9748 | Time 1.2379(1.2205) | Loss 2.658425(2.656993) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9749 | Time 1.2270(1.2209) | Loss 2.678914(2.658527) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9750 | Time 1.2017(1.2196) | Loss 2.651349(2.658025) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9751 | Time 1.2331(1.2205) | Loss 2.696187(2.660696) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9752 | Time 1.2516(1.2227) | Loss 2.696988(2.663237) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9753 | Time 1.2445(1.2242) | Loss 2.725108(2.667568) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9754 | Time 1.2218(1.2241) | Loss 2.677186(2.668241) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9755 | Time 1.2018(1.2225) | Loss 2.661337(2.667758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9756 | Time 1.2164(1.2221) | Loss 2.649994(2.666514) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9757 | Time 1.2281(1.2225) | Loss 2.714823(2.669896) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9758 | Time 1.2131(1.2219) | Loss 2.671661(2.670019) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9759 | Time 1.2085(1.2209) | Loss 2.716261(2.673256) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9760 | Time 1.2189(1.2208) | Loss 2.692442(2.674599) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9761 | Time 1.2188(1.2206) | Loss 2.637892(2.672030) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9762 | Time 1.2359(1.2217) | Loss 2.633491(2.669332) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9763 | Time 1.2331(1.2225) | Loss 2.625352(2.666253) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9764 | Time 1.2141(1.2219) | Loss 2.603653(2.661871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9765 | Time 1.2238(1.2220) | Loss 2.708580(2.665141) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9766 | Time 1.2496(1.2240) | Loss 2.667832(2.665329) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9767 | Time 1.2272(1.2242) | Loss 2.694213(2.667351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9768 | Time 1.2223(1.2241) | Loss 2.658473(2.666730) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9769 | Time 1.2910(1.2287) | Loss 2.712541(2.669937) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9770 | Time 1.2632(1.2312) | Loss 2.657059(2.669035) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9771 | Time 1.2300(1.2311) | Loss 2.745683(2.674401) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9772 | Time 1.2485(1.2323) | Loss 2.649071(2.672627) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9773 | Time 1.2792(1.2356) | Loss 2.724450(2.676255) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9774 | Time 1.2566(1.2371) | Loss 2.692266(2.677376) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9775 | Time 1.2627(1.2388) | Loss 2.705867(2.679370) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9776 | Time 1.2377(1.2388) | Loss 2.660360(2.678039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9777 | Time 1.2541(1.2398) | Loss 2.765486(2.684161) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9778 | Time 1.2238(1.2387) | Loss 2.706257(2.685708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9779 | Time 1.2211(1.2375) | Loss 2.643504(2.682753) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9780 | Time 1.2468(1.2381) | Loss 2.727236(2.685867) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9781 | Time 1.2346(1.2379) | Loss 2.687451(2.685978) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9782 | Time 1.2302(1.2374) | Loss 2.714227(2.687955) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9783 | Time 1.2213(1.2362) | Loss 2.725804(2.690605) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9784 | Time 1.2140(1.2347) | Loss 2.717472(2.692485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9785 | Time 1.2131(1.2332) | Loss 2.658955(2.690138) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9786 | Time 1.2386(1.2335) | Loss 2.620543(2.685267) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9787 | Time 1.2261(1.2330) | Loss 2.713032(2.687210) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9788 | Time 1.2285(1.2327) | Loss 2.702284(2.688265) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9789 | Time 1.2113(1.2312) | Loss 2.676195(2.687420) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9790 | Time 1.2257(1.2308) | Loss 2.694748(2.687933) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9791 | Time 1.2003(1.2287) | Loss 2.719584(2.690149) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9792 | Time 1.2033(1.2269) | Loss 2.638826(2.686556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9793 | Time 1.2267(1.2269) | Loss 2.627000(2.682387) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9794 | Time 1.2089(1.2256) | Loss 2.601164(2.676702) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9795 | Time 1.2119(1.2247) | Loss 2.619045(2.672666) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9796 | Time 1.2031(1.2232) | Loss 2.670622(2.672523) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9797 | Time 1.2198(1.2229) | Loss 2.701929(2.674581) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9798 | Time 1.2186(1.2226) | Loss 2.630558(2.671499) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9799 | Time 1.2215(1.2225) | Loss 2.669869(2.671385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9800 | Time 1.2489(1.2244) | Loss 2.615213(2.667453) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9800 | Test Loss 2.630113 | NFE 20
Skipping vis as data dimension is >2
Iter 9801 | Time 1.2349(1.2251) | Loss 2.696698(2.669500) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9802 | Time 1.2295(1.2254) | Loss 2.698025(2.671497) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9803 | Time 1.2270(1.2255) | Loss 2.694764(2.673126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9804 | Time 1.2309(1.2259) | Loss 2.663373(2.672443) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9805 | Time 1.2326(1.2264) | Loss 2.593774(2.666936) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9806 | Time 1.2382(1.2272) | Loss 2.675586(2.667542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9807 | Time 1.2247(1.2270) | Loss 2.721286(2.671304) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9808 | Time 1.2225(1.2267) | Loss 2.609598(2.666985) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9809 | Time 1.2257(1.2266) | Loss 2.609438(2.662956) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9810 | Time 1.2163(1.2259) | Loss 2.636772(2.661123) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9811 | Time 1.2279(1.2261) | Loss 2.643861(2.659915) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9812 | Time 1.2209(1.2257) | Loss 2.589026(2.654953) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9813 | Time 1.2294(1.2260) | Loss 2.665723(2.655707) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9814 | Time 1.2410(1.2270) | Loss 2.715203(2.659871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9815 | Time 1.2206(1.2266) | Loss 2.648117(2.659049) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9816 | Time 1.2226(1.2263) | Loss 2.667210(2.659620) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9817 | Time 1.2138(1.2254) | Loss 2.689865(2.661737) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9818 | Time 1.2145(1.2246) | Loss 2.643359(2.660451) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9819 | Time 1.2171(1.2241) | Loss 2.661745(2.660541) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9820 | Time 1.2102(1.2231) | Loss 2.719023(2.664635) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9821 | Time 1.2180(1.2228) | Loss 2.701450(2.667212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9822 | Time 1.2231(1.2228) | Loss 2.662338(2.666871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9823 | Time 1.2574(1.2252) | Loss 2.627658(2.664126) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9824 | Time 1.2084(1.2241) | Loss 2.671206(2.664621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9825 | Time 1.2123(1.2232) | Loss 2.626486(2.661952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9826 | Time 1.2513(1.2252) | Loss 2.669356(2.662470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9827 | Time 1.2290(1.2255) | Loss 2.610329(2.658820) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9828 | Time 1.2320(1.2259) | Loss 2.689934(2.660998) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9829 | Time 1.2306(1.2263) | Loss 2.667785(2.661473) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9830 | Time 1.2276(1.2263) | Loss 2.621982(2.658709) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9831 | Time 1.2354(1.2270) | Loss 2.606164(2.655031) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9832 | Time 1.2335(1.2274) | Loss 2.653900(2.654952) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9833 | Time 1.2235(1.2272) | Loss 2.667962(2.655862) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9834 | Time 1.2262(1.2271) | Loss 2.632283(2.654212) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9835 | Time 1.2151(1.2262) | Loss 2.629694(2.652496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9836 | Time 1.2244(1.2261) | Loss 2.650871(2.652382) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9837 | Time 1.2278(1.2262) | Loss 2.692736(2.655207) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9838 | Time 1.2188(1.2257) | Loss 2.636741(2.653914) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9839 | Time 1.2228(1.2255) | Loss 2.624642(2.651865) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9840 | Time 1.2033(1.2240) | Loss 2.678117(2.653703) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9841 | Time 1.2036(1.2225) | Loss 2.664246(2.654441) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9842 | Time 1.2222(1.2225) | Loss 2.605039(2.650983) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9843 | Time 1.2198(1.2223) | Loss 2.666771(2.652088) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9844 | Time 1.2065(1.2212) | Loss 2.679950(2.654038) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9845 | Time 1.1969(1.2195) | Loss 2.678787(2.655771) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9846 | Time 1.2133(1.2191) | Loss 2.659173(2.656009) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9847 | Time 1.2208(1.2192) | Loss 2.692985(2.658597) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9848 | Time 1.2207(1.2193) | Loss 2.685660(2.660492) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9849 | Time 1.2099(1.2186) | Loss 2.787542(2.669385) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9850 | Time 1.2179(1.2186) | Loss 2.683488(2.670372) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9851 | Time 1.2283(1.2193) | Loss 2.684108(2.671334) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9852 | Time 1.2183(1.2192) | Loss 2.695965(2.673058) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9853 | Time 1.2432(1.2209) | Loss 2.729003(2.676974) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9854 | Time 1.2493(1.2229) | Loss 2.698337(2.678470) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9855 | Time 1.2206(1.2227) | Loss 2.689501(2.679242) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9856 | Time 1.2146(1.2221) | Loss 2.786187(2.686728) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9857 | Time 1.2266(1.2225) | Loss 2.749179(2.691100) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9858 | Time 1.2201(1.2223) | Loss 2.725994(2.693542) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9859 | Time 1.2148(1.2218) | Loss 2.720315(2.695416) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9860 | Time 1.2498(1.2237) | Loss 2.756696(2.699706) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9861 | Time 1.2239(1.2237) | Loss 2.705306(2.700098) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9862 | Time 1.2308(1.2242) | Loss 2.671832(2.698119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9863 | Time 1.2121(1.2234) | Loss 2.725823(2.700059) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9864 | Time 1.2036(1.2220) | Loss 2.614613(2.694077) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9865 | Time 1.2020(1.2206) | Loss 2.645245(2.690659) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9866 | Time 1.2047(1.2195) | Loss 2.692646(2.690798) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9867 | Time 1.2216(1.2196) | Loss 2.625478(2.686226) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9868 | Time 1.2329(1.2206) | Loss 2.585285(2.679160) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9869 | Time 1.2179(1.2204) | Loss 2.642339(2.676582) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9870 | Time 1.2571(1.2229) | Loss 2.660911(2.675485) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9871 | Time 1.2254(1.2231) | Loss 2.680148(2.675812) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9872 | Time 1.2184(1.2228) | Loss 2.616287(2.671645) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9873 | Time 1.2156(1.2223) | Loss 2.569202(2.664474) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9874 | Time 1.2448(1.2239) | Loss 2.647105(2.663258) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9875 | Time 1.2305(1.2243) | Loss 2.672771(2.663924) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9876 | Time 1.2479(1.2260) | Loss 2.690772(2.665803) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9877 | Time 1.2220(1.2257) | Loss 2.676549(2.666556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9878 | Time 1.2167(1.2251) | Loss 2.668412(2.666686) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9879 | Time 1.2001(1.2233) | Loss 2.624496(2.663732) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9880 | Time 1.2479(1.2250) | Loss 2.630711(2.661421) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9881 | Time 1.2261(1.2251) | Loss 2.702501(2.664296) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9882 | Time 1.2286(1.2254) | Loss 2.677287(2.665206) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9883 | Time 1.2255(1.2254) | Loss 2.660826(2.664899) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9884 | Time 1.2373(1.2262) | Loss 2.659316(2.664508) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9885 | Time 1.2368(1.2269) | Loss 2.657582(2.664023) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9886 | Time 1.2270(1.2270) | Loss 2.624785(2.661277) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9887 | Time 1.2768(1.2304) | Loss 2.698357(2.663872) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9888 | Time 1.2444(1.2314) | Loss 2.669938(2.664297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9889 | Time 1.2372(1.2318) | Loss 2.552740(2.656488) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9890 | Time 1.2332(1.2319) | Loss 2.657083(2.656530) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9891 | Time 1.2075(1.2302) | Loss 2.678129(2.658042) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9892 | Time 1.2286(1.2301) | Loss 2.668910(2.658802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9893 | Time 1.2771(1.2334) | Loss 2.599387(2.654643) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9894 | Time 1.2386(1.2338) | Loss 2.628946(2.652844) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9895 | Time 1.2304(1.2335) | Loss 2.615496(2.650230) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9896 | Time 1.2262(1.2330) | Loss 2.620099(2.648121) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9897 | Time 1.2315(1.2329) | Loss 2.632734(2.647044) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9898 | Time 1.2235(1.2322) | Loss 2.688777(2.649965) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9899 | Time 1.2251(1.2317) | Loss 2.661889(2.650800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9900 | Time 1.2288(1.2315) | Loss 2.621270(2.648733) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 9900 | Test Loss 2.640109 | NFE 20
Skipping vis as data dimension is >2
Iter 9901 | Time 1.2226(1.2309) | Loss 2.660892(2.649584) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9902 | Time 1.2297(1.2308) | Loss 2.699420(2.653072) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9903 | Time 1.2164(1.2298) | Loss 2.597519(2.649184) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9904 | Time 1.2121(1.2286) | Loss 2.695135(2.652400) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9905 | Time 1.2041(1.2269) | Loss 2.700766(2.655786) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9906 | Time 1.2426(1.2280) | Loss 2.749670(2.662358) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9907 | Time 1.2294(1.2281) | Loss 2.753756(2.668756) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9908 | Time 1.2258(1.2279) | Loss 2.699994(2.670942) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9909 | Time 1.2347(1.2284) | Loss 2.666321(2.670619) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9910 | Time 1.2258(1.2282) | Loss 2.754814(2.676513) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9911 | Time 1.2597(1.2304) | Loss 2.698673(2.678064) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9912 | Time 1.2508(1.2318) | Loss 2.704483(2.679913) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9913 | Time 1.2355(1.2321) | Loss 2.700336(2.681343) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9914 | Time 1.2275(1.2318) | Loss 2.769381(2.687505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9915 | Time 1.2245(1.2313) | Loss 2.699579(2.688351) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9916 | Time 1.2395(1.2318) | Loss 2.693640(2.688721) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9917 | Time 1.2160(1.2307) | Loss 2.711016(2.690282) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9918 | Time 1.2150(1.2296) | Loss 2.717226(2.692168) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9919 | Time 1.2199(1.2289) | Loss 2.698645(2.692621) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9920 | Time 1.2164(1.2281) | Loss 2.698934(2.693063) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9921 | Time 1.2261(1.2279) | Loss 2.708310(2.694130) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9922 | Time 1.2393(1.2287) | Loss 2.717442(2.695762) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9923 | Time 1.2236(1.2284) | Loss 2.706240(2.696496) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9924 | Time 1.2129(1.2273) | Loss 2.686743(2.695813) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9925 | Time 1.2055(1.2258) | Loss 2.756827(2.700084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9926 | Time 1.2294(1.2260) | Loss 2.693141(2.699598) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9927 | Time 1.2282(1.2262) | Loss 2.695347(2.699300) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9928 | Time 1.2388(1.2270) | Loss 2.661388(2.696646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9929 | Time 1.2313(1.2273) | Loss 2.673842(2.695050) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9930 | Time 1.2212(1.2269) | Loss 2.641862(2.691327) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9931 | Time 1.2151(1.2261) | Loss 2.648100(2.688301) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9932 | Time 1.2176(1.2255) | Loss 2.696094(2.688847) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9933 | Time 1.2507(1.2273) | Loss 2.641913(2.685561) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9934 | Time 1.2296(1.2274) | Loss 2.642529(2.682549) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9935 | Time 1.2390(1.2282) | Loss 2.749411(2.687229) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9936 | Time 1.2279(1.2282) | Loss 2.715797(2.689229) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9937 | Time 1.2283(1.2282) | Loss 2.640596(2.685825) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9938 | Time 1.2334(1.2286) | Loss 2.640411(2.682646) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9939 | Time 1.2393(1.2293) | Loss 2.608014(2.677422) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9940 | Time 1.2405(1.2301) | Loss 2.598984(2.671931) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9941 | Time 1.2266(1.2299) | Loss 2.652617(2.670579) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9942 | Time 1.2209(1.2292) | Loss 2.580559(2.664278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9943 | Time 1.2155(1.2283) | Loss 2.632661(2.662064) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9944 | Time 1.2248(1.2280) | Loss 2.608537(2.658318) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9945 | Time 1.2240(1.2278) | Loss 2.611484(2.655039) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9946 | Time 1.2271(1.2277) | Loss 2.602419(2.651356) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9947 | Time 1.2288(1.2278) | Loss 2.632364(2.650026) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9948 | Time 1.2331(1.2282) | Loss 2.661618(2.650838) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9949 | Time 1.2303(1.2283) | Loss 2.620408(2.648708) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9950 | Time 1.2239(1.2280) | Loss 2.644652(2.648424) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9951 | Time 1.2207(1.2275) | Loss 2.658806(2.649151) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9952 | Time 1.2195(1.2269) | Loss 2.682143(2.651460) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9953 | Time 1.2169(1.2262) | Loss 2.631018(2.650029) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9954 | Time 1.2136(1.2253) | Loss 2.659994(2.650727) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9955 | Time 1.2202(1.2250) | Loss 2.669903(2.652069) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9956 | Time 1.2122(1.2241) | Loss 2.626208(2.650259) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9957 | Time 1.2378(1.2251) | Loss 2.617936(2.647996) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9958 | Time 1.2127(1.2242) | Loss 2.646936(2.647922) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9959 | Time 1.2282(1.2245) | Loss 2.613693(2.645526) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9960 | Time 1.2608(1.2270) | Loss 2.682315(2.648101) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9961 | Time 1.2332(1.2274) | Loss 2.622055(2.646278) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9962 | Time 1.2359(1.2280) | Loss 2.707736(2.650580) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9963 | Time 1.2353(1.2285) | Loss 2.676086(2.652365) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9964 | Time 1.2445(1.2297) | Loss 2.724678(2.657427) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9965 | Time 1.2405(1.2304) | Loss 2.720904(2.661871) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9966 | Time 1.2371(1.2309) | Loss 2.629948(2.659636) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9967 | Time 1.2423(1.2317) | Loss 2.716073(2.663587) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9968 | Time 1.2478(1.2328) | Loss 2.687126(2.665234) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9969 | Time 1.2450(1.2337) | Loss 2.744774(2.670802) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9970 | Time 1.2599(1.2355) | Loss 2.743063(2.675860) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9971 | Time 1.2333(1.2353) | Loss 2.659273(2.674699) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9972 | Time 1.2281(1.2348) | Loss 2.743758(2.679533) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9973 | Time 1.2124(1.2333) | Loss 2.790127(2.687275) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9974 | Time 1.2243(1.2326) | Loss 2.717520(2.689392) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9975 | Time 1.2226(1.2319) | Loss 2.740632(2.692979) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9976 | Time 1.2351(1.2322) | Loss 2.702872(2.693671) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9977 | Time 1.2276(1.2318) | Loss 2.702926(2.694319) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9978 | Time 1.2168(1.2308) | Loss 2.719079(2.696053) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9979 | Time 1.2196(1.2300) | Loss 2.716368(2.697475) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9980 | Time 1.2137(1.2289) | Loss 2.787234(2.703758) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9981 | Time 1.2214(1.2283) | Loss 2.622293(2.698055) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9982 | Time 1.2305(1.2285) | Loss 2.710665(2.698938) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9983 | Time 1.2151(1.2275) | Loss 2.688525(2.698209) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9984 | Time 1.2263(1.2275) | Loss 2.733883(2.700706) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9985 | Time 1.2052(1.2259) | Loss 2.716119(2.701785) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9986 | Time 1.2550(1.2279) | Loss 2.691033(2.701032) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9987 | Time 1.2691(1.2308) | Loss 2.713132(2.701879) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9988 | Time 1.2363(1.2312) | Loss 2.682971(2.700556) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9989 | Time 1.2369(1.2316) | Loss 2.631633(2.695731) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9990 | Time 1.2395(1.2322) | Loss 2.712322(2.696893) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9991 | Time 1.2281(1.2319) | Loss 2.624878(2.691852) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9992 | Time 1.2371(1.2322) | Loss 2.744039(2.695505) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9993 | Time 1.2441(1.2331) | Loss 2.674070(2.694004) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9994 | Time 1.2353(1.2332) | Loss 2.662277(2.691783) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9995 | Time 1.2595(1.2351) | Loss 2.648280(2.688738) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9996 | Time 1.2445(1.2357) | Loss 2.646761(2.685800) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9997 | Time 1.2379(1.2359) | Loss 2.632717(2.682084) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9998 | Time 1.2291(1.2354) | Loss 2.664216(2.680833) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 9999 | Time 1.2601(1.2371) | Loss 2.642060(2.678119) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
Iter 10000 | Time 1.2300(1.2366) | Loss 2.666381(2.677297) | NFE Forward 20(20.0) | NFE Backward 21(21.0)
[TEST] Iter 10000 | Test Loss 2.619638 | NFE 20
Skipping vis as data dimension is >2
Training has finished.
